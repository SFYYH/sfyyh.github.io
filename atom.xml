<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>山城冰荔枝</title>
  
  
  <link href="http://blog.ioimp.top/atom.xml" rel="self"/>
  
  <link href="http://blog.ioimp.top/"/>
  <updated>2023-12-22T09:15:01.244Z</updated>
  <id>http://blog.ioimp.top/</id>
  
  <author>
    <name>山城冰荔枝</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>为什么 yarn build 命令非常耗时</title>
    <link href="http://blog.ioimp.top/2023/12/22/%E4%B8%BA%E4%BB%80%E4%B9%88-yarn-build%E5%91%BD%E4%BB%A4%E9%9D%9E%E5%B8%B8%E8%80%97%E6%97%B6/"/>
    <id>http://blog.ioimp.top/2023/12/22/%E4%B8%BA%E4%BB%80%E4%B9%88-yarn-build%E5%91%BD%E4%BB%A4%E9%9D%9E%E5%B8%B8%E8%80%97%E6%97%B6/</id>
    <published>2023-12-22T08:31:23.000Z</published>
    <updated>2023-12-22T09:15:01.244Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在开发过程中，我们经常使用构建工具来编译、打包和优化我们的代码。而在前端开发中，<code>yarn build</code> 命令是常用的构建命令之一。然而，你可能会注意到，<code>yarn build</code> 命令有时候会非常耗时，特别是在项目变得庞大复杂时。那么，为什么 <code>yarn build</code> 命令会如此耗时呢？本篇博客将对此进行探讨。</p><h2 id="1-代码量的增加"><a href="#1-代码量的增加" class="headerlink" title="1. 代码量的增加"></a>1. 代码量的增加</h2><p>随着项目的发展，代码量也会逐渐增加。更多的代码需要被编译、转换、打包和优化，这必然会增加构建的时间。尤其是在处理大量文件时，构建工具需要遍历每一个文件并进行处理，这将会消耗大量的时间。</p><h2 id="2-依赖项的增多"><a href="#2-依赖项的增多" class="headerlink" title="2. 依赖项的增多"></a>2. 依赖项的增多</h2><p>在现代的前端开发中，我们通常依赖于许多第三方库和工具。这些依赖项可能有自己的构建过程，当我们执行 <code>yarn build</code> 命令时，构建工具需要先编译和打包这些依赖项，然后再处理我们自己的代码。因此，依赖项的增多也会导致构建时间的增加。</p><h2 id="3-文件读取和加载"><a href="#3-文件读取和加载" class="headerlink" title="3. 文件读取和加载"></a>3. 文件读取和加载</h2><p>在构建过程中，构建工具需要从磁盘读取对应的文件到内存中进行处理。这个过程涉及到磁盘的读取速度和文件的大小。如果项目中有大量的文件或者文件很大，那么读取和加载的时间将会增加。</p><h2 id="4-代码处理和转换"><a href="#4-代码处理和转换" class="headerlink" title="4. 代码处理和转换"></a>4. 代码处理和转换</h2><p>一旦文件被加载到内存中，构建工具开始根据配置使用对应的 loader 对代码进行处理和转换。例如，对于 JavaScript 文件，可能会使用 Babel 进行转换；对于 CSS 文件，可能会使用 PostCSS 进行处理。这个处理和转换的过程可能会涉及到复杂的算法和逻辑，因此会耗费一定的时间。</p><h2 id="5-输出到磁盘"><a href="#5-输出到磁盘" class="headerlink" title="5. 输出到磁盘"></a>5. 输出到磁盘</h2><p>在代码处理和转换完成后，构建工具会将处理完的内容输出到磁盘的指定目录。这个过程也需要写入磁盘的速度和文件的大小。如果输出的文件很多或者文件很大，那么写入磁盘的时间将会增加。</p><h2 id="6-优化和压缩过程"><a href="#6-优化和压缩过程" class="headerlink" title="6. 优化和压缩过程"></a>6. 优化和压缩过程</h2><p>在构建过程中，我们通常会对代码进行优化和压缩，以提高性能和减少文件大小。这些优化和压缩过程可能需要较长的时间，特别是在处理大型项目时。例如，压缩和混淆 JavaScript 代码、优化 CSS 样式、压缩图片等都需要一定的时间。</p><h2 id="7-硬件性能限制"><a href="#7-硬件性能限制" class="headerlink" title="7. 硬件性能限制"></a>7. 硬件性能限制</h2><p>在一些较老或配置较低的计算机上，构建过程可能会更加耗时。较慢的处理器、较少的内存和较慢的硬盘都会对构建时间产生影响。因此，如果你的计算机性能较低，那么构建时间可能会更长。</p><h2 id="8-构建过程的优化"><a href="#8-构建过程的优化" class="headerlink" title="8. 构建过程的优化"></a>8. 构建过程的优化</h2><p>尽管 <code>yarn build</code> 命令可能会耗时，但我们仍然可以采取一些措施来优化构建过程，以减少构建时间。以下是一些常见的优化方法：</p><ul><li>使用增量构建：只重新构建修改过的文件，而不是整个项目。</li><li>使用缓存：将构建过程中生成的中间文件缓存起来，以便下次构建时能够复用。</li><li>并行处理：将构建过程中的任务并行执行，以提高整体的构建速度。</li><li>优化配置文件：检查构建工具的配置文件，确保其使用了最佳的配置选项。</li></ul><p>总结起来，<code>yarn build</code> 命令耗时的原因有很多，包括代码量的增加、依赖项的增多、文件读取和加载、代码处理和转换、输出到磁盘、优化和压缩过程、硬件性能限制等。然而，我们可以通过优化构建过程和硬件环境，来减少构建时间，提高开发效率。希望本篇博客能够对你理解 <code>yarn build</code> 命令的耗时问题有所帮助。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="前端" scheme="http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"/>
    
    
    <category term="ES6" scheme="http://blog.ioimp.top/tags/ES6/"/>
    
    <category term="前端学习" scheme="http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Node.js" scheme="http://blog.ioimp.top/tags/Node-js/"/>
    
    <category term="WebPack" scheme="http://blog.ioimp.top/tags/WebPack/"/>
    
  </entry>
  
  <entry>
    <title>WebPack对Js降级实现兼容低版本浏览器</title>
    <link href="http://blog.ioimp.top/2023/12/22/WebPack%E5%AF%B9js%E9%99%8D%E7%BA%A7%E5%AE%9E%E7%8E%B0%E5%85%BC%E5%AE%B9%E4%BD%8E%E7%89%88%E6%9C%AC%E6%B5%8F%E8%A7%88%E5%99%A8/"/>
    <id>http://blog.ioimp.top/2023/12/22/WebPack%E5%AF%B9js%E9%99%8D%E7%BA%A7%E5%AE%9E%E7%8E%B0%E5%85%BC%E5%AE%B9%E4%BD%8E%E7%89%88%E6%9C%AC%E6%B5%8F%E8%A7%88%E5%99%A8/</id>
    <published>2023-12-22T07:34:51.000Z</published>
    <updated>2023-12-22T08:16:09.125Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在前端开发中，我们经常会遇到兼容性的问题，特别是在低版本浏览器中。为了解决这个问题，Webpack提供了一种降级的方式，可以实现在低版本浏览器中兼容新的JavaScript语法和功能。</p><h2 id="什么是Webpack"><a href="#什么是Webpack" class="headerlink" title="什么是Webpack"></a>什么是Webpack</h2><p>Webpack是一个现代化的前端构建工具，它能够将多个JavaScript文件打包成一个或多个bundle文件，从而提高网站性能和加载速度。除此之外，Webpack还提供了许多其他功能，如代码分割、模块化、热模块替换等。</p><h2 id="为什么需要兼容低版本浏览器"><a href="#为什么需要兼容低版本浏览器" class="headerlink" title="为什么需要兼容低版本浏览器"></a>为什么需要兼容低版本浏览器</h2><p>虽然现代浏览器已经支持了许多新的JavaScript语法和功能，但是在实际开发中我们仍然需要兼容低版本浏览器。因为在某些情况下，用户可能仍然使用低版本的浏览器，我们不能因为他们的浏览器版本低就让他们无法正常使用我们的网站或应用。</p><h2 id="Webpack如何实现兼容低版本浏览器"><a href="#Webpack如何实现兼容低版本浏览器" class="headerlink" title="Webpack如何实现兼容低版本浏览器"></a>Webpack如何实现兼容低版本浏览器</h2><p>Webpack提供了一些插件和配置选项，可以帮助我们实现兼容低版本浏览器。下面是一些常用的方法：</p><h3 id="Babel插件"><a href="#Babel插件" class="headerlink" title="Babel插件"></a>Babel插件</h3><p>Babel是一个广泛使用的JavaScript编译器，它可以将新的JavaScript语法转换成低版本浏览器可以理解的语法。在Webpack中，我们可以使用Babel插件来处理JavaScript文件。</p><p>首先，我们需要安装一些必要的Babel插件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install babel-loader @babel/core @babel/preset-env --save-dev</span><br></pre></td></tr></table></figure><p>然后，在Webpack配置文件中，我们需要添加一个规则来处理JavaScript文件：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">module</span>: &#123;</span><br><span class="line">  <span class="attr">rules</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">test</span>: <span class="regexp">/\.js$/</span>,</span><br><span class="line">      <span class="attr">exclude</span>: <span class="regexp">/node_modules/</span>,</span><br><span class="line">      <span class="attr">use</span>: &#123;</span><br><span class="line">        <span class="attr">loader</span>: <span class="string">&#x27;babel-loader&#x27;</span>,</span><br><span class="line">        <span class="attr">options</span>: &#123;</span><br><span class="line">          <span class="attr">presets</span>: [<span class="string">&#x27;@babel/preset-env&#x27;</span>]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样，Webpack就会使用Babel插件来转换JavaScript文件，并将其输出到bundle文件中。</p><h3 id="Polyfill"><a href="#Polyfill" class="headerlink" title="Polyfill"></a>Polyfill</h3><p>除了转换新的JavaScript语法外，我们还需要处理一些新的JavaScript功能，比如Promise、Map、Set等。为了在低版本浏览器中使用这些功能，我们可以使用Polyfill。</p><p>Polyfill是一种JavaScript代码片段，它可以在低版本浏览器中实现新的JavaScript功能。在Webpack中，我们可以使用<code>@babel/polyfill</code>插件来引入Polyfill。</p><p>首先，我们需要安装<code>@babel/polyfill</code>插件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install @babel/polyfill --save</span><br></pre></td></tr></table></figure><p>然后，在入口文件中，我们需要引入Polyfill：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> <span class="string">&#x27;@babel/polyfill&#x27;</span>;</span><br></pre></td></tr></table></figure><p>这样，Webpack会将Polyfill打包到bundle文件中，并在低版本浏览器中自动加载。</p><h3 id="其他配置选项"><a href="#其他配置选项" class="headerlink" title="其他配置选项"></a>其他配置选项</h3><p>除了Babel插件和Polyfill外，Webpack还提供了其他一些配置选项，可以帮助我们实现兼容低版本浏览器。比如，我们可以使用<code>target</code>选项来指定要兼容的浏览器版本：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">target</span>: [<span class="string">&#x27;web&#x27;</span>, <span class="string">&#x27;browserslist:ie &gt;= 8&#x27;</span>]</span><br></pre></td></tr></table></figure><p>这样，Webpack会根据我们的配置自动处理兼容性问题。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在本文中，我们介绍了Webpack如何实现兼容低版本浏览器的方法。通过使用Babel插件、Polyfill和其他配置选项，我们可以让我们的网站或应用在低版本浏览器中正常运行。希望本文对你有所帮助！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="前端" scheme="http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"/>
    
    
    <category term="ES6" scheme="http://blog.ioimp.top/tags/ES6/"/>
    
    <category term="前端学习" scheme="http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Node.js" scheme="http://blog.ioimp.top/tags/Node-js/"/>
    
    <category term="WebPack" scheme="http://blog.ioimp.top/tags/WebPack/"/>
    
  </entry>
  
  <entry>
    <title>Webpack中加载器模式设为asset，为什么以8KB大小区分图片</title>
    <link href="http://blog.ioimp.top/2023/12/22/Webpack%E4%B8%AD%E5%8A%A0%E8%BD%BD%E5%99%A8%E6%A8%A1%E5%BC%8F%E8%AE%BE%E4%B8%BAasset%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BB%A58KB%E5%A4%A7%E5%B0%8F%E5%8C%BA%E5%88%86%E5%9B%BE%E7%89%87/"/>
    <id>http://blog.ioimp.top/2023/12/22/Webpack%E4%B8%AD%E5%8A%A0%E8%BD%BD%E5%99%A8%E6%A8%A1%E5%BC%8F%E8%AE%BE%E4%B8%BAasset%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BB%A58KB%E5%A4%A7%E5%B0%8F%E5%8C%BA%E5%88%86%E5%9B%BE%E7%89%87/</id>
    <published>2023-12-22T07:11:39.000Z</published>
    <updated>2023-12-22T07:34:50.722Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="Webpack中加载器模式设为asset，为什么以8KB大小区分图片"><a href="#Webpack中加载器模式设为asset，为什么以8KB大小区分图片" class="headerlink" title="Webpack中加载器模式设为asset，为什么以8KB大小区分图片"></a>Webpack中加载器模式设为asset，为什么以8KB大小区分图片</h2><p>在Webpack中，加载器模式可以设置为<code>asset</code>，用于处理资源文件（例如图片、字体等）。</p><p>默认情况下，Webpack会根据资源文件的大小来决定如何处理。当资源文件的大小小于8KB时，Webpack会将其转换为Data URL（base64编码）并直接嵌入到生成的JS文件中，以减少HTTP请求。而当资源文件的大小大于等于8KB时，Webpack会将其处理为独立的文件，并通过URL引用的方式加载。</p><p>这种区分是为了平衡代码体积和加载性能之间的关系。将小文件转换为Data URL可以减少HTTP请求的数量，但会增加JS文件的体积，可能导致加载时间变长。而将大文件作为独立文件加载可以减小JS文件的体积，但会增加HTTP请求的数量，可能导致加载时间变长。</p><p>因此，8KB是一个经验值，可以根据具体项目的需求进行调整。如果项目中有大量的小文件，可以考虑将此值调低，以减少HTTP请求的数量；如果项目中有大量的大文件，可以考虑将此值调高，以减小JS文件的体积。</p><h2 id="将图片转换为base64编码的好处和坏处如下："><a href="#将图片转换为base64编码的好处和坏处如下：" class="headerlink" title="将图片转换为base64编码的好处和坏处如下："></a>将图片转换为base64编码的好处和坏处如下：</h2><p><strong>好处：</strong></p><ol><li>减少HTTP请求：将图片转换为base64编码后，可以直接嵌入到HTML、CSS或JavaScript文件中，避免了额外的HTTP请求，从而加快页面加载速度。</li><li>简化项目结构：将图片嵌入到代码中，可以减少项目中的文件数量，简化项目结构，方便管理和部署。</li><li>提高图片加载速度：由于base64编码的图片嵌入到文件中，不需要再进行额外的网络请求，因此可以减少图片的加载时间。</li></ol><p><strong>坏处：</strong></p><ol><li>增加文件体积：base64编码会使图片的体积增加约1&#x2F;3，因为编码后的文本比原始二进制数据要大。这可能导致文件体积增大，特别是当有多个图片被转换为base64编码时，会增加整个文件的大小。</li><li>缓存问题：base64编码的图片无法被浏览器缓存，因为它们被嵌入到了文件中。每次文件更新都会导致图片重新下载，这可能会影响网页的性能。</li><li>不适用于大型图片：由于base64编码会增加文件体积，因此对于大型图片（通常超过几十KB或更大）来说，将其转换为base64编码可能导致文件过大，影响页面加载速度。</li></ol><p>因此，将图片转换为base64编码适用于小型图标或小图片，并且在需要减少HTTP请求和简化项目结构的情况下使用。对于大型图片，最好将其作为独立文件加载，以避免文件过大和缓存问题。</p><h2 id="WebPack如何处理字体图标"><a href="#WebPack如何处理字体图标" class="headerlink" title="WebPack如何处理字体图标"></a>WebPack如何处理字体图标</h2><p><img src="/images/image15.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="前端" scheme="http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"/>
    
    
    <category term="ES6" scheme="http://blog.ioimp.top/tags/ES6/"/>
    
    <category term="前端学习" scheme="http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Node.js" scheme="http://blog.ioimp.top/tags/Node-js/"/>
    
    <category term="WebPack" scheme="http://blog.ioimp.top/tags/WebPack/"/>
    
  </entry>
  
  <entry>
    <title>宏任务和微任务的区别</title>
    <link href="http://blog.ioimp.top/2023/12/22/%E5%AE%8F%E4%BB%BB%E5%8A%A1%E5%92%8C%E5%BE%AE%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://blog.ioimp.top/2023/12/22/%E5%AE%8F%E4%BB%BB%E5%8A%A1%E5%92%8C%E5%BE%AE%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%8C%BA%E5%88%AB/</id>
    <published>2023-12-21T16:52:10.000Z</published>
    <updated>2023-12-21T16:53:53.609Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>宏任务是由浏览器引擎进行调度和执行的，它们会被放入宏任务队列中，并且按照队列的顺序执行。宏任务的执行时间较长，因此会造成较大的延迟。常见的宏任务包括DOM事件处理、setTimeout和setInterval等。</p><p>微任务是在宏任务执行完毕之后立即执行的任务，它们会被放入微任务队列中，并且在宏任务队列为空时执行。微任务的执行时间较短，因此不会造成较大的延迟。常见的微任务包括Promise的resolve和reject回调、MutationObserver和process.nextTick等。</p><p>由于微任务会在宏任务执行完毕之后立即执行，因此微任务的优先级较高。也就是说，当一个宏任务执行完毕后，会立即执行所有的微任务，而不会等待下一个宏任务。这样可以保证微任务的执行顺序不会被打乱。</p><p>总结起来，宏任务的执行顺序是先进先出，而微任务的执行顺序是后进先出。宏任务的执行时间较长，会造成较大的延迟，而微任务的执行时间较短，不会造成较大的延迟。微任务的优先级较高，会在宏任务执行完毕之后立即执行。<br><img src="/images/image13.png"><br><img src="/images/image14.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>为什么说同步任务是非耗时任务，异步任务是耗时任务</title>
    <link href="http://blog.ioimp.top/2023/12/22/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4%E5%90%8C%E6%AD%A5%E4%BB%BB%E5%8A%A1%E6%98%AF%E9%9D%9E%E8%80%97%E6%97%B6%E4%BB%BB%E5%8A%A1%EF%BC%8C%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E6%98%AF%E8%80%97%E6%97%B6%E4%BB%BB%E5%8A%A1/"/>
    <id>http://blog.ioimp.top/2023/12/22/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4%E5%90%8C%E6%AD%A5%E4%BB%BB%E5%8A%A1%E6%98%AF%E9%9D%9E%E8%80%97%E6%97%B6%E4%BB%BB%E5%8A%A1%EF%BC%8C%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E6%98%AF%E8%80%97%E6%97%B6%E4%BB%BB%E5%8A%A1/</id>
    <published>2023-12-21T16:35:58.000Z</published>
    <updated>2023-12-21T16:43:04.362Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>同步任务</strong>是指在程序执行过程中，必须等待该任务完成后才能继续执行下面的代码。因为同步任务会阻塞程序的执行，所以它通常被认为是非耗时任务。</p><p><strong>异步任务</strong>是指在程序执行过程中，不需要等待该任务完成就可以继续执行下面的代码。异步任务通常会通过多线程、回调函数或者事件驱动等方式实现。由于异步任务不会阻塞程序的执行，所以它通常被认为是耗时任务。</p><p><strong>需要注意的是</strong>，同步任务和异步任务的耗时性质与任务本身的执行时间没有直接关系。一个同步任务可能执行时间很长，但它会阻塞程序的执行，所以被认为是非耗时任务。而一个异步任务可能执行时间很短，但它不会阻塞程序的执行，所以被认为是耗时任务。</p><p>耗时和非耗时的区分是<strong>相对于程序整体执行流程而言的</strong>。耗时任务通常指的是那些需要较长时间才能完成的任务，比如访问网络、读写大文件、进行复杂计算等。这些任务如果以同步方式执行，会导致程序在等待任务完成期间无法进行任何其他操作，即阻塞了程序的执行流程，用户体验较差。</p><p>非耗时任务则是那些可以迅速完成的任务，如简单的数学计算、修改变量值等。这些任务即使以同步方式执行，也不会对程序的流畅性造成太大影响。</p><p>因此，在编程中，通常会将耗时任务设计为异步执行，以避免阻塞主线程，提高程序的响应性和效率。通过回调函数、Promise、async&#x2F;await等机制，可以在耗时任务完成后再执行相关的操作，而不必让整个程序等待耗时任务的完成。这就是为什么通常将同步任务视为非耗时任务，而将异步任务视为耗时任务的原因。</p><h2 id="同步任务和异步任务的执行过程"><a href="#同步任务和异步任务的执行过程" class="headerlink" title="同步任务和异步任务的执行过程"></a>同步任务和异步任务的执行过程</h2><p><img src="/images/image12.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="前端" scheme="http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"/>
    
    
    <category term="ES6" scheme="http://blog.ioimp.top/tags/ES6/"/>
    
    <category term="前端学习" scheme="http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Node.js" scheme="http://blog.ioimp.top/tags/Node-js/"/>
    
    <category term="异步操作" scheme="http://blog.ioimp.top/tags/%E5%BC%82%E6%AD%A5%E6%93%8D%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>Node中async/await的详解</title>
    <link href="http://blog.ioimp.top/2023/12/22/Node%E4%B8%ADasync-await%E7%9A%84%E8%AF%A6%E8%A7%A3/"/>
    <id>http://blog.ioimp.top/2023/12/22/Node%E4%B8%ADasync-await%E7%9A%84%E8%AF%A6%E8%A7%A3/</id>
    <published>2023-12-21T16:23:35.000Z</published>
    <updated>2023-12-21T16:30:07.422Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="async-await的基本使用"><a href="#async-await的基本使用" class="headerlink" title="async&#x2F;await的基本使用"></a>async&#x2F;await的基本使用</h1><p>async&#x2F;await是ES8中引入的新语法，用于简化promise的异步操作。</p><p>使用async关键字修饰函数，表示该函数是一个异步函数。异步函数内部可以使用await关键字来等待一个promise对象的执行结果。</p><p>await关键字可以放在任何返回promise的表达式前面，它会暂停函数的执行，直到promise被解析或拒绝。如果promise被解析，await表达式会返回解析的值；如果promise被拒绝，await表达式会抛出一个错误。</p><p>异步函数可以像普通函数一样返回一个值，返回的值会被包装成一个被解析的promise对象。</p><p>以下是示例代码：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 异步函数示例</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">function</span> <span class="title function_">fetchData</span>(<span class="params"></span>) &#123;</span><br><span class="line">  <span class="comment">// 使用await等待promise的执行结果</span></span><br><span class="line">  <span class="keyword">const</span> result = <span class="keyword">await</span> <span class="title function_">fetch</span>(<span class="string">&#x27;https://api.example.com/data&#x27;</span>);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 返回一个被解析的promise对象</span></span><br><span class="line">  <span class="keyword">return</span> result.<span class="title function_">json</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用异步函数示例</span></span><br><span class="line"><span class="title function_">fetchData</span>()</span><br><span class="line">  .<span class="title function_">then</span>(<span class="function"><span class="params">data</span> =&gt;</span> &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(data);</span><br><span class="line">  &#125;)</span><br><span class="line">  .<span class="title function_">catch</span>(<span class="function"><span class="params">error</span> =&gt;</span> &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">error</span>(error);</span><br><span class="line">  &#125;);</span><br></pre></td></tr></table></figure><p>在上面的示例中，<code>fetchData</code>是一个异步函数，使用<code>await</code>关键字等待<code>fetch</code>函数返回的promise对象的执行结果。在<code>fetchData</code>函数内部，可以像同步代码一样使用<code>result</code>变量来访问<code>fetch</code>函数返回的结果。</p><p>调用异步函数时，可以像调用普通函数一样使用<code>.then</code>和<code>.catch</code>方法来处理异步操作的结果。在上面的示例中，使用<code>.then</code>方法来处理异步操作的成功结果，使用<code>.catch</code>方法来处理异步操作的错误结果。</p><p>通过使用async&#x2F;await，我们可以将异步操作的代码写得更加简洁、易读，并且可以避免使用链式调用then的方式。</p><h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><h2 id="1-使用环境"><a href="#1-使用环境" class="headerlink" title="1. 使用环境"></a>1. 使用环境</h2><ul><li><code>async</code>和<code>await</code>是ES7的新特性，需要确保Node版本至少为7.6.0或更高。</li><li><code>async</code>函数返回一个Promise对象，可以使用<code>.then()</code>和<code>.catch()</code>进行链式调用。</li></ul><h2 id="2-错误处理"><a href="#2-错误处理" class="headerlink" title="2. 错误处理"></a>2. 错误处理</h2><ul><li>使用<code>await</code>时，如果Promise被拒绝（reject），会抛出异常。因此需要使用<code>try...catch</code>语句进行错误处理。</li><li>如果没有正确处理错误，可能会导致程序崩溃。</li></ul><h2 id="3-循环中使用"><a href="#3-循环中使用" class="headerlink" title="3. 循环中使用"></a>3. 循环中使用</h2><ul><li>在循环中使用<code>await</code>时，需要注意可能会导致代码变成串行执行，影响性能。</li><li>如果需要并行执行，可以使用<code>Promise.all()</code>。</li></ul><h2 id="4-await的使用"><a href="#4-await的使用" class="headerlink" title="4. await的使用"></a>4. <code>await</code>的使用</h2><ul><li><code>await</code>只能在<code>async</code>函数内部使用。</li><li><code>await</code>后面跟着的应该是一个Promise对象或者任何要等待的值。</li><li>在async修饰的方法中，第一个await之前的代码都是同步执行的，而第一个await之后的代码都会异步执行。</li></ul><h2 id="5-返回值"><a href="#5-返回值" class="headerlink" title="5. 返回值"></a>5. 返回值</h2><ul><li><code>async</code>函数总是返回一个Promise，即使函数内部没有使用<code>await</code>。</li><li>如果<code>async</code>函数内部抛出错误，返回的Promise会被拒绝（reject）。</li></ul><h2 id="6-await的等待"><a href="#6-await的等待" class="headerlink" title="6. await的等待"></a>6. <code>await</code>的等待</h2><ul><li><code>await</code>会暂停其后的代码执行，直到Promise解决（resolve）或拒绝（reject）。</li></ul><h2 id="7-调试"><a href="#7-调试" class="headerlink" title="7. 调试"></a>7. 调试</h2><ul><li>在使用<code>async</code>和<code>await</code>时，可能会使得调试变得更加困难，因为它们会改变错误堆栈的追踪方式。</li><li></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="前端" scheme="http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"/>
    
    
    <category term="ES6" scheme="http://blog.ioimp.top/tags/ES6/"/>
    
    <category term="前端学习" scheme="http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Node.js" scheme="http://blog.ioimp.top/tags/Node-js/"/>
    
    <category term="异步操作" scheme="http://blog.ioimp.top/tags/%E5%BC%82%E6%AD%A5%E6%93%8D%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>Node中Promise 对象的意思</title>
    <link href="http://blog.ioimp.top/2023/12/21/Node%E4%B8%ADPromise-%E5%AF%B9%E8%B1%A1%E7%9A%84%E6%84%8F%E6%80%9D/"/>
    <id>http://blog.ioimp.top/2023/12/21/Node%E4%B8%ADPromise-%E5%AF%B9%E8%B1%A1%E7%9A%84%E6%84%8F%E6%80%9D/</id>
    <published>2023-12-21T02:56:30.000Z</published>
    <updated>2023-12-21T16:02:47.417Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Node中Promise-对象的意思"><a href="#Node中Promise-对象的意思" class="headerlink" title="Node中Promise 对象的意思"></a>Node中Promise 对象的意思</h1><p>Promise 对象是 JavaScript 中处理异步操作的一种方式，它代表了一个异步操作的最终完成或失败，并可以返回一个结果或错误。</p><h2 id="Promise-的基本概念"><a href="#Promise-的基本概念" class="headerlink" title="Promise 的基本概念"></a>Promise 的基本概念</h2><ul><li>Promise 是一个构造函数<ul><li>我们可以创建Promise的实例 const p&#x3D; new Promise()</li><li>new 出来的一个实例对象，代表一个异步操作</li></ul></li><li>Promise.prototype上包含then() </li><li>.then（）方法用来预先指定成功和失败的回调函数<ul><li>p.then(成功回调函数，失败的回调函数)</li><li>p.then(result&#x3D;&gt;{},error&#x3D;&gt;{})</li></ul></li></ul><h2 id="Promise-对象有三种状态："><a href="#Promise-对象有三种状态：" class="headerlink" title="Promise 对象有三种状态："></a>Promise 对象有三种状态：</h2><ol><li>Pending（进行中）：初始状态，表示异步操作还在进行中，既不是成功也不是失败状态。</li><li>Fulfilled（已完成）：表示异步操作成功完成，并返回了一个结果。</li><li>Rejected（已失败）：表示异步操作失败，并返回了一个错误。</li></ol><p>Promise 对象的构造函数接受一个执行器函数作为参数，这个执行器函数有两个参数，分别是 resolve 和 reject 函数。在执行器函数中，我们可以执行异步操作，并在适当的时候调用 resolve 或 reject 函数来改变 Promise 对象的状态。</p><h2 id="Promise-对象具有以下特点："><a href="#Promise-对象具有以下特点：" class="headerlink" title="Promise 对象具有以下特点："></a>Promise 对象具有以下特点：</h2><ol><li>Promise 对象是不可变的，一旦状态改变就无法再次改变。</li><li>Promise 对象可以通过 <code>.then()</code> 方法添加成功状态的回调函数，通过 <code>.catch()</code> 方法添加失败状态的回调函数，也可以使用 <code>.finally()</code> 方法添加无论成功或失败都会执行的回调函数。</li><li>Promise 对象可以通过 Promise 链实现对多个异步操作的串行或并行处理。</li><li>Promise 对象可以通过 <code>async/await</code> 语法进行更简洁的异步操作处理。</li></ol><h2 id="可以用console-dir-Promise-来查课Promise对象的属性"><a href="#可以用console-dir-Promise-来查课Promise对象的属性" class="headerlink" title="可以用console.dir(Promise) 来查课Promise对象的属性"></a>可以用console.dir(Promise) 来查课Promise对象的属性</h2><p>使用 Promise 对象可以更好地处理异步操作，避免了回调地狱和层层嵌套的问题，使代码更加清晰和可维护。<br><code>console.dir</code> 是 JavaScript 中的一个方法，用于将一个对象的所有可枚举属性打印到控制台中，以便查看对象的结构和属性。</p><p>该方法接受一个对象作为参数，并将对象的属性以键值对的形式打印到控制台中。与 <code>console.log()</code> 方法不同，<code>console.dir()</code> 方法会显示对象的属性的详细信息，包括属性名称、属性值和属性的数据类型。</p><p><code>console.dir()</code> 方法在调试和开发过程中非常有用，可以帮助开发人员了解对象的结构和属性，以便更好地理解和调试代码。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// 利用node  fs模块进行的读取文件操作 三次嵌套容易引起回调地狱</span></span><br><span class="line"><span class="comment">// const &#123; error &#125; = require(&#x27;console&#x27;);</span></span><br><span class="line"><span class="comment">// let fs = require(&#x27;fs&#x27;);</span></span><br><span class="line"><span class="comment">// fs.readFile(&#x27;./file/test1.txt&#x27;,&#x27;utf-8&#x27;,(error,res)=&gt;&#123;</span></span><br><span class="line"><span class="comment">//     if(error) console.log(error.message)</span></span><br><span class="line"><span class="comment">//     console.log(res)</span></span><br><span class="line"><span class="comment">//     fs.readFile(&#x27;./file/test2.txt&#x27;,&#x27;utf-8&#x27;,(error,res)=&gt;&#123;</span></span><br><span class="line"><span class="comment">//         if(error) console.log(error.message)</span></span><br><span class="line"><span class="comment">//         console.log(res)</span></span><br><span class="line"><span class="comment">//         fs.readFile(&#x27;./file/test3.txt&#x27;,&#x27;utf-8&#x27;,(error,res)=&gt;&#123;</span></span><br><span class="line"><span class="comment">//             if(error) console.log(error.message)</span></span><br><span class="line"><span class="comment">//             console.log(res)</span></span><br><span class="line"><span class="comment">//         &#125;)</span></span><br><span class="line"><span class="comment">//     &#125;)</span></span><br><span class="line"><span class="comment">// &#125;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 利用then-fs来进行文件读取操作（未进行顺序处理）</span></span><br><span class="line"><span class="comment">// import thenFs from &#x27;then-fs&#x27;</span></span><br><span class="line"><span class="comment">// thenFs.readFile(&#x27;./file/test.txt&#x27;,&#x27;utf8&#x27;).then((r1)=&gt;&#123;console.log(r1)&#125;)</span></span><br><span class="line"><span class="comment">// thenFs.readFile(&#x27;./file/test2.txt&#x27;,&#x27;utf8&#x27;).then((r2)=&gt;&#123;console.log(r2)&#125;)</span></span><br><span class="line"><span class="comment">// thenFs.readFile(&#x27;./file/test3.txt&#x27;,&#x27;utf8&#x27;).then((r3)=&gt;&#123;console.log(r3)&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// then-fs顺序处理</span></span><br><span class="line"><span class="keyword">import</span> thenFs <span class="keyword">from</span> <span class="string">&#x27;then-fs&#x27;</span></span><br><span class="line">thenFs.<span class="title function_">readFile</span>(<span class="string">&#x27;./file/test.txt&#x27;</span>,<span class="string">&#x27;utf8&#x27;</span>).<span class="title function_">then</span>(<span class="function">(<span class="params">r1</span>)=&gt;</span>&#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(r1)</span><br><span class="line">    <span class="keyword">return</span> thenFs.<span class="title function_">readFile</span>(<span class="string">&#x27;./file/test2.txt&#x27;</span>,<span class="string">&#x27;utf8&#x27;</span>).<span class="title function_">then</span>(<span class="function">(<span class="params">r2</span>)=&gt;</span>&#123;</span><br><span class="line">        <span class="variable language_">console</span>.<span class="title function_">log</span>(r2)</span><br><span class="line">        thenFs.<span class="title function_">readFile</span>(<span class="string">&#x27;./file/test3.txt&#x27;</span>,<span class="string">&#x27;utf8&#x27;</span>).<span class="title function_">then</span>(<span class="function">(<span class="params">r3</span>)=&gt;</span>&#123;</span><br><span class="line">            <span class="variable language_">console</span>.<span class="title function_">log</span>(r3)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="前端" scheme="http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"/>
    
    
    <category term="ES6" scheme="http://blog.ioimp.top/tags/ES6/"/>
    
    <category term="前端学习" scheme="http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Node.js" scheme="http://blog.ioimp.top/tags/Node-js/"/>
    
  </entry>
  
  <entry>
    <title>npm和pnpm的区别</title>
    <link href="http://blog.ioimp.top/2023/12/21/npm%E5%92%8Cpnpm%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://blog.ioimp.top/2023/12/21/npm%E5%92%8Cpnpm%E7%9A%84%E5%8C%BA%E5%88%AB/</id>
    <published>2023-12-21T01:43:41.000Z</published>
    <updated>2023-12-21T01:44:43.544Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>pnpm 和 npm 都是 JavaScript 包管理工具，用于安装和管理项目的依赖。</p><ol><li><p>安装速度：pnpm 在安装依赖时使用了硬链接的方式，可以复用已安装的依赖，因此安装速度更快。而 npm 则会将依赖完全复制到项目的 <code>node_modules</code> 目录中，因此安装速度相对较慢。</p></li><li><p>磁盘空间占用：由于 pnpm 使用了硬链接的方式，相同的依赖只会在磁盘上占用一份空间，因此 pnpm 的磁盘空间占用相对较小。而 npm 则会将每个项目的依赖都完整地复制到项目的 <code>node_modules</code> 目录中，因此磁盘空间占用较大。</p></li><li><p>内存占用：pnpm 在安装和运行时只需要占用较少的内存，因为它使用了硬链接和符号链接来共享依赖。而 npm 则需要占用较多的内存，因为它会将所有的依赖都解压到内存中。</p></li><li><p>兼容性：由于 pnpm 使用了硬链接和符号链接的方式，可能在某些操作系统或文件系统上不兼容。而 npm 则是使用了标准的文件复制方式，因此更加兼容。</p></li></ol><p>综上所述，pnpm 相对于 npm 来说，在安装速度、磁盘空间占用和内存占用方面有一定的优势，但在兼容性方面可能存在一些问题。因此，在选择使用哪个工具时，可以根据具体的项目需求和环境来决定。</p><p>pnpm 在安装依赖时会将依赖信息添加到项目的 package.json 文件中。这与 npm 的行为是一致的。当你使用 pnpm 安装依赖时，会自动更新 package.json 文件的 dependencies 或 devDependencies 字段，将安装的依赖添加到其中，以便项目在其他环境中能够正确地安装和运行所需的依赖。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="前端" scheme="http://blog.ioimp.top/categories/%E5%89%8D%E7%AB%AF/"/>
    
    
    <category term="ES6" scheme="http://blog.ioimp.top/tags/ES6/"/>
    
    <category term="前端学习" scheme="http://blog.ioimp.top/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Node.js" scheme="http://blog.ioimp.top/tags/Node-js/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy_Splash的使用</title>
    <link href="http://blog.ioimp.top/2023/12/03/13-Scrapy-Splash%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>http://blog.ioimp.top/2023/12/03/13-Scrapy-Splash%E7%9A%84%E4%BD%BF%E7%94%A8/</id>
    <published>2023-12-03T04:00:18.000Z</published>
    <updated>2023-12-03T04:02:14.277Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Scrapy-splash模块使用"><a href="#Scrapy-splash模块使用" class="headerlink" title="Scrapy_splash模块使用"></a>Scrapy_splash模块使用</h1><h2 id="一-Splash"><a href="#一-Splash" class="headerlink" title="一. Splash"></a>一. Splash</h2><p>后续的爬虫的课程. 随便随时来听.  – 樵夫说的. </p><p>splash是一个可以动态渲染js的工具. 有助于我们完成复杂的js内容加载工作.  你可以理解为另一个没有界面的selenium.</p><h3 id="1-1-splash安装"><a href="#1-1-splash安装" class="headerlink" title="1.1 splash安装"></a>1.1 splash安装</h3><p>splash的安装过程十分复杂. 复杂到官方都不推荐你去手动安装它. </p><p>官方建议. 用docker去安装splash. 所以. 你需要先去安装docker. 但是docker这玩意在windows上支持非常不好. 各种各样的问题. 外加上后期我们要把爬虫部署到linux. 那干脆. 我们就安装一个linux. 在linux上搞docker是非常easy的. </p><p>有能力, 不怕苦的同学可以在windows上搞一个docker试试. 我这里就不带你们找坑踩了. 直接上Linux. </p><h4 id="1-1-1安装VM"><a href="#1-1-1安装VM" class="headerlink" title="1.1.1安装VM"></a>1.1.1安装VM</h4><p><img src="/images/scrapy02/17561629196138_.pic_hd.jpg" alt="17561629196138_.pic_hd"></p><p><img src="/images/scrapy02/17561629196138_.pic.jpg" alt="17561629196138_.pic"></p><p><img src="/images/scrapy02/17581629196184_.pic.jpg" alt="17581629196184_.pic"></p><p><img src="/images/scrapy02/17571629196155_.pic-9199960.jpg" alt="17571629196155_.pic"></p><p><img src="/images/scrapy02/17581629196184_.pic-9199973.jpg" alt="17581629196184_.pic"></p><p><img src="/images/scrapy02/17591629196208_.pic.jpg" alt="17591629196208_.pic"></p><p><img src="/images/scrapy02/17601629196228_.pic.jpg" alt="17601629196228_.pic"></p><p><img src="/images/scrapy02/17611629196237_.pic.jpg" alt="17611629196237_.pic"></p><p><img src="/images/scrapy02/17621629196250_.pic_hd.jpg" alt="17621629196250_.pic_hd"></p><p><img src="/images/scrapy02/17631629196343_.pic_hd.jpg" alt="17631629196343_.pic_hd"></p><p><img src="/images/scrapy02/17641629196362_.pic_hd.jpg" alt="17641629196362_.pic_hd"></p><p><img src="/images/scrapy02/17651629196369_.pic_hd.jpg" alt="17651629196369_.pic_hd"></p><p><img src="/images/scrapy02/17661629196398_.pic_hd.jpg" alt="17661629196398_.pic_hd"></p><p><img src="/images/scrapy02/17671629196461_.pic_hd.jpg" alt="17671629196461_.pic_hd"></p><p><img src="/images/scrapy02/17681629196491_.pic_hd.jpg" alt="17681629196491_.pic_hd"></p><p><img src="/images/scrapy02/17691629196532_.pic_hd.jpg" alt="17691629196532_.pic_hd"></p><p><img src="/images/scrapy02/17701629196571_.pic_hd.jpg" alt="17701629196571_.pic_hd"></p><p><img src="/images/scrapy02/17711629196622_.pic_hd.jpg" alt="17711629196622_.pic_hd"></p><p><img src="/images/scrapy02/17721629196663_.pic_hd.jpg" alt="17721629196663_.pic_hd"></p><p><img src="/images/scrapy02/17731629196679_.pic_hd.jpg" alt="17731629196679_.pic_hd"></p><h4 id="1-1-2-安装Linux"><a href="#1-1-2-安装Linux" class="headerlink" title="1.1.2 安装Linux"></a>1.1.2 安装Linux</h4><img src="image-20210817140222006.png" alt="image-20210817140222006" style="zoom:50%;" /><img src="image-20210817140422074.png" alt="image-20210817140422074" style="zoom:40%;" /><img src="image-20210817140631614.png" alt="image-20210817140631614" style="zoom:40%;" /><img src="image-20210817140748141.png" alt="image-20210817140748141" style="zoom:40%;" /><img src="image-20210817140818074.png" alt="image-20210817140818074" style="zoom:40%;" /><img src="image-20210817140849908.png" alt="image-20210817140849908" style="zoom:40%;" /><img src="image-20210817140925752.png" alt="image-20210817140925752" style="zoom:40%;" /><img src="image-20210817141200195.png" alt="image-20210817141200195" style="zoom:50%;" /><img src="image-20210817141307223.png" alt="image-20210817141307223" style="zoom:50%;" /><img src="image-20210817141415716.png" alt="image-20210817141415716" style="zoom:40%;" /><img src="image-20210817141552531.png" alt="image-20210817141552531" style="zoom:40%;" /><img src="image-20210817141643259.png" alt="image-20210817141643259" style="zoom:40%;" /><img src="image-20210817141729496.png" alt="image-20210817141729496" style="zoom:40%;" /><img src="image-20210817141824889.png" alt="image-20210817141824889" style="zoom:40%;" /><img src="image-20210817141850749.png" alt="image-20210817141850749" style="zoom:40%;" /><img src="image-20210817142347367.png" alt="image-20210817142347367" style="zoom:40%;" /><img src="image-20210817142421655.png" alt="image-20210817142421655" style="zoom:50%;" /><img src="image-20210817142519453.png" alt="image-20210817142519453" style="zoom:40%;" /><img src="image-20210817142634663.png" alt="image-20210817142634663" style="zoom:40%;" /><p>安装好的linux后,我们需要学会使用linux的一个工具. 叫yum, 我们需要用它来帮我们完成各种软件的安装. 十分的方便. 我们先用<code>ifconfig</code>来做一个测试. </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum search ifconfig   // 搜索出ifconfig的包</span><br><span class="line"></span><br><span class="line">yum install net-tools.x86_64  // 安装该软件, 安装过程中会出现很多个询问. 直接y即可</span><br></pre></td></tr></table></figure><p>发现了吧, 在linux这个破黑窗口里. 属实难受+憋屈. 所以, 我们这里选择用ssh远程连接linux. </p><p>mac版本:  打开终端. 输入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh root@服务器ip地址</span><br><span class="line">输入密码</span><br></pre></td></tr></table></figure><p>就可以顺利的链接到你的linux服务器. 接下来. 我们可以使用各种命令来操纵linux了. </p><p>Windows: </p><p><img src="/images/scrapy02/17751629199547_.pic_hd.jpg" alt="17751629199547_.pic_hd"></p><p><img src="/images/scrapy02/17791629200276_.pic_hd.jpg" alt="17791629200276_.pic_hd"></p><p><img src="/images/scrapy02/17761629199585_.pic_hd.jpg" alt="17761629199585_.pic_hd"></p><p><img src="/images/scrapy02/17771629199596_.pic_hd.jpg" alt="17771629199596_.pic_hd"></p><p><img src="/images/scrapy02/17781629199688_.pic_hd.jpg" alt="17781629199688_.pic_hd"></p><h4 id="1-1-3-安装docker"><a href="#1-1-3-安装docker" class="headerlink" title="1.1.3 安装docker"></a>1.1.3 安装docker</h4><p>​安装docker就一条例命令就好了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@sylar-centos-2 ~]# yum install docker</span><br></pre></td></tr></table></figure><p>​配置docker的源. </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@sylar-centos-2 ~]# vi /etc/docker/daemon.json</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">写入一下内容, 注意.先按<span class="string">&#x27;i&#x27;</span>, 更换为输入模式. 然后再填写内容</span></span><br><span class="line">&#123;</span><br><span class="line">&quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com/&quot;]</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">保存: 先按esc. 退出输入模式, 然后输入<span class="string">&quot;:wq&quot;</span> 表示写入, 退出. 就完事儿了</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@sylar-centos-2 ~]# systemctl start docker    # 启动docker</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@sylar-centos-2 ~]# docker ps      # 查看docker运行状态</span><br></pre></td></tr></table></figure><p>如需关闭或者重新启动docker:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop docker   # 停止docker服务</span><br><span class="line">systemctl restart docker  # 重启docker服务</span><br></pre></td></tr></table></figure><p>Vm -&gt; cenos -&gt; ssh -&gt; docker -&gt; splash </p><h4 id="1-1-4-安装splash"><a href="#1-1-4-安装splash" class="headerlink" title="1.1.4 安装splash"></a>1.1.4 安装splash</h4><ol><li><p>拉取splash镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull scrapinghub/splash</span><br></pre></td></tr></table></figure><p>splash比较大. 大概2个G左右. 有点儿耐心等会儿就好了</p></li><li><p>运行splash</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 8050:8050 scrapinghub/splash</span><br></pre></td></tr></table></figure></li><li><p>打开浏览器访问splash</p><p><a href="http://192.168.31.82:8050/">http://192.168.31.82:8050/</a></p><p><img src="/images/scrapy02/image-20210817153337076.png" alt="image-20210817153337076"></p></li></ol><h3 id="1-2-splash简单使用"><a href="#1-2-splash简单使用" class="headerlink" title="1.2 splash简单使用"></a>1.2 splash简单使用</h3><p>​我们可以在文本框内输入百度的网址. 然后点击render. 可以看到splash会对我们的网页进行动态的加载. 并返回截图. 运行状况. 以及页面代码(经过js渲染后的)</p><p><img src="/images/scrapy02/image-20210817153704882.png" alt="image-20210817153704882"></p><p><img src="/images/scrapy02/image-20210817153711026.png" alt="image-20210817153711026"></p><p>快速解释一下, script中的脚本. 这里面用的是lua的脚本语法. 所以看起来会有些难受. </p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span>  <span class="comment">-- 主函数</span></span><br><span class="line">  <span class="built_in">assert</span>(splash:go(args.url))  <span class="comment">-- 进入xxx页面</span></span><br><span class="line">  <span class="built_in">assert</span>(splash:wait(<span class="number">0.5</span>))   <span class="comment">-- 等待0.5秒</span></span><br><span class="line">  <span class="keyword">return</span> &#123;  <span class="comment">-- 返回</span></span><br><span class="line">    html = splash:html(),  <span class="comment">-- splash:html() 页面源代码</span></span><br><span class="line">    png = splash:png(),   <span class="comment">-- splash:png() 页面截图</span></span><br><span class="line">    har = splash:har(),   <span class="comment">-- splash:har() 页面加载过程</span></span><br><span class="line">  &#125;</span><br><span class="line"><span class="keyword">end</span>   <span class="comment">-- 函数结束</span></span><br></pre></td></tr></table></figure><p>有必要说明一下. 在lua中, <code>.</code>表示的是属性(变量), <code>:</code>表示的是方法(函数)的调用. </p><p>常见操作符都一样. 剩下的. 我们到案例里看. </p><h3 id="1-3-splash的http-api接口"><a href="#1-3-splash的http-api接口" class="headerlink" title="1.3 splash的http-api接口"></a>1.3 splash的http-api接口</h3><p>splash提供了对外的http-api接口. 我们可以像访问一个普通url一样访问splash. 并由splash帮助我们渲染好页面内容. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://<span class="number">192.168</span><span class="number">.31</span><span class="number">.82</span>:<span class="number">8050</span>/render.html?url=http://www.baidu.com</span><br></pre></td></tr></table></figure><p>虽然看不出任何差别. 但是你心里要清楚一个事情. 此时拿到的直接是经过js渲染后的html</p><p>我们换个url你就知道了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://<span class="number">192.168</span><span class="number">.31</span><span class="number">.82</span>:<span class="number">8050</span>/render.html?url=https://www.endata.com.cn/BoxOffice/BO/Year/index.html&amp;wait=<span class="number">5</span></span><br></pre></td></tr></table></figure><p>endata这个网站. 它的数据是后期经过ajax请求二次加载进来的. 我们通过splash可以等待它后期加载完再拿html. </p><p>综上, splash的工作机制:</p><p><img src="/images/scrapy02/image-20210817155156714.png" alt="image-20210817155156714"></p><p>整个一个代理服务器的逻辑. ~~~~</p><h2 id="二-python中使用splash"><a href="#二-python中使用splash" class="headerlink" title="二. python中使用splash"></a>二. python中使用splash</h2><h3 id="2-1-splash在python中如何使用"><a href="#2-1-splash在python中如何使用" class="headerlink" title="2.1 splash在python中如何使用"></a>2.1 splash在python中如何使用</h3><p>既然splash提供了http-api接口. 那我们就可以像请求普通网站一样去请求到splash.<br>在python中, 我们最熟悉的能发送http请求的东西就是requests了. </p><p>接下来.我们就用requests来完成splash的对接. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># splash提供的api接口</span></span><br><span class="line"><span class="string">渲染html的接口</span></span><br><span class="line"><span class="string">http://192.168.31.184:8050/render.html?url=你的url&amp;wait=等待时间&amp;time_out=超时时间</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">截图的接口</span></span><br><span class="line"><span class="string">http://192.168.31.184:8050/render.png  参数和render.html基本一致, 可选width, height</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">加载过程接口</span></span><br><span class="line"><span class="string">http://192.168.31.184:8050/render.har  参数和render.html基本一致</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">json接口</span></span><br><span class="line"><span class="string">http://192.168.31.184:8050/render.json  参数和render.html基本一致</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">执行lua脚本的接口</span></span><br><span class="line"><span class="string">http://192.168.31.184:8050/execute?lua_source=你要执行的lua脚本</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最简单的调用splash的render.html</span></span><br><span class="line">url = <span class="string">&quot;http://192.168.31.184:8050/render.html?url=https://www.baidu.com&amp;wait=5&quot;</span></span><br><span class="line">resp = requests.get(url)</span><br><span class="line"><span class="built_in">print</span>(resp.text)</span><br></pre></td></tr></table></figure><h3 id="2-2-我们以网易新闻首页要闻为例"><a href="#2-2-我们以网易新闻首页要闻为例" class="headerlink" title="2.2 我们以网易新闻首页要闻为例."></a>2.2 我们以网易新闻首页<code>要闻</code>为例.</h3><p>先搞定脚本部分.</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash, args)</span></span></span><br><span class="line">    <span class="built_in">assert</span>(splash:go(args.url))</span><br><span class="line">    <span class="built_in">assert</span>(splash:wait(<span class="number">1</span>))</span><br><span class="line">    <span class="comment">-- 加载一段js, 后面作为lua函数进行调用. </span></span><br><span class="line">    <span class="comment">-- 在这个脚本中, 主要返回了&quot;加载更多&quot;按钮的状态</span></span><br><span class="line">    get_display_style = splash:jsfunc(<span class="string">[[</span></span><br><span class="line"><span class="string">      function()&#123;</span></span><br><span class="line"><span class="string">        return document.getElementsByClassName(&#x27;load_more_btn&#x27;)[0].style.display;</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    ]]</span>)</span><br><span class="line">    <span class="comment">-- lua中的循环语句. 和python的while功能一样. </span></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>)</span><br><span class="line">    <span class="keyword">do</span>  <span class="comment">-- 语法规定. 相当于开始</span></span><br><span class="line">        <span class="comment">-- 直接运行js代码, 滚动到&#x27;加载更多&#x27;按钮</span></span><br><span class="line">        splash:runjs(<span class="string">&quot;document.getElementsByClassName(&#x27;load_more_btn&#x27;)[0].scrollIntoView(true)&quot;</span>)</span><br><span class="line">        <span class="comment">-- 等待</span></span><br><span class="line">        splash:wait(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">-- 找到该按钮. 点击它</span></span><br><span class="line">        splash:<span class="built_in">select</span>(<span class="string">&quot;.load_more_btn&quot;</span>).click()</span><br><span class="line">        <span class="comment">-- 调用上方预制的js脚本, 获取&#x27;正在加载按钮&#x27;的状态</span></span><br><span class="line">        display_style = get_display_style()</span><br><span class="line">        <span class="comment">-- 如果不显示了. 也就结束了</span></span><br><span class="line">        <span class="keyword">if</span>(display_style== <span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">        <span class="keyword">then</span></span><br><span class="line">            <span class="keyword">break</span>  <span class="comment">-- 同python中的break. 打断循环</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="built_in">assert</span>(splash:wait(<span class="number">2</span>)) <span class="comment">-- 不在乎多等2秒</span></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        html = splash:html(),</span><br><span class="line">        png = splash:png(),</span><br><span class="line">        har = splash:har(),</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>到了python里面就可以使用这个脚本了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 执行lua脚本</span></span><br><span class="line">lua = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">function main(splash, args)</span></span><br><span class="line"><span class="string">    assert(splash:go(args.url))</span></span><br><span class="line"><span class="string">    assert(splash:wait(0.5))</span></span><br><span class="line"><span class="string">    -- 加载一段js, 后面作为lua函数进行调用. </span></span><br><span class="line"><span class="string">    -- 在这个脚本中, 主要返回了&quot;加载更多&quot;按钮的状态</span></span><br><span class="line"><span class="string">    get_display_style = splash:jsfunc([[</span></span><br><span class="line"><span class="string">      function()&#123;</span></span><br><span class="line"><span class="string">        return document.getElementsByClassName(&#x27;load_more_btn&#x27;)[0].style.display;</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    ]])</span></span><br><span class="line"><span class="string">    -- lua中的循环语句. 和python的while功能一样. </span></span><br><span class="line"><span class="string">    while (true)</span></span><br><span class="line"><span class="string">    do  -- 语法规定. 相当于开始</span></span><br><span class="line"><span class="string">        -- 直接运行js代码, 滚动到&#x27;加载更多&#x27;按钮</span></span><br><span class="line"><span class="string">        splash:runjs(&quot;document.getElementsByClassName(&#x27;load_more_btn&#x27;)[0].scrollIntoView(true)&quot;)</span></span><br><span class="line"><span class="string">        -- 等待</span></span><br><span class="line"><span class="string">        splash:wait(1)</span></span><br><span class="line"><span class="string">        -- 找到该按钮. 点击它</span></span><br><span class="line"><span class="string">        splash:select(&quot;.load_more_btn&quot;).click()</span></span><br><span class="line"><span class="string">        -- 调用上方预制的js脚本, 获取&#x27;正在加载按钮&#x27;的状态</span></span><br><span class="line"><span class="string">        display_style = get_display_style()</span></span><br><span class="line"><span class="string">        -- 如果不显示了. 也就结束了</span></span><br><span class="line"><span class="string">        if(display_style== &#x27;none&#x27;)</span></span><br><span class="line"><span class="string">        then</span></span><br><span class="line"><span class="string">            break  -- 同python中的break. 打断循环</span></span><br><span class="line"><span class="string">        end</span></span><br><span class="line"><span class="string">    end</span></span><br><span class="line"><span class="string">    assert(splash:wait(2)) -- 不在乎多等2秒</span></span><br><span class="line"><span class="string">    return &#123;</span></span><br><span class="line"><span class="string">        html = splash:html(),    -- 拿到页面源代码</span></span><br><span class="line"><span class="string">        cookies = splash:get_cookies()  -- 拿到cookies</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备能够执行lua脚本的url  -&gt; splash服务地址</span></span><br><span class="line">url = <span class="string">&quot;http://192.168.31.82:8050/execute&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"><span class="comment"># 远程访问splash, 执行脚本</span></span><br><span class="line">resp = requests.get(url, params=&#123;</span><br><span class="line">    <span class="string">&quot;url&quot;</span>:<span class="string">&quot;https://news.163.com&quot;</span>,</span><br><span class="line">    <span class="string">&quot;lua_source&quot;</span>: lua</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">result = resp.json()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取结果</span></span><br><span class="line">tree = etree.HTML(result.get(<span class="string">&#x27;html&#x27;</span>))</span><br><span class="line"><span class="comment"># print(resp.text)</span></span><br><span class="line">divs = tree.xpath(<span class="string">&quot;//ul[@class=&#x27;newsdata_list fixed_bar_padding noloading&#x27;]/li[1]/div[2]/div&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> div <span class="keyword">in</span> divs:</span><br><span class="line">    a = div.xpath(<span class="string">&quot;./div/div/h3/a&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> a:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    a = a[<span class="number">0</span>]</span><br><span class="line">    href = a.xpath(<span class="string">&#x27;./@href&#x27;</span>)</span><br><span class="line">    title = a.xpath(<span class="string">&#x27;./text()&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(title, href)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result.get(<span class="string">&quot;cookies&quot;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>##三. scrapy_splash</p><p>安装scrapy_splash模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy_splash</span><br></pre></td></tr></table></figure><p>创建一个普通的scrapy项目, 然后把scrapy_splash配置到settings文件中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># scrapy_splash</span></span><br><span class="line"><span class="comment"># 渲染服务的url, 这里换成你自己的</span></span><br><span class="line">SPLASH_URL = <span class="string">&#x27;http://192.168.31.82:8050&#x27;</span></span><br><span class="line"><span class="comment"># 下载器中间件, 这个必须要配置</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">&#x27;scrapy_splash.SplashCookiesMiddleware&#x27;</span>: <span class="number">723</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy_splash.SplashMiddleware&#x27;</span>: <span class="number">725</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#x27;</span>: <span class="number">810</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个可由可无</span></span><br><span class="line"><span class="comment"># SPIDER_MIDDLEWARES = &#123;</span></span><br><span class="line"><span class="comment">#     &#x27;scrapy_splash.SplashDeduplicateArgsMiddleware&#x27;: 100,</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"><span class="comment"># 去重过滤器, 这个必须要配置</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">&#x27;scrapy_splash.SplashAwareDupeFilter&#x27;</span></span><br><span class="line"><span class="comment"># 使用Splash的Http缓存, 这个必须要配置</span></span><br><span class="line">HTTPCACHE_STORAGE = <span class="string">&#x27;scrapy_splash.SplashAwareFSCacheStorage&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>最后. 整理修改一下spider</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy_splash.request <span class="keyword">import</span> SplashRequest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># splash的lua脚本</span></span><br><span class="line">lua_source = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">function main(splash, args)  -- 主函数</span></span><br><span class="line"><span class="string">    assert(splash:go(&quot;https://news.163.com/&quot;))  -- 访问url</span></span><br><span class="line"><span class="string">    assert(splash:wait(2))  -- 等待</span></span><br><span class="line"><span class="string">    -- 预存一个js函数</span></span><br><span class="line"><span class="string">    get_btn_display = splash:jsfunc([[</span></span><br><span class="line"><span class="string">        function()&#123;</span></span><br><span class="line"><span class="string">            return document.getElementsByClassName(&#x27;load_more_btn&#x27;)[0].style.display;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    ]])</span></span><br><span class="line"><span class="string">    -- lua的while循环</span></span><br><span class="line"><span class="string">    while(true)</span></span><br><span class="line"><span class="string">    do</span></span><br><span class="line"><span class="string">        -- 直接执行一个js脚本</span></span><br><span class="line"><span class="string">        -- 向下拉动滚动条. </span></span><br><span class="line"><span class="string">        splash:runjs(&quot;document.getElementsByClassName(&#x27;load_more_btn&#x27;)[0].scrollIntoView(true)&quot;)</span></span><br><span class="line"><span class="string">        assert(splash:wait(1))</span></span><br><span class="line"><span class="string">        -- 选择 &quot;加载更多&quot;</span></span><br><span class="line"><span class="string">        btn = splash:select(&quot;.load_more_btn&quot;)</span></span><br><span class="line"><span class="string">        -- 点它</span></span><br><span class="line"><span class="string">        btn:click()</span></span><br><span class="line"><span class="string">        -- 判断是否可见 调用上方预制的js函数</span></span><br><span class="line"><span class="string">        ss = get_btn_display()</span></span><br><span class="line"><span class="string">        -- 如果是none. 就没有数据了(网易自己设计的)</span></span><br><span class="line"><span class="string">        if (ss == &#x27;none&#x27;)</span></span><br><span class="line"><span class="string">        then</span></span><br><span class="line"><span class="string">            break</span></span><br><span class="line"><span class="string">        end</span></span><br><span class="line"><span class="string">    end</span></span><br><span class="line"><span class="string">    return &#123;</span></span><br><span class="line"><span class="string">        html = splash:html(),</span></span><br><span class="line"><span class="string">        cookies = splash:get_cookies()</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    end</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WangyiSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;wangyi&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;163.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://news.163.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 发送splash请求</span></span><br><span class="line">        <span class="keyword">yield</span> SplashRequest(</span><br><span class="line">            url=self.start_urls[<span class="number">0</span>],</span><br><span class="line">            callback=self.parse,</span><br><span class="line">            endpoint=<span class="string">&quot;execute&quot;</span>,</span><br><span class="line">            args=&#123;<span class="string">&quot;lua_source&quot;</span>: lua_source, &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, resp, **kwargs</span>):</span><br><span class="line">        divs = resp.xpath(<span class="string">&quot;//ul[@class=&#x27;newsdata_list fixed_bar_padding noloading&#x27;]/li[1]/div[2]/div&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> divs:</span><br><span class="line">            a = div.xpath(<span class="string">&quot;./div/div/h3/a&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> a:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            href = a.xpath(<span class="string">&#x27;./@href&#x27;</span>).extract_first()</span><br><span class="line">            title = a.xpath(<span class="string">&#x27;./text()&#x27;</span>).extract_first()</span><br><span class="line">            <span class="built_in">print</span>(href)</span><br><span class="line">            <span class="comment"># 可以采用正常的抓取方案</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">                url=href,</span><br><span class="line">                callback=self.details</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">details</span>(<span class="params">self, resp</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;data.txt&quot;</span>, mode=<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(<span class="string">&quot;____&quot;</span>.join(resp.xpath(<span class="string">&quot;//div[@class=&#x27;post_body&#x27;]//p/text()&quot;</span>).extract()))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="爬虫开发" scheme="http://blog.ioimp.top/categories/%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91/"/>
    
    
    <category term="Scrapy爬虫" scheme="http://blog.ioimp.top/tags/Scrapy%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scarpy_分布式爬虫</title>
    <link href="http://blog.ioimp.top/2023/12/03/Scarpy-%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB/"/>
    <id>http://blog.ioimp.top/2023/12/03/Scarpy-%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB/</id>
    <published>2023-12-03T03:59:26.000Z</published>
    <updated>2023-12-03T04:04:23.779Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="分布式爬虫"><a href="#分布式爬虫" class="headerlink" title="分布式爬虫"></a>分布式爬虫</h1><h2 id="一-增量式爬虫"><a href="#一-增量式爬虫" class="headerlink" title="一. 增量式爬虫"></a>一. 增量式爬虫</h2><p>​增量式爬虫, 顾名思义. 可以对网站进行反复抓取. 然后发现新东西了就保存起来. 遇到了以前抓取过的内容就自动过滤掉即可. 其核心思想就两个字. 去重. 并且可以反复去重. 今天运行一下. 明天再运行一下. 将不同的数据过滤出来. 相同的数据去除掉(不保存)即可. </p><p>​此时, 我们以天涯为目标来尝试一下完成增量式爬虫. </p><p>spider: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> redis <span class="keyword">import</span> Redis</span><br><span class="line"><span class="keyword">from</span> tianya.items <span class="keyword">import</span> TianyaItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TySpider</span>(scrapy.Spider):</span><br><span class="line"></span><br><span class="line">    name = <span class="string">&#x27;ty&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;tianya.cn&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://bbs.tianya.cn/list-worldlook-1.shtml&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">        self.red = Redis(password=<span class="string">&quot;123456&quot;</span>, db=<span class="number">6</span>, decode_responses=<span class="literal">True</span>)</span><br><span class="line">        <span class="built_in">super</span>().__init__(name, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, resp, **kwargs</span>):</span><br><span class="line">        tbodys = resp.css(<span class="string">&quot;.tab-bbs-list tbody&quot;</span>)[<span class="number">1</span>:]</span><br><span class="line">        <span class="keyword">for</span> tbody <span class="keyword">in</span> tbodys:</span><br><span class="line">            hrefs = tbody.xpath(<span class="string">&quot;./tr/td[1]/a/@href&quot;</span>).extract()</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> hrefs:</span><br><span class="line">                <span class="comment"># 两个方案.</span></span><br><span class="line">                url = resp.urljoin(h)</span><br><span class="line">                <span class="comment"># 判断是否在该set集合中有数据</span></span><br><span class="line">                r = self.red.sismember(<span class="string">&quot;tianya:details&quot;</span>, url)  </span><br><span class="line">                <span class="comment">#   1. url去重. 优点: 简单, 缺点: 如果有人回复了帖子.就无法提取到最新的数据了</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> r:</span><br><span class="line">                    <span class="keyword">yield</span> scrapy.Request(url=resp.urljoin(h), callback=self.parse_details)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f&quot;该url已经被抓取过<span class="subst">&#123;url&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        next_href = resp.xpath(<span class="string">&quot;//div[@class=&#x27;short-pages-2 clearfix&#x27;]/div[@class=&#x27;links&#x27;]/a[last()]/@href&quot;</span>).extract_first()</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url=resp.urljoin(next_href), callback=self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_details</span>(<span class="params">self, resp, **kwargs</span>):</span><br><span class="line">        title = resp.xpath(<span class="string">&#x27;//*[@id=&quot;post_head&quot;]/h1/span[1]/span/text()&#x27;</span>).extract_first()</span><br><span class="line">        content = resp.xpath(<span class="string">&#x27;//*[@id=&quot;bd&quot;]/div[4]/div[1]/div/div[2]/div[1]/text()&#x27;</span>).extract_first()</span><br><span class="line">        item = TianyaItem()</span><br><span class="line">        item[<span class="string">&#x27;title&#x27;</span>] = title</span><br><span class="line">        item[<span class="string">&#x27;content&#x27;</span>] = content</span><br><span class="line">        <span class="comment"># 提取完数据. 该url进入redis</span></span><br><span class="line">        self.red.sadd(<span class="string">&quot;tianya:details&quot;</span>, resp.url)  </span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>​pipelines</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># useful for handling different item types with a single interface</span></span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"><span class="keyword">from</span> redis <span class="keyword">import</span> Redis</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TianyaPipeline</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="comment">#   2. 数据内容去重. 优点: 保证数据的一致性. 缺点: 需要每次都把数据从网页中提取出来</span></span><br><span class="line">        <span class="built_in">print</span>(json.dumps(<span class="built_in">dict</span>(item)))</span><br><span class="line">        r = self.red.sadd(<span class="string">&quot;tianya:pipelines:items&quot;</span>, json.dumps(<span class="built_in">dict</span>(item)))</span><br><span class="line">        <span class="keyword">if</span> r:</span><br><span class="line">            <span class="comment"># 进入数据库</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;存入数据库&quot;</span>, item[<span class="string">&#x27;title&#x27;</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;已经在数据里了&quot;</span>, item[<span class="string">&#x27;title&#x27;</span>])</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.red = Redis(password=<span class="string">&quot;123456&quot;</span>, db=<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.red.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>上述方案是直接用redis进行的去重. 我们还可以选择使用数据库, mongodb进行过滤. 原理都一样, 不在赘述. </p><h2 id="二-分布式爬虫"><a href="#二-分布式爬虫" class="headerlink" title="二. 分布式爬虫"></a>二. 分布式爬虫</h2><p>​分布式爬虫, 就是搭建一个分布式的集群, 让其对一组资源进行分布联合爬取. </p><p>​既然要集群来抓取. 意味着会有好几个爬虫同时运行. 那此时就非常容易产生这样一个问题. 如果有重复的url怎么办?  在原来的程序中. scrapy中会由调度器来自动完成这个任务. 但是, 此时是多个爬虫一起跑. 而我们又知道不同的机器之间是不能直接共享调度器的. 怎么办? 我们可以采用redis来作为各个爬虫的调度器. 此时我们引出一个新的模块叫scrapy-redis. 在该模块中提供了这样一组操作. 它们重写了scrapy中的调度器. 并将调度队列和去除重复的逻辑全部引入到了redis中. 这样就形成了这样一组结构</p><p><img src="/images/scrapy02/image-20210812152215427.png" alt="image-20210812152215427"></p><p>​整体工作流程:</p><pre><code>1. 某个爬虫从redis_key获取到起始url. 传递给引擎, 到调度器. 然后把起始url直接丢到redis的请求队列里. 开始了scrapy的爬虫抓取工作.  2. 如果抓取过程中产生了新的请求. 不论是哪个节点产生的, 最终都会到redis的去重集合中进行判定是否抓取过. 3. 如果抓取过. 直接就放弃该请求. 如果没有抓取过. 自动丢到redis请求队列中. 4. 调度器继续从redis请求队列里获取要进行抓取的请求. 完成爬虫后续的工作. </code></pre><p>接下来. 我们用scrapy-redis完成上述流程</p><ol><li><p>首先, 创建项目, 和以前一样, 该怎么创建还怎么创建. </p></li><li><p>修改Spider. 将start_urls注释掉. 更换成redis_key</p></li><li><p>然后再settings中对redis以及scrapy_redis配置一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">REDIS_HOST = <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">REDIS_PORT = <span class="number">6379</span></span><br><span class="line">REDIS_DB = <span class="number">8</span></span><br><span class="line">REDIS_PARAMS = &#123;</span><br><span class="line">    <span class="string">&quot;password&quot;</span>:<span class="string">&quot;123456&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># scrapy-redis配置信息  # 固定的</span></span><br><span class="line">SCHEDULER = <span class="string">&quot;scrapy_redis.scheduler.Scheduler&quot;</span></span><br><span class="line">SCHEDULER_PERSIST = <span class="literal">True</span>  <span class="comment"># 如果为真. 在关闭时自动保存请求信息, 如果为假, 则不保存请求信息</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">&quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span> <span class="comment"># 去重的逻辑. 要用redis的</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;tianya2.pipelines.Tianya2Pipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy_redis.pipelines.RedisPipeline&#x27;</span>: <span class="number">301</span>  <span class="comment"># 配置redis的pipeline</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><p>布隆过滤器:</p><p>​平时, 我们如果需要对数据进行去重操作可以有以下方案: </p><pre><code>1. 直接用set集合来存储url. (最low的方案)2. 用set集合存储hash过的url. scrapy默认3. 用redis来存储hash过的请求, scrapy-redis默认就是这样做的. 如果请求非常非常多. redis压力是很大的.4. 用布隆过滤器. </code></pre><p>布隆过滤器的原理: 其实它里面就是一个改良版的bitmap. 何为bitmap, 假设我提前准备好一个数组, 然后把源数据经过hash计算. 会计算出一个数字. 我们按照下标来找到该下标对应的位置. 然后设置成1. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = 李嘉诚</span><br><span class="line">b = 张翠山</span><br><span class="line">....</span><br><span class="line"></span><br><span class="line">[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>]  <span class="number">10</span>个长度数组</span><br><span class="line"></span><br><span class="line"><span class="built_in">hash</span>(a) = <span class="number">3</span></span><br><span class="line"><span class="built_in">hash</span>(b) = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>] </span><br><span class="line"><span class="built_in">hash</span>(张三) = <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 找的时候依然执行该hash算法. 然后直接去找对应下标的位置看看是不是1. 是1就有, 不是1就没有</span></span><br></pre></td></tr></table></figure><p>这样有个不好的现象. 容易误判. 如果hash算法选的不够好. 很容易搞错. 那怎么办. 多选几个hash算法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">a = 李嘉诚</span><br><span class="line">b = 张翠山</span><br><span class="line"></span><br><span class="line">[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">hash1(a) = <span class="number">3</span></span><br><span class="line">hash2(a) = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">hash1(b) = <span class="number">2</span></span><br><span class="line">hash2(b) = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找的时候, 重新按照这个hash的顺序, 在重新执行一遍. 依然会得到2个值. 分别去这两个位置看是否是1. 如果全是1, 就有,  如果有一个是0, 就没有. </span></span><br></pre></td></tr></table></figure><p>在scrapy-redis中想要使用布隆过滤器是非常简单的. 你可以自己去写这个布隆过滤器的逻辑. 不过我建议直接用第三方的就可以了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装布隆过滤器</span></span><br><span class="line">pip install scrapy_redis_bloomfilter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去重类，要使用 BloomFilter 请替换 DUPEFILTER_CLASS</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">&quot;scrapy_redis_bloomfilter.dupefilter.RFPDupeFilter&quot;</span></span><br><span class="line"><span class="comment"># 哈希函数的个数，默认为 6，可以自行修改</span></span><br><span class="line">BLOOMFILTER_HASH_NUMBER = <span class="number">6</span></span><br><span class="line"><span class="comment"># BloomFilter 的 bit 参数，默认 30，占用 128MB 空间，去重量级 1 亿</span></span><br><span class="line">BLOOMFILTER_BIT = <span class="number">30</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="爬虫开发" scheme="http://blog.ioimp.top/categories/%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91/"/>
    
    
    <category term="Scrapy爬虫" scheme="http://blog.ioimp.top/tags/Scrapy%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>crawlSpider</title>
    <link href="http://blog.ioimp.top/2023/12/03/crawlSpider/"/>
    <id>http://blog.ioimp.top/2023/12/03/crawlSpider/</id>
    <published>2023-12-03T03:58:31.000Z</published>
    <updated>2023-12-03T03:59:08.333Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Scrapy抓取全网站数据"><a href="#Scrapy抓取全网站数据" class="headerlink" title="Scrapy抓取全网站数据"></a>Scrapy抓取全网站数据</h1><h2 id="一-使用常规Spider"><a href="#一-使用常规Spider" class="headerlink" title="一. 使用常规Spider"></a>一. 使用常规Spider</h2><p>我们把目光对准汽车之家. 抓取二手车信息.</p><p>注意, 汽车之家的访问频率要控制一下. 要不然会跳验证的. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOAD_DELAY = <span class="number">3</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ErshouSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;ershou&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;che168.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.che168.com/beijing/a0_0msdgscncgpi1ltocsp100exx0/?pvareaid=102179#currengpostion&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, resp, **kwargs</span>):</span><br><span class="line">        <span class="comment"># print(resp.text)</span></span><br><span class="line">        <span class="comment"># 链接提取器</span></span><br><span class="line">        le = LinkExtractor(restrict_xpaths=(<span class="string">&quot;//ul[@class=&#x27;viewlist_ul&#x27;]/li/a&quot;</span>,), deny_domains=(<span class="string">&quot;topicm.che168.com&quot;</span>,) )</span><br><span class="line">        links = le.extract_links(resp)</span><br><span class="line">        <span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">                url=link.url,</span><br><span class="line">                callback=self.parse_detail</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 翻页功能</span></span><br><span class="line">        le2 = LinkExtractor(restrict_xpaths=(<span class="string">&quot;//div[@id=&#x27;listpagination&#x27;]/a&quot;</span>,))</span><br><span class="line">        pages = le2.extract_links(resp)</span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> pages:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=page.url, callback=self.parse_detail)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_detail</span>(<span class="params">self, resp, **kwargs</span>):</span><br><span class="line">        title = resp.xpath(<span class="string">&#x27;/html/body/div[5]/div[2]/h3/text()&#x27;</span>).extract_first()</span><br><span class="line">        <span class="built_in">print</span>(title)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>LinkExtractor: 链接提取器. 可以非常方便的帮助我们从一个响应页面中提取到url链接. 我们只需要提前定义好规则即可. </p><p>参数: </p><p>​allow, 接收一堆正则表达式, 可以提取出符合该正则的链接<br>​deny, 接收一堆正则表达式, 可以剔除符合该正则的链接<br>​allow_domains: 接收一堆域名, 符合里面的域名的链接被提取<br>​deny_domains: 接收一堆域名, 剔除不符合该域名的链接<br>​restrict_xpaths: 接收一堆xpath, 可以提取符合要求xpath的链接<br>​restrict_css: 接收一堆css选择器, 可以提取符合要求的css选择器的链接<br>​tags: 接收一堆标签名, 从某个标签中提取链接, 默认a, area<br>​attrs: 接收一堆属性名, 从某个属性中提取链接, 默认href</p><p>值得注意的, &#x3D;&#x3D;在提取到的url中, 是有重复的内容的. 但是我们不用管. scrapy会自动帮我们过滤掉重复的url请求.&#x3D;&#x3D; </p><h2 id="二-使用CrawlSpider"><a href="#二-使用CrawlSpider" class="headerlink" title="二. 使用CrawlSpider"></a>二. 使用CrawlSpider</h2><p>在scrapy中提供了CrawlSpider来完成全站数据抓取. </p><ol><li><p>创建项目</p><p><code>scrapy startproject qichezhijia</code></p></li><li><p>进入项目</p><p><code>cd qichezhijia</code></p></li><li><p>创建爬虫(CrawlSpider)</p><p><code>scrapy genspider </code>&#x3D;&#x3D;-t crawl&#x3D;&#x3D;<code> ershouche che168.com</code></p><p>和以往的爬虫不同. 该爬虫需要用到crawl的模板来创建爬虫. </p></li><li><p>修改spider中的rules和回调函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ErshoucheSpider</span>(<span class="title class_ inherited__">CrawlSpider</span>):</span><br><span class="line">    name = <span class="string">&#x27;ershouche&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;che168.com&#x27;</span>, <span class="string">&#x27;autohome.com.cn&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.che168.com/beijing/a0_0msdgscncgpi1ltocsp1exx0/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    le = LinkExtractor(restrict_xpaths=(<span class="string">&quot;//ul[@class=&#x27;viewlist_ul&#x27;]/li/a&quot;</span>,), deny_domains=(<span class="string">&quot;topicm.che168.com&quot;</span>,) )</span><br><span class="line">    le1 = LinkExtractor(restrict_xpaths=(<span class="string">&quot;//div[@id=&#x27;listpagination&#x27;]/a&quot;</span>,))</span><br><span class="line">    rules = (</span><br><span class="line">        Rule(le1, follow=<span class="literal">True</span>),  <span class="comment"># 单纯为了做分页</span></span><br><span class="line">        Rule(le, callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">False</span>), <span class="comment"># 单纯提取数据</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_item</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="built_in">print</span>(response.url)</span><br></pre></td></tr></table></figure><p>CrawlSpider的工作流程. </p><p>前期和普通的spider是一致的. 在第一次请求回来之后. 会自动的将返回的response按照rules中订制的规则来提取链接. 并进一步执行callback中的回调. 如果follow是True, 则继续在响应的内容中继续使用该规则提取链接.  相当于在parse中的scrapy.request(xxx, callback&#x3D;self.parse)</p></li></ol><h2 id="三-Redis简单使用"><a href="#三-Redis简单使用" class="headerlink" title="三. Redis简单使用"></a>三. Redis简单使用</h2><p>​redis作为一款目前这个星球上性能最高的非关系型数据库之一. 拥有每秒近十万次的读写能力. 其实力只能用恐怖来形容. </p><ol><li><p>安装redis</p><p>redis是我见过这个星球上最好安装的软件了. 比起前面的那一坨. 它简直了…</p><p>直接把压缩包解压. 然后配置一下环境变量就可以了. </p><p><img src="/images/scrapy01/image-20210810184227132.png" alt="images/scrapy01/image-20210810184227132"></p><p><img src="/images/scrapy01/image-20210810184318301.png" alt="images/scrapy01/image-20210810184318301"></p><p>接下来, 在环境变量中将该文件夹配置到path中. </p><p><img src="/images/scrapy01/image-20210810184649037.png" alt="images/scrapy01/image-20210810184649037"></p><p>win7的同学自求多福吧…</p><p>我们给redis多配置几个东西(修改redis的配置文件, mac是: redis.conf, windows是: )</p><ol><li><p>关闭bind</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bind 127.0.0.1 ::1  # 注释掉它</span></span><br></pre></td></tr></table></figure></li><li><p>关闭保护模式  windows不用设置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">protected-mode no    <span class="comment"># 设置为no</span></span><br></pre></td></tr></table></figure></li><li><p>设置密码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requirepass <span class="number">123456</span>   <span class="comment"># 设置密码</span></span><br></pre></td></tr></table></figure></li></ol><p>将redis怼到windows服务&#x3D;&#x3D;必须进入到redis目录后才可以&#x3D;&#x3D;</p><p><img src="/images/scrapy01/image-20210810185306517.png" alt="images/scrapy01/image-20210810185306517"></p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 将redis安装到windows服务</span><br><span class="line">redis-server.exe --service-install redis.windows.conf --loglevel verbose</span><br><span class="line"># 卸载服务：</span><br><span class="line">redis-server --service-uninstall</span><br><span class="line"># 开启服务：</span><br><span class="line">redis-server --service-<span class="built_in">start</span></span><br><span class="line"># 停止服务：</span><br><span class="line">redis-server --service-stop</span><br></pre></td></tr></table></figure><p>使用redis-cli链接redis</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">redis-cli -h ip地址 -p 端口 --raw   <span class="comment"># raw可以让redis显示出中文</span></span><br><span class="line">auth 密码   <span class="comment"># 如果有密码可以这样来登录, 如果没有.不用这一步</span></span><br></pre></td></tr></table></figure><p><img src="/images/scrapy01/image-20210810185605290.png" alt="images/scrapy01/image-20210810185605290"></p><p>附赠RDM, redis desktop manager. 可以帮我们完成redis数据库的可视化操作(需要就装, 不需要就算)</p><p><img src="/images/scrapy01/image-20210810185659813.png" alt="images/scrapy01/image-20210810185659813"></p></li><li><p>redis常见数据类型</p><p>redis中常见的数据类型有5个. </p><p>命令规则:  <code>命令 key 参数</code></p><ol><li><p>string</p><p>字符串(它自己认为是字符串, 我认为是任何东西. ), redis最基础的数据类型. </p><p>常用命令</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> key value  <span class="comment"># 添加一条数据</span></span><br><span class="line">get key   <span class="comment"># 查看一条数据</span></span><br><span class="line">incr key       <span class="comment"># 让该key对应的数据自增1(原子性, 安全)</span></span><br><span class="line">incrby key count     <span class="comment"># 让该key对应的value自增 count </span></span><br><span class="line"><span class="built_in">type</span> key<span class="comment"># 查看数据类型(set进去的东西一律全是字符串)</span></span><br></pre></td></tr></table></figure><p>例如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> name zhangsan  <span class="comment"># 添加数据  name = zhangsan</span></span><br><span class="line">get name<span class="comment"># 查看数据 zhangsan</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> age <span class="number">10</span></span><br><span class="line">get age <span class="comment"># 10</span></span><br><span class="line">incr age<span class="comment"># 11</span></span><br><span class="line">get age <span class="comment"># 11</span></span><br><span class="line">incrby age <span class="number">5</span><span class="comment"># 16</span></span><br></pre></td></tr></table></figure></li><li><p>hash</p><p>哈希, 相当于字典. </p><p>常见操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hset key k1 v1   <span class="comment"># 将k1, v1存储在key上</span></span><br><span class="line">hget key k1      <span class="comment"># 将key上的k1提取出来</span></span><br><span class="line">hmset key k1 v1 k2 v2 k3 v3....  <span class="comment"># 一次性将多个k,v存储在key</span></span><br><span class="line">hmget key k1 k2....<span class="comment"># 一次性将key中的k1, k2...提取出来</span></span><br><span class="line">hgetall key <span class="comment"># 一次性将key中所有内容全部提取</span></span><br><span class="line">hkeys key<span class="comment"># 将key中所有的k全部提取</span></span><br><span class="line">hvals key <span class="comment"># 将key中所有的v全部提取</span></span><br></pre></td></tr></table></figure><p>示例:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HMSET stu <span class="built_in">id</span> <span class="number">1</span> name sylar age <span class="number">18</span></span><br><span class="line">HMGET stu name age   <span class="comment"># syalr 18</span></span><br><span class="line">HGETALL stu    <span class="comment"># id 1 name sylar age 18</span></span><br><span class="line">HKEYS stu <span class="comment"># id name age</span></span><br><span class="line">HVALS stu   <span class="comment"># 1 syalr 18</span></span><br></pre></td></tr></table></figure></li><li><p>list</p><p>列表, 底层是一个双向链表. 可以从左边和右边进行插入. 记住每次插入都要记得这货是个&#x3D;&#x3D;双向链表&#x3D;&#x3D;</p><p>常见操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">LPUSH key 数据<span class="number">1</span> 数据<span class="number">2</span> 数据<span class="number">3.</span>... <span class="comment"># 从左边插入数据</span></span><br><span class="line">RPUSH key 数据<span class="number">1</span> 数据<span class="number">2</span> 数据<span class="number">3.</span>... <span class="comment"># 从右边插入数据</span></span><br><span class="line">LRANGE key start stop     <span class="comment"># 从start到stop提取数据. </span></span><br><span class="line"></span><br><span class="line">LLEN key<span class="comment"># 返回key对应列表的长度</span></span><br><span class="line">LPOP key        <span class="comment"># 从左边删除一个.并返回被删除元素</span></span><br><span class="line">RPOP key<span class="comment"># 从右边删除一个.并返回被删除元素</span></span><br></pre></td></tr></table></figure><p>示例:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">LPUSH banji yiban erban sanban siban</span><br><span class="line">LRANGE banji <span class="number">0</span> -<span class="number">1</span>   <span class="comment"># yiban erban sanban siban</span></span><br><span class="line">RPUSH ban ban1 ban2 ban3</span><br><span class="line">LRANGE ban <span class="number">0</span> -<span class="number">1</span>     <span class="comment"># ban1 ban2 ban3</span></span><br><span class="line">LPOP ban  <span class="comment"># ban1</span></span><br><span class="line">LLEN key  <span class="comment"># 2</span></span><br></pre></td></tr></table></figure></li><li><p>set</p><p>set是无序的超大集合. 无序, 不重复. </p><p>常见操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SADD key 值   <span class="comment"># 向集合内存入数据</span></span><br><span class="line">SMEMBERS key  <span class="comment"># 查看集合内所有元素</span></span><br><span class="line">SCARD key <span class="comment"># 查看key中元素的个数</span></span><br><span class="line">SISMEMBER key val  <span class="comment"># 查看key中是否包含val</span></span><br><span class="line">SUNION key1 key2  <span class="comment"># 并集</span></span><br><span class="line">SDIFF key1 key2  <span class="comment"># 差集合, 在key1中, 但不在key2中的数据</span></span><br><span class="line">SINTER key1 key2 <span class="comment"># 计算交集, 在key1和key2中都出现了的</span></span><br><span class="line">SPOP key  <span class="comment"># 随机从key中删除一个数据</span></span><br><span class="line">SRANDMEMBER key count <span class="comment"># 随机从key中查询count个数据</span></span><br></pre></td></tr></table></figure><p>实例:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">SADD stars 柯震东 吴亦凡 张默 房祖名   <span class="comment"># 4</span></span><br><span class="line">SADD stars 吴亦凡    <span class="comment"># 0. 重复的数据是存储不进去的.</span></span><br><span class="line">SMEMBERS stars   <span class="comment"># 柯震东 吴亦凡 张默 房祖名</span></span><br><span class="line">SISMEMBER stars 吴亦凡  <span class="comment"># 吴亦凡在 stars里么?  1 在  0 不在</span></span><br><span class="line"></span><br><span class="line">SADD my 周杰伦 吴亦凡 房祖名  </span><br><span class="line">SINTER stars my  <span class="comment"># 计算交集  吴亦凡 房祖名</span></span><br><span class="line"></span><br><span class="line">SPOP my  <span class="comment"># 随机删除一个</span></span><br><span class="line">SRANDMEMEBER my <span class="number">2</span>   <span class="comment"># 从集合总随机查看2个</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>zset</p><p>有序集合, 有序集合中的内容也是不可以重复的. 并且存储的数据也是redis最基础的string数据. 但是在存储数据的同时还增加了一个score. 表示分值. redis就是通过这个score作为排序的规则的. </p><p>常用操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ZADD key s1 m1 s2 m2 ... <span class="comment"># 向key中存入 m1 m2 分数分别为s1 s2</span></span><br><span class="line">ZRANGE key start stop [withscores]   <span class="comment"># 查看从start 到stop中的所有数据 [是否要分数]</span></span><br><span class="line">ZREVRANGE key start stop <span class="comment"># 倒叙查看start到stop的数据</span></span><br><span class="line">ZCARD key   <span class="comment"># 查看zset的数据个数</span></span><br><span class="line">ZCOUNT key <span class="built_in">min</span> <span class="built_in">max</span>  <span class="comment"># 查看分数在min和max之间的数据量</span></span><br><span class="line">ZINCRBY key score member  <span class="comment"># 将key中member的分值score</span></span><br><span class="line">ZSCORE key m  <span class="comment"># 查看key中m的分值</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>示例:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ZADD fam <span class="number">1</span> sylar <span class="number">2</span> alex <span class="number">3</span> tory  <span class="comment"># 添加三个数据</span></span><br><span class="line">ZRANGE fam <span class="number">0</span> -<span class="number">1</span> WITHSCORES <span class="comment"># 正序查看</span></span><br><span class="line">ZREVRANGE fam <span class="number">0</span> -<span class="number">1</span> WITHSCORES   <span class="comment"># 倒叙查看</span></span><br><span class="line">ZINCRBY fam <span class="number">10</span> alex  <span class="comment"># 给alex加10分</span></span><br><span class="line">ZADD fam <span class="number">100</span> alex   <span class="comment"># 给alex修改分数为100分</span></span><br><span class="line">ZSCORE fam alex   <span class="comment"># 查看alex的分数</span></span><br><span class="line">ZCARD fam    <span class="comment"># 查看fam的数据个数</span></span><br></pre></td></tr></table></figure></li></ol><p>redis还有非常非常多的操作. 我们就不一一列举了. 各位可以在网络上找到非常多的资料. </p><p>&#x3D;&#x3D;各位大佬们注意. 数据保存完一定要save一下, 避免数据没有写入硬盘而产生的数据丢失&#x3D;&#x3D;</p></li></ol><h2 id="四-python搞定redis"><a href="#四-python搞定redis" class="headerlink" title="四. python搞定redis"></a>四. python搞定redis</h2><p>​python处理redis使用专用的redis模块. 同样的, 它也是一个第三方库.</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install redis</span><br></pre></td></tr></table></figure><p>​获取连接(1)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> redis <span class="keyword">import</span> Redis</span><br><span class="line"></span><br><span class="line">red = Redis(host=<span class="string">&quot;127.0.0.1&quot;</span>,  <span class="comment"># 地址</span></span><br><span class="line">            port=<span class="number">6379</span>,   <span class="comment"># 端口</span></span><br><span class="line">            db=<span class="number">0</span>,   <span class="comment"># 数据库</span></span><br><span class="line">            password=<span class="number">123456</span>,  <span class="comment"># 密码</span></span><br><span class="line">            decode_responses=<span class="literal">True</span>)  <span class="comment"># 是否自动解码</span></span><br></pre></td></tr></table></figure><p>​获取连接(2)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">pool = redis.ConnectionPool(</span><br><span class="line">        host=<span class="string">&quot;127.0.0.1&quot;</span>,  <span class="comment"># 地址</span></span><br><span class="line">        port=<span class="number">6379</span>,   <span class="comment"># 端口</span></span><br><span class="line">        db=<span class="number">0</span>,   <span class="comment"># 数据库</span></span><br><span class="line">        password=<span class="number">123456</span>,  <span class="comment"># 密码</span></span><br><span class="line">        decode_responses=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">r = redis.Redis(connection_pool=pool)</span><br><span class="line"><span class="built_in">print</span>(r.keys())</span><br></pre></td></tr></table></figure><p>​我们以一个免费代理IP池能用到的操作来尝试一下redis</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 存入数据</span></span><br><span class="line">red.<span class="built_in">set</span>(<span class="string">&quot;sylar&quot;</span>, <span class="string">&quot;邱彦涛&quot;</span>)</span><br><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line"><span class="built_in">print</span>(red.get(<span class="string">&quot;sylar&quot;</span>))</span><br><span class="line"></span><br><span class="line">lst = [<span class="string">&quot;张三丰&quot;</span>, <span class="string">&quot;张无忌&quot;</span>, <span class="string">&quot;张翠山&quot;</span>, <span class="string">&quot;张娜拉&quot;</span>]</span><br><span class="line">red.lpush(<span class="string">&quot;names&quot;</span>, *lst)  <span class="comment"># 将所有的名字都存入names</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 查询所有数据</span></span><br><span class="line">result = red.lrange(<span class="string">&quot;names&quot;</span>, <span class="number">0</span>, -<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从上面的操作上可以看出. python中的redis和redis-cli中的操作是几乎一样的</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来, 咱们站在一个代理IP池的角度来分析各个功能</span></span><br><span class="line"><span class="comment"># 抓取到了IP. 保存入库</span></span><br><span class="line">red.zadd(<span class="string">&quot;proxy&quot;</span>, &#123;<span class="string">&quot;192.168.1.1&quot;</span>: <span class="number">10</span>, <span class="string">&quot;192.168.1.2&quot;</span>: <span class="number">10</span>&#125;)</span><br><span class="line">red.zadd(<span class="string">&quot;proxy&quot;</span>, &#123;<span class="string">&quot;192.168.1.3&quot;</span>: <span class="number">10</span>, <span class="string">&quot;192.168.1.6&quot;</span>: <span class="number">10</span>&#125;)</span><br><span class="line">red.zadd(<span class="string">&quot;proxy&quot;</span>, &#123;<span class="string">&quot;192.168.1.4&quot;</span>: <span class="number">10</span>, <span class="string">&quot;192.168.1.7&quot;</span>: <span class="number">10</span>&#125;)</span><br><span class="line">red.zadd(<span class="string">&quot;proxy&quot;</span>, &#123;<span class="string">&quot;192.168.1.5&quot;</span>: <span class="number">10</span>, <span class="string">&quot;192.168.1.8&quot;</span>: <span class="number">10</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给某一个ip增加到100分</span></span><br><span class="line">red.zadd(<span class="string">&quot;proxy&quot;</span>, &#123;<span class="string">&quot;192.168.1.4&quot;</span>: <span class="number">100</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给&quot;192.168.1.4&quot; 扣10分</span></span><br><span class="line">red.zincrby(<span class="string">&quot;proxy&quot;</span>, -<span class="number">10</span>, <span class="string">&quot;192.168.1.4&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分扣没了. 删除掉它</span></span><br><span class="line">red.zrem(<span class="string">&quot;proxy&quot;</span>, <span class="string">&quot;192.168.1.4&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可用的代理数量</span></span><br><span class="line">c = red.zcard(<span class="string">&quot;proxy&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="comment"># 根据分值进行查询(0~100)之间</span></span><br><span class="line">r = red.zrangebyscore(<span class="string">&quot;proxy&quot;</span>, <span class="number">0</span>, <span class="number">100</span>)</span><br><span class="line"><span class="built_in">print</span>(r)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询前100个数据(分页查询)</span></span><br><span class="line">r = red.zrevrange(<span class="string">&#x27;proxy&#x27;</span>, <span class="number">0</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断proxy是否存在, 如果是None就是不存在</span></span><br><span class="line">r = red.zscore(<span class="string">&quot;proxy&quot;</span>, <span class="string">&quot;192.168.1.4&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(r)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="爬虫开发" scheme="http://blog.ioimp.top/categories/%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91/"/>
    
    
    <category term="Scrapy爬虫" scheme="http://blog.ioimp.top/tags/Scrapy%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy_模拟登录与中间件</title>
    <link href="http://blog.ioimp.top/2023/12/03/Scrapy-%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%E4%B8%8E%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    <id>http://blog.ioimp.top/2023/12/03/Scrapy-%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%E4%B8%8E%E4%B8%AD%E9%97%B4%E4%BB%B6/</id>
    <published>2023-12-03T03:57:12.000Z</published>
    <updated>2023-12-03T03:58:19.284Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="模拟登录与中间件"><a href="#模拟登录与中间件" class="headerlink" title="模拟登录与中间件"></a>模拟登录与中间件</h1><h2 id="一-Scrapy处理cookie"><a href="#一-Scrapy处理cookie" class="headerlink" title="一. Scrapy处理cookie"></a>一. Scrapy处理cookie</h2><p>​在requests中我们讲解处理cookie主要有两个方案. 第一个方案. 从浏览器里直接把cookie搞出来. 贴到heades里. 这种方案, 简单粗暴. 第二个方案是走正常的登录流程. 通过session来记录请求过程中的cookie. 那么到了scrapy中如何处理cookie?  其实也是这两个方案. </p><p>​首先, 我们依然是把目标定好,  还是我们的老朋友, <a href="https://user.17k.com/ck/author/shelf?page=1&appKey=2406394919">https://user.17k.com/ck/author/shelf?page=1&amp;appKey=2406394919</a></p><p>​这个url必须要登录后才能访问(用户书架). &#x3D;&#x3D;对于该网页而言&#x3D;&#x3D;, 就必须要用到cookie了. 首先, 创建项目, 建立爬虫. 把该填的地方填上. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request, FormRequest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LoginSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;login&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;17k.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://user.17k.com/ck/author/shelf?page=1&amp;appKey=2406394919&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="built_in">print</span>(response.text)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>​此时运行时, 显示的是该用户还未登录. 不论是哪个方案. 在请求到start_urls里面的url之前必须得获取到cookie. 但是默认情况下, scrapy会自动的帮我们完成其实request的创建. 此时, 我们需要自己去组装第一个请求. 这时就需要我们自己的爬虫中重写start_requests()方法. 该方法负责起始request的组装工作. 我们不妨先看看原来的start_requests()是如何工作的. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下是scrapy源码</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">    cls = self.__class__</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.start_urls <span class="keyword">and</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;start_url&#x27;</span>):</span><br><span class="line">        <span class="keyword">raise</span> AttributeError(</span><br><span class="line">            <span class="string">&quot;Crawling could not start: &#x27;start_urls&#x27; not found &quot;</span></span><br><span class="line">            <span class="string">&quot;or empty (but found &#x27;start_url&#x27; attribute instead, &quot;</span></span><br><span class="line">            <span class="string">&quot;did you miss an &#x27;s&#x27;?)&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> method_is_overridden(cls, Spider, <span class="string">&#x27;make_requests_from_url&#x27;</span>):</span><br><span class="line">        warnings.warn(</span><br><span class="line">            <span class="string">&quot;Spider.make_requests_from_url method is deprecated; it &quot;</span></span><br><span class="line">            <span class="string">&quot;won&#x27;t be called in future Scrapy releases. Please &quot;</span></span><br><span class="line">            <span class="string">&quot;override Spider.start_requests method instead (see %s.%s).&quot;</span> % (</span><br><span class="line">                cls.__module__, cls.__name__</span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls:</span><br><span class="line">            <span class="keyword">yield</span> self.make_requests_from_url(url)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls:</span><br><span class="line">            <span class="comment"># 核心就这么一句话. 组建一个Request对象.我们也可以这么干. </span></span><br><span class="line">            <span class="keyword">yield</span> Request(url, dont_filter=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>自己写个start_requests()看看. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;我是万恶之源&quot;</span>)</span><br><span class="line">    <span class="keyword">yield</span> Request(</span><br><span class="line">        url=LoginSpider.start_urls[<span class="number">0</span>],</span><br><span class="line">        callback=self.parse</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>接下来, 我们去处理cookie</p><h3 id="1-方案一-直接从浏览器复制cookie过来"><a href="#1-方案一-直接从浏览器复制cookie过来" class="headerlink" title="1. 方案一, 直接从浏览器复制cookie过来"></a>1. 方案一, 直接从浏览器复制cookie过来</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 直接从浏览器复制</span></span><br><span class="line">        cookies = <span class="string">&quot;GUID=bbb5f65a-2fa2-40a0-ac87-49840eae4ad1; c_channel=0; c_csc=web; Hm_lvt_9793f42b498361373512340937deb2a0=1627572532,1627711457,1627898858,1628144975; accessToken=avatarUrl%3Dhttps%253A%252F%252Fcdn.static.17k.com%252Fuser%252Favatar%252F16%252F16%252F64%252F75836416.jpg-88x88%253Fv%253D1610625030000%26id%3D75836416%26nickname%3D%25E5%25AD%25A4%25E9%25AD%2582%25E9%2587%258E%25E9%25AC%25BCsb%26e%3D1643697376%26s%3D73f8877e452e744c; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%2275836416%22%2C%22%24device_id%22%3A%2217700ba9c71257-035a42ce449776-326d7006-2073600-17700ba9c728de%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%9B%B4%E6%8E%A5%E6%B5%81%E9%87%8F%22%2C%22%24latest_referrer%22%3A%22%22%2C%22%24latest_referrer_host%22%3A%22%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5%88%B0%E5%80%BC_%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%22%7D%2C%22first_id%22%3A%22bbb5f65a-2fa2-40a0-ac87-49840eae4ad1%22%7D; Hm_lpvt_9793f42b498361373512340937deb2a0=1628145672&quot;</span></span><br><span class="line">        cookie_dic = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> cookies.split(<span class="string">&quot;; &quot;</span>):</span><br><span class="line">            k, v = c.split(<span class="string">&quot;=&quot;</span>)</span><br><span class="line">            cookie_dic[k] = v</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> Request(</span><br><span class="line">            url=LoginSpider.start_urls[<span class="number">0</span>],</span><br><span class="line">            cookies=cookie_dic,</span><br><span class="line">            callback=self.parse</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><p>这种方案和原来的requests几乎一模一样.  需要注意的是: cookie需要通过cookies参数进行传递!</p><h3 id="2-方案二-完成登录过程"><a href="#2-方案二-完成登录过程" class="headerlink" title="2. 方案二, 完成登录过程."></a>2. 方案二, 完成登录过程.</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># 登录流程</span></span><br><span class="line">    username = <span class="string">&quot;18614075987&quot;</span></span><br><span class="line">    password = <span class="string">&quot;q6035945&quot;</span></span><br><span class="line">    url = <span class="string">&quot;https://passport.17k.com/ck/user/login&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 发送post请求</span></span><br><span class="line">    <span class="comment"># yield Request(</span></span><br><span class="line">    <span class="comment">#     url=url,</span></span><br><span class="line">    <span class="comment">#     method=&quot;post&quot;,</span></span><br><span class="line">    <span class="comment">#     body=&quot;loginName=18614075987&amp;password=q6035945&quot;,</span></span><br><span class="line">    <span class="comment">#     callback=self.parse</span></span><br><span class="line">    <span class="comment"># )</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 发送post请求</span></span><br><span class="line">    <span class="keyword">yield</span> FormRequest(</span><br><span class="line">        url=url,</span><br><span class="line">        formdata=&#123;</span><br><span class="line">            <span class="string">&quot;loginName&quot;</span>: username,</span><br><span class="line">            <span class="string">&quot;password&quot;</span>: password</span><br><span class="line">        &#125;,</span><br><span class="line">        callback=self.parse</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="comment"># 得到响应结果. 直接请求到默认的start_urls</span></span><br><span class="line">    <span class="keyword">yield</span> Request(</span><br><span class="line">        url=LoginSpider.start_urls[<span class="number">0</span>],</span><br><span class="line">        callback=self.parse_detail</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_detail</span>(<span class="params">self, resp</span>):</span><br><span class="line">    <span class="built_in">print</span>(resp.text)</span><br></pre></td></tr></table></figure><p>​注意, 发送post请求有两个方案, </p><ol><li><p>Scrapy.Request(url&#x3D;url, method&#x3D;’post’, body&#x3D;数据)</p></li><li><p>Scarpy.FormRequest(url&#x3D;url, formdata&#x3D;数据)  -&gt; 推荐</p><p>区别: 方式1的数据只能是字符串. 这个就很难受. 所以推荐用第二种.</p></li></ol><h2 id="二-Scrapy的中间件"><a href="#二-Scrapy的中间件" class="headerlink" title="二. Scrapy的中间件"></a>二. Scrapy的中间件</h2><p>​中间件的作用: 负责处理引擎和爬虫以及引擎和下载器之间的请求和响应. 主要是可以对request和response做预处理. 为后面的操作做好充足的准备工作. 在python中准备了两种中间件, 分别是下载器中间件和爬虫中间件. </p><h3 id="1-DownloaderMiddleware"><a href="#1-DownloaderMiddleware" class="headerlink" title="1. DownloaderMiddleware"></a>1. DownloaderMiddleware</h3><p>​下载中间件, 它是介于引擎和下载器之间,  引擎在获取到request对象后, 会交给下载器去下载, 在这之间我们可以设置下载中间件. 它的执行流程:</p><p>​引擎拿到request -&gt; 中间件1(process_request) -&gt; 中间件2(process_request) …..-&gt;      下载器-|<br>​    引擎拿到request &lt;- 中间件1(process_response) &lt;- 中间件2(process_response) ….. &lt;-下载器-|</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MidDownloaderMiddleware1</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process_request&quot;</span>, <span class="string">&quot;ware1&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process_response&quot;</span>, <span class="string">&quot;ware1&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process_exception&quot;</span>, <span class="string">&quot;ware1&quot;</span>)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MidDownloaderMiddleware2</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process_request&quot;</span>, <span class="string">&quot;ware2&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process_response&quot;</span>, <span class="string">&quot;ware2&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process_exception&quot;</span>, <span class="string">&quot;ware2&quot;</span>)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>设置中间件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="comment"># &#x27;mid.middlewares.MidDownloaderMiddleware&#x27;: 542,</span></span><br><span class="line">   <span class="string">&#x27;mid.middlewares.MidDownloaderMiddleware1&#x27;</span>: <span class="number">543</span>,</span><br><span class="line">   <span class="string">&#x27;mid.middlewares.MidDownloaderMiddleware2&#x27;</span>: <span class="number">544</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>优先级参考管道. </p><p>运行效果;</p><p><img src="/images/scrapy01/image-20210805180841148.png" alt="images/scrapy01/image-20210805180841148"></p><p>接下来, 我们来说说这几个方法的返回值问题(难点)</p><ol><li><p>process_request(request, spider):  在每个请求到达下载器之前调用</p><p>一, return None  不拦截, 把请求继续向后传递给权重低的中间件或者下载器</p><p>二, return request 请求被拦截, 并将一个新的请求返回. 后续中间件以及下载器收不到本次请求</p><p>三, return response 请求被拦截, 下载器将获取不到请求, 但是引擎是可以接收到本次响应的内容, 也就是说在当前方法内就已经把响应内容获取到了. </p></li><li><p>proccess_response(request, response, spider): 每个请求从下载器出来调用</p><p>一, return response 通过引擎将响应内容继续传递给其他组件或传递给其他process_response()处理</p><p>二, return request  响应被拦截. 将返回内容直接回馈给调度器(通过引擎), 后续process_response()接收不到响应内容.</p></li></ol><p>OK, 至此, 中间件的含义算是完事儿了. 那这东西有啥用?  我们上案例!</p><h4 id="1-1-动态随机设置UA"><a href="#1-1-动态随机设置UA" class="headerlink" title="1.1. 动态随机设置UA"></a>1.1. 动态随机设置UA</h4><p>设置统一的UA很简单. 直接在settings里设置即可. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">USER_AGENT = <span class="string">&#x27;User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36&#x27;</span></span><br></pre></td></tr></table></figure><p>但是这个不够好, 我希望得到一个随机的UA.  此时就可以这样设计, 首先, 在settings里定义好一堆UserAgent.  <a href="http://useragentstring.com/pages/useragentstring.php?name=Chrome">http://useragentstring.com/pages/useragentstring.php?name=Chrome</a> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">USER_AGENT_LIST = [</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (X11; Ubuntu; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2919.83 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2866.71 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (X11; Ubuntu; Linux i686 on x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2820.59 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2762.73 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2656.18 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML like Gecko) Chrome/44.0.2403.155 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2226.0 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.4; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2224.3 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.124 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 4.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36&#x27;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>​中间件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyRandomUserAgentMiddleware</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        UA = choice(USER_AGENT_LIST)</span><br><span class="line">        request.headers[<span class="string">&#x27;User-Agent&#x27;</span>] = UA</span><br><span class="line">        <span class="comment"># 不要返回任何东西</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h4 id="1-2-处理代理问题"><a href="#1-2-处理代理问题" class="headerlink" title="1.2 处理代理问题"></a>1.2 处理代理问题</h4><p>代理问题一直是我们作为一名爬虫工程师很蛋疼的问题. 不加容易被检测, 加了效率低, 免费的可用IP更是凤毛麟角. 没办法, 无论如何还是得面对它. 这里, 我们采用两个方案来给各位展示scrapy中添加代理的逻辑.</p><ol><li><p>免费代理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ProxyMiddleware</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;又来&quot;</span>)</span><br><span class="line">        proxy = choice(PROXY_LIST)</span><br><span class="line">        request.meta[<span class="string">&#x27;proxy&#x27;</span>] = <span class="string">&quot;https://&quot;</span>+proxy  <span class="comment"># 设置代理</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;有么有结果???&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> response.status != <span class="number">200</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;尝试失败&quot;</span>)</span><br><span class="line">            request.dont_filter = <span class="literal">True</span>  <span class="comment"># 丢回调度器重新请求</span></span><br><span class="line">            <span class="keyword">return</span> request</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;出错了!&quot;</span>)</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></li><li><p>收费代理</p><p>免费代理实在太难用了. 我们这里直接选择一个收费代理. 依然选择<code>快代理</code>, 这个根据你自己的喜好进行调整. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MoneyProxyMiddleware</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_get_proxy</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        912831993520336t12831993520578每次请求换IP</span></span><br><span class="line"><span class="string">        tps138.kdlapi.com 15818</span></span><br><span class="line"><span class="string">        需实名认证5次/s5Mb/s有效续费|订单详情|实名认证</span></span><br><span class="line"><span class="string">        隧道用户名密码修改密码</span></span><br><span class="line"><span class="string">        用户名：t12831993520578密码：t72a13xu</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        url = <span class="string">&quot;http://tps138.kdlapi.com:15818&quot;</span></span><br><span class="line">        auth = basic_auth_header(username=<span class="string">&quot;t12831993520578&quot;</span>, password=<span class="string">&quot;t72a13xu&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> url, auth</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;......&quot;</span>)</span><br><span class="line">        url, auth = self._get_proxy()</span><br><span class="line">        request.meta[<span class="string">&#x27;proxy&#x27;</span>] = url</span><br><span class="line">        request.headers[<span class="string">&#x27;Proxy-Authorization&#x27;</span>] = auth</span><br><span class="line">        request.headers[<span class="string">&#x27;Connection&#x27;</span>] = <span class="string">&#x27;close&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(response.status, <span class="built_in">type</span>(response.status))</span><br><span class="line">        <span class="keyword">if</span> response.status != <span class="number">200</span>:</span><br><span class="line">            request.dont_filter = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">return</span> request</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></li></ol><h4 id="1-3-使用selenium完成数据抓取"><a href="#1-3-使用selenium完成数据抓取" class="headerlink" title="1.3 使用selenium完成数据抓取"></a>1.3 使用selenium完成数据抓取</h4><p>首先, 我们需要使用selenium作为下载器进行下载. 那么我们的请求应该也是特殊订制的. 所以, 在我的设计里, 我可以重新设计一个请求. 就叫SeleniumRequest</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.http.request <span class="keyword">import</span> Request</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SeleniumRequest</span>(<span class="title class_ inherited__">Request</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>这里面不需要做任何操作. 整体还是用它父类的东西来进行操作. </p><p>接下来. 完善一下spider</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> boss.request <span class="keyword">import</span> SeleniumRequest</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BeijingSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;beijing&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;zhipin.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.zhipin.com/job_detail/?query=python&amp;city=101010100&amp;industry=&amp;position=&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">yield</span> SeleniumRequest(</span><br><span class="line">            url=BeijingSpider.start_urls[<span class="number">0</span>],</span><br><span class="line">            callback=self.parse,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, resp, **kwargs</span>):</span><br><span class="line">        li_list = resp.xpath(<span class="string">&#x27;//*[@id=&quot;main&quot;]/div/div[3]/ul/li&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">            href = li.xpath(<span class="string">&quot;./div[1]/div[1]/div[1]/div[1]/div[1]/span[1]/a[1]/@href&quot;</span>).extract_first()</span><br><span class="line">            name = li.xpath(<span class="string">&quot;./div[1]/div[1]/div[1]/div[1]/div[1]/span[1]/a[1]/text()&quot;</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(name, href)</span><br><span class="line">            <span class="built_in">print</span>(resp.urljoin(href))</span><br><span class="line">            <span class="keyword">yield</span> SeleniumRequest(</span><br><span class="line">                url=resp.urljoin(href),</span><br><span class="line">                callback=self.parse_detail,</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 下一页.....</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_detail</span>(<span class="params">self, resp, **kwargs</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;招聘人&quot;</span>, resp.xpath(<span class="string">&#x27;//*[@id=&quot;main&quot;]/div[3]/div/div[2]/div[1]/h2&#x27;</span>).extract())</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>中间件~</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BossDownloaderMiddleware</span>:</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        <span class="comment"># This method is used by Scrapy to create your spiders.</span></span><br><span class="line">        s = cls()</span><br><span class="line">        <span class="comment"># 这里很关键哦. </span></span><br><span class="line">        <span class="comment"># 在爬虫开始的时候. 执行spider_opened</span></span><br><span class="line">        <span class="comment"># 在爬虫结束的时候. 执行spider_closed</span></span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)</span><br><span class="line">        crawler.signals.connect(s.spider_closed, signal=signals.spider_closed)</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(request, SeleniumRequest):</span><br><span class="line">            self.web.get(request.url)</span><br><span class="line">            time.sleep(<span class="number">3</span>)</span><br><span class="line">            page_source = self.web.page_source</span><br><span class="line">            <span class="keyword">return</span> HtmlResponse(url=request.url, encoding=<span class="string">&#x27;utf-8&#x27;</span>, request=request, body=page_source)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">spider_opened</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.web = Chrome()</span><br><span class="line">        self.web.implicitly_wait(<span class="number">10</span>)</span><br><span class="line">        <span class="comment"># 完成登录. 拿到cookie. 很容易...</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;创建浏览器&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">spider_closed</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.web.close()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;关闭浏览器&quot;</span>)</span><br></pre></td></tr></table></figure><p>settings</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="comment"># 怼在所有默认中间件前面. 只要是selenium后面所有的中间件都给我停</span></span><br><span class="line">   <span class="string">&#x27;boss.middlewares.BossDownloaderMiddleware&#x27;</span>: <span class="number">99</span>,  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-4-用selenium设置cookie"><a href="#1-4-用selenium设置cookie" class="headerlink" title="1.4 用selenium设置cookie"></a>1.4 用selenium设置cookie</h4><p>有了这个案例. 想要用selenium处理cookie也很容易了. 直接在spider_opened位置完成登录, 然后在process_request()中简单设置一下即可. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ChaojiyingDownloaderMiddleware</span>:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        <span class="comment"># This method is used by Scrapy to create your spiders.</span></span><br><span class="line">        s = cls()</span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> request.cookies:</span><br><span class="line">            request.cookies = self.cookie</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">spider_opened</span>(<span class="params">self, spider</span>):</span><br><span class="line">        web = Chrome()</span><br><span class="line">        web.get(<span class="string">&quot;https://www.chaojiying.com/user/login/&quot;</span>)</span><br><span class="line">        web.find_element_by_xpath(<span class="string">&#x27;/html/body/div[3]/div/div[3]/div[1]/form/p[1]/input&#x27;</span>).send_keys(<span class="string">&quot;18614075987&quot;</span>)</span><br><span class="line">        web.find_element_by_xpath(<span class="string">&#x27;/html/body/div[3]/div/div[3]/div[1]/form/p[2]/input&#x27;</span>).send_keys(<span class="string">&#x27;q6035945&#x27;</span>)</span><br><span class="line">        img = web.find_element_by_xpath(<span class="string">&#x27;/html/body/div[3]/div/div[3]/div[1]/form/div/img&#x27;</span>)</span><br><span class="line">        verify_code = self.base64_api(<span class="string">&quot;q6035945&quot;</span>, <span class="string">&quot;q6035945&quot;</span>, img.screenshot_as_base64, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        web.find_element_by_xpath(<span class="string">&#x27;/html/body/div[3]/div/div[3]/div[1]/form/p[3]/input&#x27;</span>).send_keys(verify_code)</span><br><span class="line"></span><br><span class="line">        web.find_element_by_xpath(<span class="string">&#x27;/html/body/div[3]/div/div[3]/div[1]/form/p[4]/input&#x27;</span>).click()</span><br><span class="line">        time.sleep(<span class="number">3</span>)</span><br><span class="line">        cookies = web.get_cookies()</span><br><span class="line">        self.cookie = &#123;dic[<span class="string">&#x27;name&#x27;</span>]:dic[<span class="string">&#x27;value&#x27;</span>] <span class="keyword">for</span> dic <span class="keyword">in</span> cookies&#125;</span><br><span class="line">        web.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">base64_api</span>(<span class="params">self, uname, pwd, b64_img, typeid</span>):</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">&quot;username&quot;</span>: uname,</span><br><span class="line">            <span class="string">&quot;password&quot;</span>: pwd,</span><br><span class="line">            <span class="string">&quot;typeid&quot;</span>: typeid,</span><br><span class="line">            <span class="string">&quot;image&quot;</span>: b64_img</span><br><span class="line">        &#125;</span><br><span class="line">        result = json.loads(requests.post(<span class="string">&quot;http://api.ttshitu.com/predict&quot;</span>, json=data).text)</span><br><span class="line">        <span class="keyword">if</span> result[<span class="string">&#x27;success&#x27;</span>]:</span><br><span class="line">            <span class="keyword">return</span> result[<span class="string">&quot;data&quot;</span>][<span class="string">&quot;result&quot;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> result[<span class="string">&quot;message&quot;</span>]</span><br></pre></td></tr></table></figure><h3 id="2-SpiderMiddleware-了解"><a href="#2-SpiderMiddleware-了解" class="headerlink" title="2. SpiderMiddleware(了解)"></a>2. SpiderMiddleware(了解)</h3><p>​爬虫中间件. 是处于引擎和spider之间的中间件. 里面常用的方法有:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CuowuSpiderMiddleware</span>:</span><br><span class="line">    <span class="comment"># Not all methods need to be defined. If a method is not defined,</span></span><br><span class="line">    <span class="comment"># scrapy acts as if the spider middleware does not modify the</span></span><br><span class="line">    <span class="comment"># passed objects.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        <span class="comment"># This method is used by Scrapy to create your spiders.</span></span><br><span class="line">        s = cls()</span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_input</span>(<span class="params">self, response, spider</span>):</span><br><span class="line">        <span class="comment"># 请求被返回, 即将进入到spider时调用</span></span><br><span class="line">        <span class="comment"># 要么返回None, 要么报错</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;我是process_spider_input&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span><br><span class="line">        <span class="comment"># 处理完spider中的数据. 返回数据后. 执行</span></span><br><span class="line">        <span class="comment"># 返回值要么是item, 要么是request.</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;我是process_spider_output&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> result:</span><br><span class="line">            <span class="keyword">yield</span> i</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;我是process_spider_output&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process_spider_exception&quot;</span>)</span><br><span class="line">        <span class="comment"># spider中报错 或者, process_spider_input() 方法报错</span></span><br><span class="line">        <span class="comment"># 返回None或者Request或者item.</span></span><br><span class="line">        it = ErrorItem()</span><br><span class="line">        it[<span class="string">&#x27;name&#x27;</span>] = <span class="string">&quot;exception&quot;</span></span><br><span class="line">        it[<span class="string">&#x27;url&#x27;</span>] = response.url</span><br><span class="line">        <span class="keyword">yield</span> it</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_start_requests</span>(<span class="params">self, start_requests, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process_start_requests&quot;</span>)</span><br><span class="line">        <span class="comment"># 第一次启动爬虫时被调用.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must return only requests (not items).</span></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> start_requests:</span><br><span class="line">            <span class="keyword">yield</span> r</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">spider_opened</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>items</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ErrorItem</span>(scrapy.Item):</span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    url = scrapy.Field()</span><br></pre></td></tr></table></figure><p>spider:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BaocuoSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;baocuo&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;baidu.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.baidu.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, resp, **kwargs</span>):</span><br><span class="line">        name = resp.xpath(<span class="string">&#x27;//title/text()&#x27;</span>).extract_first()</span><br><span class="line">        <span class="comment"># print(1/0)  # 调整调整这个. 简单琢磨一下即可~~</span></span><br><span class="line">        it = CuowuItem()</span><br><span class="line">        it[<span class="string">&#x27;name&#x27;</span>] = name</span><br><span class="line">        <span class="built_in">print</span>(name)</span><br><span class="line">        <span class="keyword">yield</span> it</span><br></pre></td></tr></table></figure><p>pipeline:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cuowu.items <span class="keyword">import</span> ErrorItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CuowuPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(item, ErrorItem):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;错误&quot;</span>, item)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;没错&quot;</span>, item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>目录结构:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cuowu</span><br><span class="line">├── cuowu</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── items.py</span><br><span class="line">│   ├── middlewares.py</span><br><span class="line">│   ├── pipelines.py</span><br><span class="line">│   ├── settings.py</span><br><span class="line">│   └── spiders</span><br><span class="line">│       ├── __init__.py</span><br><span class="line">│       └── baocuo.py</span><br><span class="line">└── scrapy.cfg</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="爬虫开发" scheme="http://blog.ioimp.top/categories/%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91/"/>
    
    
    <category term="Scrapy爬虫" scheme="http://blog.ioimp.top/tags/Scrapy%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy_管道</title>
    <link href="http://blog.ioimp.top/2023/12/03/09-Scrapy-%E7%AE%A1%E9%81%93/"/>
    <id>http://blog.ioimp.top/2023/12/03/09-Scrapy-%E7%AE%A1%E9%81%93/</id>
    <published>2023-12-03T03:56:22.000Z</published>
    <updated>2023-12-03T03:56:56.718Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Scrapy管道"><a href="#Scrapy管道" class="headerlink" title="Scrapy管道"></a>Scrapy管道</h1><p>在上一小节中, 我们初步掌握了Scrapy的基本运行流程以及基本开发流程. 本节继续讨论关于Scrapy更多的内容. </p><h2 id="一-关于管道"><a href="#一-关于管道" class="headerlink" title="一. 关于管道"></a>一. 关于管道</h2><p>上一节内容, 我们已经可以从spider中提取到数据. 然后通过引擎将数据传递给pipeline, 那么在pipeline中如何对数据进行保存呢? 我们主要针对四种数据存储展开讲解. </p><p>前三个案例以<a href="http://datachart.500.com/ssq/%E4%B8%BA%E6%A1%88%E4%BE%8B%E5%9F%BA%E7%A1%80">http://datachart.500.com/ssq/为案例基础</a>. 最后一个以<a href="https://www.tupianzj.com/bizhi/DNmeinv/%E4%B8%BA%E6%A1%88%E4%BE%8B%E5%9F%BA%E7%A1%80">https://www.tupianzj.com/bizhi/DNmeinv/为案例基础</a>. </p><h3 id="1-csv文件写入"><a href="#1-csv文件写入" class="headerlink" title="1. csv文件写入"></a>1. csv文件写入</h3><p>​写入文件是一个非常简单的事情. 直接在pipeline中开启文件即可. 但这里要说明的是. 如果我们只在process_item中进行处理文件是不够优雅的.  总不能有一条数据就open一次吧</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CaipiaoFilePipeline</span>:</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;caipiao.txt&quot;</span>, mode=<span class="string">&quot;a&quot;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="comment"># 写入文件</span></span><br><span class="line">            f.write(<span class="string">f&quot;<span class="subst">&#123;item[<span class="string">&#x27;qihao&#x27;</span>]&#125;</span>, <span class="subst">&#123;<span class="string">&#x27;_&#x27;</span>.join(item[<span class="string">&#x27;red_ball&#x27;</span>])&#125;</span>, <span class="subst">&#123;<span class="string">&#x27;_&#x27;</span>.join(item[<span class="string">&#x27;blue_ball&#x27;</span>])&#125;</span>\n&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p>​我们希望的是, 能不能打开一个文件, 然后就用这一个文件句柄来完成数据的保存. 答案是可以的. 我们可以在pipeline中创建两个方法, 一个是open_spider(), 另一个是close_spider(). 看名字也能明白其含义: </p><p>​open_spider(), 在爬虫开始的时候执行一次<br>​close_spider(), 在爬虫结束的时候执行一次</p><p>​有了这俩货, 我们就可以很简单的去处理这个问题</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CaipiaoFilePipeline</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.f = <span class="built_in">open</span>(<span class="string">&quot;caipiao.txt&quot;</span>, mode=<span class="string">&quot;a&quot;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="keyword">if</span> self.f:</span><br><span class="line">            self.f.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="comment"># 写入文件</span></span><br><span class="line">        self.f.write(<span class="string">f&quot;<span class="subst">&#123;item[<span class="string">&#x27;qihao&#x27;</span>]&#125;</span>, <span class="subst">&#123;<span class="string">&#x27;_&#x27;</span>.join(item[<span class="string">&#x27;red_ball&#x27;</span>])&#125;</span>, <span class="subst">&#123;<span class="string">&#x27;_&#x27;</span>.join(item[<span class="string">&#x27;blue_ball&#x27;</span>])&#125;</span>\n&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p>​在爬虫开始的时候打开一个文件, 在爬虫结束的时候关闭这个文件. 满分~</p><p>​对了, 别忘了设置settings</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;caipiao.pipelines.CaipiaoFilePipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-mysql数据库写入"><a href="#2-mysql数据库写入" class="headerlink" title="2. mysql数据库写入"></a>2. mysql数据库写入</h3><p>​有了上面的示例, 写入数据库其实也就很顺其自然了, 首先, 在open_spider中创建好数据库连接. 在close_spider中关闭链接. 在proccess_item中对数据进行保存工作. </p><p>先把mysql相关设置丢到settings里</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MYSQL配置信息</span></span><br><span class="line">MYSQL_CONFIG = &#123;</span><br><span class="line">   <span class="string">&quot;host&quot;</span>: <span class="string">&quot;localhost&quot;</span>,</span><br><span class="line">   <span class="string">&quot;port&quot;</span>: <span class="number">3306</span>,</span><br><span class="line">   <span class="string">&quot;user&quot;</span>: <span class="string">&quot;root&quot;</span>,</span><br><span class="line">   <span class="string">&quot;password&quot;</span>: <span class="string">&quot;test123456&quot;</span>,</span><br><span class="line">   <span class="string">&quot;database&quot;</span>: <span class="string">&quot;spider&quot;</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> caipiao.settings <span class="keyword">import</span> MYSQL_CONFIG <span class="keyword">as</span> mysql</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CaipiaoMySQLPipeline</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.conn = pymysql.connect(host=mysql[<span class="string">&quot;host&quot;</span>], port=mysql[<span class="string">&quot;port&quot;</span>], user=mysql[<span class="string">&quot;user&quot;</span>], password=mysql[<span class="string">&quot;password&quot;</span>], database=mysql[<span class="string">&quot;database&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.conn.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="comment"># 写入文件</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            cursor = self.conn.cursor()</span><br><span class="line">            sql = <span class="string">&quot;insert into caipiao(qihao, red, blue) values(%s, %s, %s)&quot;</span></span><br><span class="line">            red = <span class="string">&quot;,&quot;</span>.join(item[<span class="string">&#x27;red_ball&#x27;</span>])</span><br><span class="line">            blue = <span class="string">&quot;,&quot;</span>.join(item[<span class="string">&#x27;blue_ball&#x27;</span>])</span><br><span class="line">            cursor.execute(sql, (item[<span class="string">&#x27;qihao&#x27;</span>], red, blue))</span><br><span class="line">            self.conn.commit()</span><br><span class="line">            spider.logger.info(<span class="string">f&quot;保存数据<span class="subst">&#123;item&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            self.conn.rollback()</span><br><span class="line">            spider.logger.error(<span class="string">f&quot;保存数据库失败!&quot;</span>, e, <span class="string">f&quot;数据是: <span class="subst">&#123;item&#125;</span>&quot;</span>)  <span class="comment"># 记录错误日志</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>别忘了把pipeline设置一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;caipiao.pipelines.CaipiaoMySQLPipeline&#x27;</span>: <span class="number">301</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-mongodb数据库写入"><a href="#3-mongodb数据库写入" class="headerlink" title="3. mongodb数据库写入"></a>3. mongodb数据库写入</h3><p>​mongodb数据库写入和mysql写入如出一辙…不废话直接上代码吧</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">MONGO_CONFIG = &#123;</span><br><span class="line">   <span class="string">&quot;host&quot;</span>: <span class="string">&quot;localhost&quot;</span>,</span><br><span class="line">   <span class="string">&quot;port&quot;</span>: <span class="number">27017</span>,</span><br><span class="line">   <span class="string">&#x27;has_user&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">   <span class="string">&#x27;user&#x27;</span>: <span class="string">&quot;python_admin&quot;</span>,</span><br><span class="line">   <span class="string">&quot;password&quot;</span>: <span class="string">&quot;123456&quot;</span>,</span><br><span class="line">   <span class="string">&quot;db&quot;</span>: <span class="string">&quot;python&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> caipiao.settings <span class="keyword">import</span> MONGO_CONFIG <span class="keyword">as</span> mongo</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CaipiaoMongoDBPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        client = pymongo.MongoClient(host=mongo[<span class="string">&#x27;host&#x27;</span>],</span><br><span class="line">                                     port=mongo[<span class="string">&#x27;port&#x27;</span>])</span><br><span class="line">        db = client[mongo[<span class="string">&#x27;db&#x27;</span>]]</span><br><span class="line">        <span class="keyword">if</span> mongo[<span class="string">&#x27;has_user&#x27;</span>]:</span><br><span class="line">            db.authenticate(mongo[<span class="string">&#x27;user&#x27;</span>], mongo[<span class="string">&#x27;password&#x27;</span>])</span><br><span class="line">        self.client = client</span><br><span class="line">        self.collection = db[<span class="string">&#x27;caipiao&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        self.collection.insert(&#123;<span class="string">&quot;qihao&quot;</span>: item[<span class="string">&#x27;qihao&#x27;</span>], <span class="string">&#x27;red&#x27;</span>: item[<span class="string">&quot;red_ball&quot;</span>], <span class="string">&#x27;blue&#x27;</span>: item[<span class="string">&#x27;blue_ball&#x27;</span>]&#125;)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="comment"># 三个管道可以共存~</span></span><br><span class="line">   <span class="string">&#x27;caipiao.pipelines.CaipiaoFilePipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">   <span class="string">&#x27;caipiao.pipelines.CaipiaoMySQLPipeline&#x27;</span>: <span class="number">301</span>,</span><br><span class="line">   <span class="string">&#x27;caipiao.pipelines.CaipiaoMongoDBPipeline&#x27;</span>: <span class="number">302</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-文件保存"><a href="#4-文件保存" class="headerlink" title="4. 文件保存"></a>4. 文件保存</h3><p>接下来我们来尝试使用scrapy来下载一些图片, 看看效果如何. </p><p>首先, 随便找个图片网站(安排好的). <a href="https://www.tupianzj.com/bizhi/DNmeinv/">https://www.tupianzj.com/bizhi/DNmeinv/</a>. 可以去看看, 妹子们还是很漂亮的. </p><p>接下来. 创建好项目,  定义好数据结构</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MeinvItem</span>(scrapy.Item):</span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    img_url = scrapy.Field()</span><br><span class="line">    img_path = scrapy.Field()</span><br></pre></td></tr></table></figure><p>完善spider, 注意看yield scrapy.Request()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> meinv.items <span class="keyword">import</span> MeinvItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TupianzhijiaSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;tupianzhijia&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;tupianzj.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.tupianzj.com/bizhi/DNmeinv/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, resp, **kwargs</span>):</span><br><span class="line">        li_list = resp.xpath(<span class="string">&quot;//ul[@class=&#x27;list_con_box_ul&#x27;]/li&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">            href = li.xpath(<span class="string">&quot;./a/@href&quot;</span>).extract_first()</span><br><span class="line">            <span class="comment"># 拿到href为了什么? 进入详情页啊</span></span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            url: 请求地址</span></span><br><span class="line"><span class="string">            method: 请求方式</span></span><br><span class="line"><span class="string">            callback: 回调函数</span></span><br><span class="line"><span class="string">            errback: 报错回调</span></span><br><span class="line"><span class="string">            dont_filter: 默认False, 表示&quot;不过滤&quot;, 该请求会重新进行发送</span></span><br><span class="line"><span class="string">            headers: 请求头. </span></span><br><span class="line"><span class="string">            cookies: cookie信息</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">                url=resp.urljoin(href),  <span class="comment"># scrapy的url拼接</span></span><br><span class="line">                method=<span class="string">&#x27;get&#x27;</span>,</span><br><span class="line">                callback=self.parse_detail,</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 下一页</span></span><br><span class="line">        next_page = resp.xpath(<span class="string">&#x27;//div[@class=&quot;pages&quot;]/ul/li/a[contains(text(), &quot;下一页&quot;)]/@href&#x27;</span>).extract_first()</span><br><span class="line">        <span class="keyword">if</span> next_page:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">                url=resp.urljoin(next_page),</span><br><span class="line">                method=<span class="string">&#x27;get&#x27;</span>,</span><br><span class="line">                callback=self.parse</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_detail</span>(<span class="params">self, resp</span>):</span><br><span class="line">        img_src = resp.xpath(<span class="string">&#x27;//*[@id=&quot;bigpic&quot;]/a[1]/img/@src&#x27;</span>).extract_first()</span><br><span class="line">        name = resp.xpath(<span class="string">&#x27;//*[@id=&quot;container&quot;]/div/div/div[2]/h1/text()&#x27;</span>).extract_first()</span><br><span class="line">        meinv = MeinvItem()</span><br><span class="line">        meinv[<span class="string">&#x27;name&#x27;</span>] = name</span><br><span class="line">        meinv[<span class="string">&#x27;img_url&#x27;</span>] = img_src</span><br><span class="line">        <span class="keyword">yield</span> meinv</span><br><span class="line">        </span><br></pre></td></tr></table></figure><p>​关于Request()的参数:<br>​url: 请求地址<br>​            method: 请求方式<br>​            callback: 回调函数<br>​            errback: 报错回调<br>​            dont_filter: 默认False, 表示”不过滤”, 该请求会重新进行发送<br>​            headers: 请求头.<br>​            cookies: cookie信息</p><p>​接下来就是下载问题了. 如何在pipeline中下载一张图片呢? Scrapy早就帮你准备好了. 在Scrapy中有一个ImagesPipeline可以实现自动图片下载功能. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline, FilesPipeline</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> meinv.settings <span class="keyword">import</span> MYSQL</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MeinvPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.conn = pymysql.connect(</span><br><span class="line">            host=MYSQL[<span class="string">&#x27;host&#x27;</span>],</span><br><span class="line">            port=MYSQL[<span class="string">&#x27;port&#x27;</span>],</span><br><span class="line">            user=MYSQL[<span class="string">&#x27;user&#x27;</span>],</span><br><span class="line">            password=MYSQL[<span class="string">&#x27;password&#x27;</span>],</span><br><span class="line">            database=MYSQL[<span class="string">&#x27;database&#x27;</span>]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="keyword">if</span> self.conn:</span><br><span class="line">            self.conn.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            cursor = self.conn.cursor()</span><br><span class="line">            sql = <span class="string">&quot;insert into tu (name, img_src, img_path) values (%s, %s, %s)&quot;</span></span><br><span class="line">            cursor.execute(sql, (item[<span class="string">&#x27;name&#x27;</span>], item[<span class="string">&#x27;img_src&#x27;</span>], item[<span class="string">&#x27;img_path&#x27;</span>]))</span><br><span class="line">            self.conn.commit()</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            self.conn.rollback()</span><br><span class="line">        <span class="keyword">finally</span>:</span><br><span class="line">            <span class="keyword">if</span> cursor:</span><br><span class="line">                cursor.close()</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MeinvSavePipeline</span>(<span class="title class_ inherited__">ImagesPipeline</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_media_requests</span>(<span class="params">self, item, info</span>):</span><br><span class="line">        <span class="comment"># 发送请求去下载图片</span></span><br><span class="line">        <span class="comment"># 如果是一堆图片. 可以使用循环去得到每一个url, 然后在yield每一个图片对应的Request对象</span></span><br><span class="line">        <span class="keyword">return</span> scrapy.Request(item[<span class="string">&#x27;img_url&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">file_path</span>(<span class="params">self, request, response=<span class="literal">None</span>, info=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 准备好图片的名称</span></span><br><span class="line">        filename = request.url.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;img/<span class="subst">&#123;filename&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">item_completed</span>(<span class="params">self, results, item, info</span>):</span><br><span class="line">        <span class="comment"># 文件存储的路径</span></span><br><span class="line">        ok, res = results[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># print(res[&#x27;path&#x27;])</span></span><br><span class="line">        item[<span class="string">&#x27;img_path&#x27;</span>] = res[<span class="string">&quot;path&quot;</span>]</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p>最后, 需要在settings中设置以下内容:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">MYSQL = &#123;</span><br><span class="line">   <span class="string">&quot;host&quot;</span>: <span class="string">&quot;localhost&quot;</span>,</span><br><span class="line">   <span class="string">&quot;port&quot;</span>: <span class="number">3306</span>,</span><br><span class="line">   <span class="string">&quot;user&quot;</span>: <span class="string">&quot;root&quot;</span>,</span><br><span class="line">   <span class="string">&quot;password&quot;</span>: <span class="string">&quot;test123456&quot;</span>,</span><br><span class="line">   <span class="string">&quot;database&quot;</span>: <span class="string">&#x27;spider&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">&#x27;meinv.pipelines.MeinvPipeline&#x27;</span>: <span class="number">303</span>,</span><br><span class="line">    <span class="string">&#x27;meinv.pipelines.MeinvSavePipeline&#x27;</span>: <span class="number">301</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 图片保存路径  -&gt; ImagesPipeline</span></span><br><span class="line">IMAGES_STORE= <span class="string">&#x27;./my_tu&#x27;</span></span><br><span class="line"><span class="comment"># 文件保存路径 -&gt; FilesPipeline</span></span><br><span class="line">FILES_STORE = <span class="string">&#x27;./my_tu&#x27;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="爬虫开发" scheme="http://blog.ioimp.top/categories/%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91/"/>
    
    
    <category term="Scrapy爬虫" scheme="http://blog.ioimp.top/tags/Scrapy%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>scrapy入门课件</title>
    <link href="http://blog.ioimp.top/2023/12/03/scrapy%E5%85%A5%E9%97%A8%E8%AF%BE%E4%BB%B6/"/>
    <id>http://blog.ioimp.top/2023/12/03/scrapy%E5%85%A5%E9%97%A8%E8%AF%BE%E4%BB%B6/</id>
    <published>2023-12-03T03:53:32.000Z</published>
    <updated>2023-12-03T03:55:45.577Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Scrapy-基本介绍与使用"><a href="#Scrapy-基本介绍与使用" class="headerlink" title="Scrapy 基本介绍与使用"></a>Scrapy 基本介绍与使用</h1><h2 id="一-爬虫工程化"><a href="#一-爬虫工程化" class="headerlink" title="一, 爬虫工程化"></a>一, 爬虫工程化</h2><p>​在之前的学习中我们已经掌握了爬虫这门技术需要的大多数的技术点, 但是我们现在写的代码还很流程化, 很难进行商用的. 想要我们的爬虫达到商用级别, 必须要对我们现在编写的爬虫代码进行大刀阔斧式的重组, 已达到工程化的爬虫. 何为工程化, 就是让你的程序更加的有体系, 有逻辑, 更加的模块化. </p><p>​就好比, 我们家里以前做过鞋子, 我妈妈给我做鞋, 她需要从画图纸到裁剪到最后的缝合, 一步一步的完成一双鞋子的制作. 这种手工鞋子如果每年做个几双, 没问题. 我妈妈辛苦一点, 也能搞定. 但是, 如果现在我想去售卖这个鞋子. 再依靠妈妈一双一双的缝制. 你不赔死, 也得让你妈打死. 为什么? 第一, 产能跟不上. 一个人的力量是有限的, 第二, 一个人要完整的把制作鞋子的工艺从头搞到尾. 就算你想招人分担一下. 貌似也不好找这样厉害的手艺人. 怎么办? 聪明的你可能已经想到了. 从头到尾完成一双鞋的人不好找. 那我就把这个工艺过程分开. 分成4份, 画图, 裁剪, 缝合, 验收.  招4个人. 每个人就负责一小部分. 并且这一小部分是很容易完成的. 最终只要有一个人(我)来做一个总指挥. 我的制鞋小工厂就建起来了. </p><p>​上述逻辑同样适用于我们的爬虫, 想想, 到目前为止, 我们所编写的爬虫我们都是从头到尾的每一步都要亲力亲为. 这样做固然有其优点(可控性更好), 但是各位请认真思考. 这样的代码逻辑是不能形成批量生产的效果的(写100个爬虫). 很多具有共通性的代码逻辑都没有进行重复利用. 那我们就可以考虑看看, 能不能把一些共性的问题(获取页面源代码, 数据存储), 单独搞成一个功能. 如果我们把这些功能单独进行编写. 并且产生类似单独的功能模块, 将大大的提高我们爬虫的效率.  已达到我们爬虫工程化开发的效果. </p><p>​爬虫工程化: 对爬虫的功能进行模块化的开发. 并达到可以批量生产的效果(不论是开发还是数据产出)</p><h2 id="二-scrapy简介"><a href="#二-scrapy简介" class="headerlink" title="二, scrapy简介"></a>二, scrapy简介</h2><p>​Scrapy到目前为止依然是这个星球上最流行的爬虫框架. 摘一下官方给出对scrapy的介绍</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">An open source and collaborative framework for extracting the data you need from websites.</span><br><span class="line"></span><br><span class="line">In a fast, simple, yet extensible way.</span><br></pre></td></tr></table></figure><p>​scrapy的特点: 速度快, 简单, 可扩展性强. </p><p>​scrapy的官方文档: <a href="https://docs.scrapy.org/en/latest/">https://docs.scrapy.org/en/latest/</a></p><h2 id="三-scrapy工作流程-重点"><a href="#三-scrapy工作流程-重点" class="headerlink" title="三, scrapy工作流程(重点)"></a>三, scrapy工作流程(重点)</h2><p>​之前我们所编写的爬虫的逻辑: </p><p><img src="/images/scrapy01/image-20210803105808636.png" alt="images/scrapy01/image-20210803105808636"></p><p>​scrapy的工作流程: </p><p><img src="/images/scrapy01/image-20210803113438252.png" alt="images/scrapy01/image-20210803113438252"></p><p>整个工作流程, </p><ol><li><p>爬虫中起始的url构造成request对象, 并传递给调度器. </p></li><li><p><code>引擎</code>从<code>调度器</code>中获取到request对象. 然后交给<code>下载器</code></p></li><li><p>由<code>下载器</code>来获取到页面源代码, 并封装成response对象. 并回馈给<code>引擎</code></p></li><li><p><code>引擎</code>将获取到的response对象传递给<code>spider</code>, 由<code>spider</code>对数据进行解析(parse). 并回馈给<code>引擎</code></p></li><li><p><code>引擎</code>将数据传递给pipeline进行数据持久化保存或进一步的数据处理. </p></li><li><p>在此期间如果spider中提取到的并不是数据. 而是子页面url. 可以进一步提交给调度器, 进而重复<code>步骤2</code>的过程</p></li></ol><p>上述过程中一直在重复着几个东西, </p><ol><li><p>引擎(engine)</p><p>scrapy的核心, 所有模块的衔接, 数据流程梳理.</p></li><li><p>调度器(scheduler)</p><p>本质上这东西可以看成是一个队列. 里面存放着一堆我们即将要发送的请求. 可以看成是一个url的容器. 它决定了下一步要去爬取哪一个url. 通常我们在这里可以对url进行去重操作.  </p></li><li><p>下载器(downloader)</p><p>它的本质就是用来发动请求的一个模块. 小白们完全可以把它理解成是一个get_page_source()的功能. 只不过这货返回的是一个response对象. </p></li><li><p>爬虫(spider)</p><p>这是我们要写的第一个部分的内容, 负责解析下载器返回的response对象.从中提取到我们需要的数据. </p></li><li><p>管道(pipeline)</p><p>这是我们要写的第二个部分的内容, 主要负责数据的存储和各种持久化操作.</p></li></ol><p>经过上述的介绍来看, scrapy其实就是把我们平时写的爬虫进行了四分五裂式的改造. 对每个功能进行了单独的封装, 并且, 各个模块之间互相的不做依赖. 一切都由引擎进行调配. 这种思想希望你能知道–解耦. 让模块与模块之间的关联性更加的松散. 这样我们如果希望替换某一模块的时候会非常的容易. 对其他模块也不会产生任何的影响. </p><p>到目前为止, 我们对scrapy暂时了解这么多就够了. 后面会继续在这个图上进一步展开. </p><h2 id="四-scrapy安装"><a href="#四-scrapy安装" class="headerlink" title="四, scrapy安装"></a>四, scrapy安装</h2><p>​在windows上安装scrapy是一个很痛苦的事情. 可能会出现各种各样的异常BUG. </p><p>​先使用pip直接安装看看报错不</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple scrapy </span><br></pre></td></tr></table></figure><p>​如果安装成功, 直接去创建项目即可. 如果报错可能需要安装VC++14.0库才可以. 安装的时候一定不要死记安装步骤, 要观察报错信息. 根据报错信息进行一点点的调整, 多试几次pip. 直至success. </p><p>如果上述过程还是无法正常安装scrapy, 可以考虑用下面的方案来安装:</p><ol><li><p>安装wheel</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install wheel</span><br></pre></td></tr></table></figure></li><li><p>下载twisted安装包, <a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted">https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</a></p><p><img src="/images/scrapy01/image-20210803144429440.png" alt="images/scrapy01/image-20210803144429440"></p><ol start="3"><li>用wheel安装twisted.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install Twisted‑<span class="number">21.7</span><span class="number">.0</span>‑py3‑none‑<span class="built_in">any</span>.whl</span><br></pre></td></tr></table></figure></li><li><p>安装pywin32</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pywin32</span><br></pre></td></tr></table></figure></li><li><p>安装scrapy</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br></pre></td></tr></table></figure></li></ol><p>总之, 最终你的控制台输入<code>scrapy version</code>能显示版本号. 就算成功了</p><h2 id="五-scrapy实例"><a href="#五-scrapy实例" class="headerlink" title="五, scrapy实例"></a>五, scrapy实例</h2><p>​接下来, 我们用scrapy来完成一个超级简单的爬虫, 目标: 深入理解Scrapy工作的流程, 以及各个模块之间是如何搭配工作的. </p><ol><li><p>创建项目</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject 项目名称</span><br></pre></td></tr></table></figure><p>示例:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject mySpider_2</span><br></pre></td></tr></table></figure><p>创建好项目后, 我们可以在pycharm里观察到scrapy帮我们创建了一个文件夹, 里面的目录结构如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mySpider_2   <span class="comment"># 项目所在文件夹, 建议用pycharm打开该文件夹</span></span><br><span class="line">    ├── mySpider_2  <span class="comment"># 项目跟目录</span></span><br><span class="line">    │   ├── __init__.py</span><br><span class="line">    │   ├── items.py  <span class="comment"># 封装数据的格式</span></span><br><span class="line">    │   ├── middlewares.py  <span class="comment"># 所有中间件</span></span><br><span class="line">    │   ├── pipelines.py<span class="comment"># 所有的管道</span></span><br><span class="line">    │   ├── settings.py<span class="comment"># 爬虫配置信息</span></span><br><span class="line">    │   └── spiders<span class="comment"># 爬虫文件夹, 稍后里面会写入爬虫代码</span></span><br><span class="line">    │       └── __init__.py</span><br><span class="line">    └── scrapy.cfg<span class="comment"># scrapy项目配置信息,不要删它,别动它,善待它. </span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>创建爬虫</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd 文件夹  <span class="comment"># 进入项目所在文件夹</span></span><br><span class="line">scrapy genspider 爬虫名称 允许抓取的域名范围</span><br></pre></td></tr></table></figure><p>示例:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd mySpider_2</span><br><span class="line">scrapy genspider youxi 4399.com</span><br></pre></td></tr></table></figure><p>效果:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(base) sylardeMBP:第七章 sylar$ cd mySpider_2</span><br><span class="line">(base) sylardeMBP:mySpider_2 sylar$ ls</span><br><span class="line">mySpider_2      scrapy.cfg</span><br><span class="line">(base) sylardeMBP:mySpider_2 sylar$ scrapy genspider youxi http://www<span class="number">.4399</span>.com/</span><br><span class="line">Created spider <span class="string">&#x27;youxi&#x27;</span> using template <span class="string">&#x27;basic&#x27;</span> <span class="keyword">in</span> module:</span><br><span class="line">  mySpider_2.spiders.youxi</span><br><span class="line">(base) sylardeMBP:mySpider_2 sylar$ </span><br></pre></td></tr></table></figure><p>至此, 爬虫创建完毕, 我们打开文件夹看一下. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">├── mySpider_2</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── items.py</span><br><span class="line">│   ├── middlewares.py</span><br><span class="line">│   ├── pipelines.py</span><br><span class="line">│   ├── settings.py</span><br><span class="line">│   └── spiders</span><br><span class="line">│       ├── __init__.py</span><br><span class="line">│       └── youxi.py   <span class="comment"># 多了一个这个. </span></span><br><span class="line">└── scrapy.cfg</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>编写数据解析过程</p><p>完善youxi.py中的内容. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">YouxiSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;youxi&#x27;</span>  <span class="comment"># 该名字非常关键, 我们在启动该爬虫的时候需要这个名字</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;4399.com&#x27;</span>]  <span class="comment"># 爬虫抓取的域.</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.4399.com/flash/&#x27;</span>]  <span class="comment"># 起始页</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response, **kwargs</span>):</span><br><span class="line">        <span class="comment"># response.text  # 页面源代码</span></span><br><span class="line">        <span class="comment"># response.xpath()  # 通过xpath方式提取</span></span><br><span class="line">        <span class="comment"># response.css()  # 通过css方式提取</span></span><br><span class="line">        <span class="comment"># response.json() # 提取json数据</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用我们最熟悉的方式: xpath提取游戏名称, 游戏类别, 发布时间等信息</span></span><br><span class="line">        li_list = response.xpath(<span class="string">&quot;//ul[@class=&#x27;n-game cf&#x27;]/li&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">            name = li.xpath(<span class="string">&quot;./a/b/text()&quot;</span>).extract_first()</span><br><span class="line">            category = li.xpath(<span class="string">&quot;./em/a/text()&quot;</span>).extract_first()</span><br><span class="line">            date = li.xpath(<span class="string">&quot;./em/text()&quot;</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            dic = &#123;</span><br><span class="line">                <span class="string">&quot;name&quot;</span>: name,</span><br><span class="line">                <span class="string">&quot;category&quot;</span>: category,</span><br><span class="line">                <span class="string">&quot;date&quot;</span>: date</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将提取到的数据提交到管道内.</span></span><br><span class="line">            <span class="comment"># 注意, 这里只能返回 request对象, 字典, item数据, or None</span></span><br><span class="line">            <span class="keyword">yield</span> dic</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>注意: </p><p>&#x3D;&#x3D;spider返回的内容只能是字典, requestes对象, item数据或者None. 其他内容一律报错&#x3D;&#x3D;</p><p>运行爬虫: </p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl 爬虫名字</span><br></pre></td></tr></table></figure><p>实例:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl youxi</span><br></pre></td></tr></table></figure></li><li><p>编写pipeline.对数据进行简单的保存</p><p>数据传递到pipeline, 我们先看一下在pipeline中的样子. </p><p>首先修改settings.py文件中的pipeline信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="comment"># 前面是pipeline的类名地址               </span></span><br><span class="line">    <span class="comment"># 后面是优先级, 优先级月低越先执行</span></span><br><span class="line">   <span class="string">&#x27;mySpider_2.pipelines.Myspider2Pipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后我们修改一下pipeline中的代码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Myspider2Pipeline</span>:</span><br><span class="line">    <span class="comment"># 这个方法的声明不能动!!! 在spider返回的数据会自动的调用这里的process_item方法. </span></span><br><span class="line">    <span class="comment"># 你把它改了. 管道就断了</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></li></ol><h2 id="六-自定义数据传输结构item"><a href="#六-自定义数据传输结构item" class="headerlink" title="六, 自定义数据传输结构item"></a>六, 自定义数据传输结构item</h2><p>​在上述案例中, 我们使用字典作为数据传递的载体, 但是如果数据量非常大. 由于字典的key是随意创建的. 极易出现问题,  此时再用字典就不合适了. Scrapy中提供item作为数据格式的声明位置. 我们可以在items.py文件提前定义好该爬虫在进行数据传输时的数据格式. 然后再写代码的时候就有了数据名称的依据了. </p><p>item.py文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GameItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># 定义数据结构</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    category = scrapy.Field()</span><br><span class="line">    date = scrapy.Field()</span><br></pre></td></tr></table></figure><p>spider中. 这样来使用:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mySpider_2.items <span class="keyword">import</span> GameItem</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下代码在spider中的parse替换掉原来的字典</span></span><br><span class="line">item = GameItem()</span><br><span class="line">item[<span class="string">&quot;name&quot;</span>] = name</span><br><span class="line">item[<span class="string">&quot;category&quot;</span>] = category</span><br><span class="line">item[<span class="string">&quot;date&quot;</span>] = date</span><br><span class="line"><span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><h2 id="七-scrapy使用小总结"><a href="#七-scrapy使用小总结" class="headerlink" title="七, scrapy使用小总结"></a>七, scrapy使用小总结</h2><p>至此, 我们对scrapy有了一个非常初步的了解和使用. 快速总结一下. scrapy框架的使用流程: </p><ol><li>创建爬虫项目.   <code>scrapy startproject xxx     </code></li><li>进入项目目录.    <code>cd xxx  </code></li><li>创建爬虫            <code>scrapy genspider 名称 抓取域</code></li><li>编写<code>item.py</code> 文件, 定义好数据item</li><li>修改spider中的parse方法. 对返回的响应response对象进行解析. 返回item</li><li>在pipeline中对数据进行保存工作. </li><li>修改<code>settings.py</code>文件, 将pipeline设置为生效, 并设置好优先级</li><li>启动爬虫   <code>scrapy crawl 名称</code></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="爬虫开发" scheme="http://blog.ioimp.top/categories/%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91/"/>
    
    
    <category term="Scrapy爬虫" scheme="http://blog.ioimp.top/tags/Scrapy%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>搭建github博客教程</title>
    <link href="http://blog.ioimp.top/2023/12/03/%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"/>
    <id>http://blog.ioimp.top/2023/12/03/%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/</id>
    <published>2023-12-03T03:47:04.000Z</published>
    <updated>2023-12-03T03:52:10.585Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="【2023最新版】Hexo-github搭建个人博客并绑定个人域名"><a href="#【2023最新版】Hexo-github搭建个人博客并绑定个人域名" class="headerlink" title="【2023最新版】Hexo+github搭建个人博客并绑定个人域名"></a>【2023最新版】Hexo+github搭建个人博客并绑定个人域名</h1><h2 id="Hexo-github搭建个人博客并绑定个人域名"><a href="#Hexo-github搭建个人博客并绑定个人域名" class="headerlink" title="Hexo+github搭建个人博客并绑定个人域名"></a><a href="https://so.csdn.net/so/search?q=Hexo&spm=1001.2101.3001.7020">Hexo</a>+github搭建个人博客并绑定个人域名</h2><h4 id="安装并配置Node-js"><a href="#安装并配置Node-js" class="headerlink" title="安装并配置Node.js"></a>安装并配置Node.js</h4><p>Node.js下载:【它让JavaScript成为与PHP、Python、Perl、Ruby等服务端语言平起平坐的脚本语言。】</p><p>教程：<a href="https://blog.csdn.net/weixin/_52799373/article/details/123840137%EF%BC%88%E8%BF%87%E7%A8%8B%E8%AF%A6%E7%BB%86%EF%BC%8C%E8%BF%98%E8%A6%86%E7%9B%96win11%EF%BC%8C%E8%AF%84%E8%AE%BA%E4%B8%8B%E9%9D%A2%E8%BF%98%E6%9C%89%E5%B8%88%E5%8F%94%E7%9A%84%E8%B6%B3%E8%BF%B9%EF%BC%89">https://blog.csdn.net/weixin\_52799373/article/details/123840137（过程详细，还覆盖win11，评论下面还有师叔的足迹）</a></p><p>注意一</p><p>全局安装最常用的 express 模块 进行测试命令如下:</p><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">npm</span> install express -g</span><br></pre></td></tr></table></figure><p>报错图片：</p><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/566631a93202e6841e3b9728d4181c78.png"></p><p>解决方法：</p><ul><li><p>【亲测有效】</p></li><li><p>需要删除 npmrc 文件。</p></li><li><p>**强调：**不是nodejs安装目录npm模块下的那个npmrc文件</p></li><li><p>而是在 C:\Users\（你的用户名）\下的.npmrc文件</p></li><li><p><em><strong>聪明的你，一定想到了直接用evering搜索，省的还要调用文件管理器在一点一点的找</strong></em></p></li></ul><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/c55ce90256466fb4321c7ceec2174333.png"></p><p>注意二</p><p><strong>在文章第四歩测试上查看安装结果</strong></p><p>可能会出现下面照片结果，更改了目录为什么还是C盘目录下，这时候只需要以管理员身份运行命令即可。</p><p>在下面路径下找到cmd.exe并且管理员身份运行即可。</p><p>推测：出像这种现象的原因就是执行权限不够，推荐大家在桌面建立一个快捷方式（管理员命令的）cmd</p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">C:</span>\Windows\System32\cmd.exe</span><br></pre></td></tr></table></figure><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/910c8ab64ab09363b6746b621bc8055a.png"></p><p>创建管理员权限的cmd桌面快捷方式</p><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/a24b65ce36754df5b84ebd74dcae3937.png"></p><h4 id="安装并配置Git"><a href="#安装并配置Git" class="headerlink" title="安装并配置Git"></a>安装并配置Git</h4><p>git是一个并源的分布式版本控制系统，可以有效、高速地处理从很小到非常大的项目版本管理</p><p>Windows系统Git安装教程：<a href="https://www.cnblogs.com/xueweisuoyong/p/11914045.html">https://www.cnblogs.com/xueweisuoyong/p/11914045.html</a></p><h4 id="生成SSH-Keys"><a href="#生成SSH-Keys" class="headerlink" title="生成SSH Keys"></a>生成SSH Keys</h4><p>生成ssh</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;你的邮箱地址&quot;</span></span><br></pre></td></tr></table></figure><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/ffb1c86562ad1f7c9e38918c6a71ac24.png"></p><p>找到秘钥位置并复制</p><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/40764e8f2ea3b220e1492173880c71b4.png"></p><p>测试ssh是否绑定成功</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git<span class="meta">@github</span>.<span class="property">com</span></span><br></pre></td></tr></table></figure><p>如果问你（yes or no）,直接 yes 就可以得到下面这段话</p><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/0b809bb5e4c84157c139e010e1bc7917.png"></p><h4 id="本地访问博客"><a href="#本地访问博客" class="headerlink" title="本地访问博客"></a>本地访问博客</h4><p>1、创建一个名为 Blog 的文件，在里面启用 Git Bash Here</p><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/124885ee5a5be2c2605f30b880329825.png"></p><p>2、初始化hexo</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo <span class="keyword">init</span></span><br></pre></td></tr></table></figure><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/93962986201290891b09b0266420f301.png"></p><p>3、生成本地的hexo页面</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/7c03fe7e8d60be5cf82963e8103f9f6b.png"></p><p>4、访问</p><p>打开本地服务区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://localhost:4000/</span><br></pre></td></tr></table></figure><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/975e56860c24c8076fb91ad2ec24e4f2.png"></p><blockquote><p>长按 Ctrl + c 关闭服务器</p></blockquote><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/745143e1d8f6411d54566506d555c313.png"></p><h4 id="上传到Github"><a href="#上传到Github" class="headerlink" title="上传到Github"></a>上传到Github</h4><p>修改-config.yml文件</p><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/5dad89898d2144898261ea18aefaa30f.png"></p><p>把图片上位置更换成</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">deploy</span>:  <span class="class"><span class="keyword">type</span>: git  repository: 你的github地址  branch: main</span></span><br></pre></td></tr></table></figure><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/85937e8781a64d4a893c4981e00e6a26.png"></p><p>安装hexo-deployer-git 自动部署发布工具</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo<span class="operator">-</span>deployer<span class="operator">-</span>git <span class="comment">--save</span></span><br></pre></td></tr></table></figure><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/681b76b969aa195e15d44c8bfc71565c.png"></p><p>生成页面</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/412c15c15d8987484a32f9546e4f9095.png"></p><p>注意一</p><p>如果报错如下：（无报错，请忽略此条）</p><p>报错信息是提示hexo的yml配置文件 冒号后面少了空格解决方案：到提示行将对应的空格补上即可</p><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/5f5f79045917c6d093df24b22e641516.png"></p><p>本地文件上传到Github上面</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>中间会出现一个登录界面，可以用令牌登录。（令牌及时保存，就看不到了）</p><p>结束以后就上传 Github 就成功了！！！</p><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/15bc2a357a6eac25ec4719dcec9c5007.png"></p><p>注意二</p><p>如果出现如图错误网络报错，再次尝试，多次尝试，直到更换WiFi~~~~</p><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/5a2a4dabd744ba6a750102dfc623993e.png"></p><h4 id="访问GitHub博客"><a href="#访问GitHub博客" class="headerlink" title="访问GitHub博客"></a>访问GitHub博客</h4><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/aa9870ef4369263b6560c13aa5d753c9.png"></p><p>访问博客，开始的页面是初始化页面，没有做美化和增加内容。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://wushishu.github.io/</span><br></pre></td></tr></table></figure><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/c05cd48fac0243ff9963413b078d2fcc.png"></p><h3 id="第二部分-文档学习"><a href="#第二部分-文档学习" class="headerlink" title="第二部分 文档学习"></a>第二部分 文档学习</h3><h4 id="撰写博客"><a href="#撰写博客" class="headerlink" title="撰写博客"></a>撰写博客</h4><p><em><strong>电脑要必须有Typora！电脑要必须有Typora！电脑要必须有Typora！</strong></em>（重要的事情说三遍）</p><p>文本教程：<a href="https://dhndzwxj.vercel.app/3276806131.html">https://dhndzwxj.vercel.app/3276806131.html</a></p><p>hexo标签教程：<a href="http://haiyong.site/post/cda958f2.html%EF%BC%88%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3%E7%9C%8B%E9%9C%80%E6%B1%82%E5%8A%A0%E4%B8%8D%E5%8A%A0%EF%BC%89">http://haiyong.site/post/cda958f2.html（参考文档看需求加不加）</a></p><p>我们打开自己的博客根目录，跟着我一个个了解里面的这些文件（夹）都是干什么的：</p><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/d12390a33a7bc45be0582ac20092c28b.png"></p><ul><li><p><code>_config.yml</code>：俗称站点配置文件，很多与博客网站的格式、内容相关的设置都需要在里面改。</p></li><li><p><code>node_modules</code>:存储Hexo插件的文件，可以实现各种扩展功能。一般不需要管。</p></li><li><p><code>package.json</code>：别问我，我也不知道干嘛的。</p></li><li><p><code>scaffolds</code>：模板文件夹，里面的<code>post.md</code>文件可以设置每一篇博客的模板。具体用起来就知道能干嘛了。</p></li><li><p><code>source</code>：非常重要。所有的个人文件都在里面！</p></li><li><p><code>themes</code>：主题文件夹，可以从<a href="https://hexo.io/themes/" title="Hexo主题官网">Hexo主题官网</a>或者网上大神的Github主页下载各种各样美观的主题，让自己的网站变得逼格高端的关键！</p></li></ul><p>接下来重点介绍<code>source</code>文件夹。新建的博客中，<code>source</code>文件夹下默认只有一个子文件夹——<code>_posts</code>。我们写的博客都放在这个子文件夹里面。我们还可以在<code>source</code>里面新建各种子文件夹满足自己的个性化需求，对初学者而言，我们先把精力放在主线任务上，然后再来搞这些细节。</p><blockquote><p>hexo官方文档：<a href="https://hexo.io/zh-cn/docs/commands.html">https://hexo.io/zh-cn/docs/commands.html</a></p></blockquote><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/8389594494d846358792082d4096fce7.png"></p><p>写好内容后，在命令行一键三连：</p><blockquote><p>‘hexo cl’命令用于清除缓存文件（db.json）和已生成的静态文件（public）。</p><p>例如：在更换主题后，如果发现站点更改不生效，可以运行该命令。</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo cl</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure><p>然后随便打开一个浏览器，在网址栏输入<code>localhost:4000/</code>，就能发现自己的网站更新了！不过这只是在本地进行了更新，要想部署到网上（Github上），输入如下代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>然后在浏览器地址栏输入<code>https://yourname.github.io</code>，或者<code>yourname.github.io</code>就能在网上浏览自己的博客了！</p><p>以上，我们的博客网站1.0版本就搭建完成了，如果没有更多的需求，做到这里基本上就可以了。如果有更多的要求，还需要进一步的精耕细作！</p><h4 id="精耕细作"><a href="#精耕细作" class="headerlink" title="精耕细作"></a>精耕细作</h4><p>**海拥\Butterfly 主题美化：**<a href="http://haiyong.site/post/22e1d5da.html">http://haiyong.site/post/22e1d5da.html</a></p><p><strong>Butterfly参考文档（小白慎入，但是他也是你走向DIY必须迈出的一歩）</strong>:<a href="https://butterfly.js.org/posts/dc584b87/#Post-Front-matter">https://butterfly.js.org/posts/dc584b87/#Post-Front-matter</a></p><p>文章中要更改的文件（.yml .bug 等）可以要用viscode打开！！！</p><p>Butterfly 主题安装</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/butterfly</span><br></pre></td></tr></table></figure><p>这里面如果报错，如下图所示（长路漫漫，bug满满）</p><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/e28a8a9ed9eb2848f84075e0e0aeba0e.png"></p><p>只需要在命令行中执行</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --<span class="keyword">global</span> --<span class="keyword">unset</span> http.proxy git config --<span class="keyword">global</span> --<span class="keyword">unset</span> https.proxy</span><br></pre></td></tr></table></figure><p>再次安装主题即可成功</p><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/8e0ff6ac2ae7b9712214f13438a40f27.png"></p><p><strong>应用主题</strong></p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">theme:</span> butterfly</span><br></pre></td></tr></table></figure><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/1a1fed2db4bbb066f43e879fb70cf5c5.png"></p><p><strong>安装插件</strong></p><p>如果你没有 pug 以及 stylus 的渲染器，请下载安装：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo<span class="operator">-</span>renderer<span class="operator">-</span>pug hexo<span class="operator">-</span>renderer<span class="operator">-</span>stylus <span class="comment">--save</span></span><br></pre></td></tr></table></figure><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/cee71aa9a5c2766541aac4d5aa03c48a.png"></p><h4 id="Butterfly-主题美化"><a href="#Butterfly-主题美化" class="headerlink" title="Butterfly 主题美化"></a>Butterfly 主题美化</h4><p>生成文章唯一链接</p><p>Hexo的默认文章链接格式是年，月，日，标题这种格式来生成的。如果你的标题是中文的话，那你的URL链接就会包含中文，</p><p>复制后的URL路径就是把中文变成了一大堆字符串编码，如果你在其他地方用这边文章的url链接，偶然你又修改了改文章的标题，那这个URL链接就会失效。为了给每一篇文章来上一个属于自己的链接，写下此教程，利用 hexo-abbrlink 插件，A Hexo plugin to generate static post link based on post titles ,来解决这个问题。 参考github官方： hexo-abbrlink 按照此教程配置完之后如下：</p><p>1、安装插件，在博客根目录 [Blogroot] 下打开终端，运行以下指令：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo<span class="operator">-</span>abbrlink <span class="comment">--save</span></span><br></pre></td></tr></table></figure><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/6af8bde01cd64d3a7e38d3e4c2fd9816.png"></p><p>2、插件安装成功后，在根目录 [Blogroot] 的配置文件 _config.yml 找到 permalink：</p><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/img_convert/34cc88e45553f0dad679ca75f96168c2.png"></p><h4 id="发布博客"><a href="#发布博客" class="headerlink" title="发布博客"></a>发布博客</h4><p>这次了解我上面只有一个HelloWord的时候，为什么不让右键新建，<strong>因为需要命令生成啊，铁汁！</strong></p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">npm</span> <span class="selector-tag">i</span> <span class="selector-tag">hexo-deployer-git</span></span><br></pre></td></tr></table></figure><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo <span class="keyword">new</span> post <span class="string">&quot;新建博客文章名&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo cl &amp;&amp; hexo g  &amp;&amp; hexo s</span><br></pre></td></tr></table></figure><h4 id="hexo更换背景图片"><a href="#hexo更换背景图片" class="headerlink" title="hexo更换背景图片"></a>hexo更换背景图片</h4><p>背景图片参考网址：</p><ul><li><p><a href="https://wallhaven.cc/">https://wallhaven.cc/</a></p></li><li><p><a href="https://wall.alphacoders.com/">https://wall.alphacoders.com/</a></p></li><li><p><a href="https://bz.zzzmh.cn/index">https://bz.zzzmh.cn/index</a></p></li></ul><p><em>本方法解决的是多次同步到GitHub上背景图片未成功的情况</em></p><p>直接更改原文件</p><p>图片所在目录：<code>hexo/themes/landscape/source/css/images/</code></p><p>图片名称：<code>banner.jpg</code></p><h3 id=""><a href="#" class="headerlink" title=""></a></h3><h3 id="第三部分-绑定自己的域名"><a href="#第三部分-绑定自己的域名" class="headerlink" title="第三部分 绑定自己的域名"></a>第三部分 绑定自己的域名</h3><p>博客地址：<a href="https://www.likecs.com/show-30474.html">https://www.likecs.com/show-30474.html</a></p><p><strong>绑定之后你就有有一个自己专属的博客了。</strong></p><p>买一个域名，可以一块钱白嫖，但是续费贵的飞天！！！</p><p><em><strong>注意请谨慎绑定，想我就会出现提交一次 (hexo d) ,需要重新绑定域名</strong></em></p><blockquote><p>声明：如果遇到什么不懂的可以先百度，在不懂可以微信我wushibo0820</p></blockquote><h2 id="问题：解决-103-x69-x74-x40-103-105-116-104-x75-x62-46-x63-111-109-Permission-denied-publickey-fatal-Could-not-read-from-remote-repository-Pleas"><a href="#问题：解决-103-x69-x74-x40-103-105-116-104-x75-x62-46-x63-111-109-Permission-denied-publickey-fatal-Could-not-read-from-remote-repository-Pleas" class="headerlink" title="问题：解决&#103;&#x69;&#x74;&#x40;&#103;&#105;&#116;&#104;&#x75;&#x62;&#46;&#x63;&#111;&#109;: Permission denied (publickey). fatal: Could not read from remote repository. Pleas"></a>问题：解决<a href="mailto:&#103;&#x69;&#x74;&#x40;&#103;&#105;&#116;&#104;&#x75;&#x62;&#46;&#x63;&#111;&#109;">&#103;&#x69;&#x74;&#x40;&#103;&#105;&#116;&#104;&#x75;&#x62;&#46;&#x63;&#111;&#109;</a>: Permission denied (publickey). fatal: Could not read from remote repository. Pleas</h2><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/20200603101902230.png"></p><h4 id="一-原因分析"><a href="#一-原因分析" class="headerlink" title="一:原因分析"></a>一:原因分析</h4><p>Permission denied (publickey) 没有权限的publickey ，出现这错误一般是以下两种原因</p><ul><li>客户端与服务端未生成 ssh key</li><li>客户端与服务端的ssh key不匹配</li></ul><p>找到问题的原因了，解决办法也就有了，重新生成一次ssh key ，服务端也重新配置一次即可。</p><h4 id="-1"><a href="#-1" class="headerlink" title=""></a></h4><h4 id="二-客户端生成ssh-key"><a href="#二-客户端生成ssh-key" class="headerlink" title="二:客户端生成ssh key"></a>二:客户端生成ssh key</h4><p>在cmd里面输入</p><p>ssh-keygen -t rsa -C “<a href="mailto:&#x78;&#x78;&#120;&#x78;&#120;&#120;&#120;&#x78;&#64;&#113;&#x71;&#46;&#99;&#111;&#x6d;">&#x78;&#x78;&#120;&#x78;&#120;&#120;&#120;&#x78;&#64;&#113;&#x71;&#46;&#99;&#111;&#x6d;</a>“</p><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;youremail@example.com&quot;</span></span><br></pre></td></tr></table></figure><p>xxxxxx<a href="mailto:youremail@example.com">@qq.com</a>改为自己的邮箱即可，途中会让你输入密码啥的，不需要管，一路回车即可，会生成你的ssh key。（如果重新生成的话会覆盖之前的ssh key。）</p><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/202006031025357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dfMzE3,size_16,color_FFFFFF,t_70"></p><h4 id="三-输入箭头处路径"><a href="#三-输入箭头处路径" class="headerlink" title="三:输入箭头处路径"></a>三:输入箭头处路径</h4><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/20200603102554725.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dfMzE3,size_16,color_FFFFFF,t_70"></p><h4 id="四-打开id-rsa-pub文件-并且复制内容"><a href="#四-打开id-rsa-pub文件-并且复制内容" class="headerlink" title="四:打开id_rsa.pub文件,并且复制内容"></a>四:打开id_rsa.pub文件,并且复制内容</h4><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/20200603102735768.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dfMzE3,size_16,color_FFFFFF,t_70"></p><h4 id="配置服务端"><a href="#配置服务端" class="headerlink" title="配置服务端"></a>配置服务端</h4><h4 id="五-在github上打开箭头处-点击Setting"><a href="#五-在github上打开箭头处-点击Setting" class="headerlink" title="五:在github上打开箭头处,点击Setting"></a>五:在github上打开箭头处,点击Setting</h4><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/20200603102157756.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dfMzE3,size_16,color_FFFFFF,t_70"></p><h4 id="六-点击SSH-and-GPG-keys"><a href="#六-点击SSH-and-GPG-keys" class="headerlink" title="六:点击SSH and GPG keys"></a>六:点击SSH and GPG keys</h4><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/20200603102223280.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dfMzE3,size_16,color_FFFFFF,t_70"></p><h4 id="七-打开你刚刚生成的id-rsa-pub，将里面的内容复制，进入你的github账号，在settings下，SSH-and-GPG-keys下new-SSH-key，然后将id-rsa-pub里的内容复制到Key中，完成后Add-SSH-Key。"><a href="#七-打开你刚刚生成的id-rsa-pub，将里面的内容复制，进入你的github账号，在settings下，SSH-and-GPG-keys下new-SSH-key，然后将id-rsa-pub里的内容复制到Key中，完成后Add-SSH-Key。" class="headerlink" title="七:打开你刚刚生成的id_rsa.pub，将里面的内容复制，进入你的github账号，在settings下，SSH and GPG keys下new SSH key，然后将id_rsa.pub里的内容复制到Key中，完成后Add SSH Key。"></a>七:打开你刚刚生成的id_rsa.pub，将里面的内容复制，进入你的github账号，在settings下，SSH and GPG keys下new SSH key，然后将id_rsa.pub里的内容复制到Key中，完成后Add SSH Key。</h4><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/2020060310174263.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dfMzE3,size_16,color_FFFFFF,t_70"></p><h4 id="八-然后添加后入下图所示"><a href="#八-然后添加后入下图所示" class="headerlink" title="八:然后添加后入下图所示"></a>八:然后添加后入下图所示</h4><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/20200603101823216.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dfMzE3,size_16,color_FFFFFF,t_70"></p><h4 id="九-用idea再次提交文件到-github上-显示提交成功"><a href="#九-用idea再次提交文件到-github上-显示提交成功" class="headerlink" title="九:用idea再次提交文件到 github上,显示提交成功"></a>九:用idea再次提交文件到 github上,显示提交成功</h4><p><img src="https://proxy.iximp.top/https://img-blog.csdnimg.cn/20200603103159683.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dfMzE3,size_16,color_FFFFFF,t_70"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="杂类学习" scheme="http://blog.ioimp.top/categories/%E6%9D%82%E7%B1%BB%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="博客搭建教程" scheme="http://blog.ioimp.top/tags/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes核心概念</title>
    <link href="http://blog.ioimp.top/2023/11/24/kubernetes%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/"/>
    <id>http://blog.ioimp.top/2023/11/24/kubernetes%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/</id>
    <published>2023-11-24T00:29:53.000Z</published>
    <updated>2023-11-24T00:35:24.630Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="01-kubernetes核心概念"><a href="#01-kubernetes核心概念" class="headerlink" title="01-kubernetes核心概念"></a>01-kubernetes核心概念</h2><p>1 有了docker 什么要用kubernetes</p><p>2 多容器跨主机提供服务</p><p>3 多容器分布节点部署</p><p>4 多容器的升级</p><p>5 高效管理容器</p><p>docker管理工具</p><p>Docker compose, docker machine, docker swarm</p><p>常见的容器编排工具：</p><p>kubernetes  google伯格系统</p><p>swarm  2018已经被docker废弃</p><p>mesos marathon  容器编排组件</p><!-- ![img](01-kubernetes核心概念.assets\kuadm01-17006173052243.png) --><h2 id="02-kubernetes是什么"><a href="#02-kubernetes是什么" class="headerlink" title="02-kubernetes是什么"></a>02-kubernetes是什么</h2><p>Kubernetes 是一个开源的容器编排引擎，用来对容器化应用进行自动化部署、 扩缩和管理。该项目托管在 <a href="https://www.cncf.io/about">CNCF</a>。</p><p>kubernetes是google 2014年开源的一个容器集群管理系统。简称k8s</p><p>Kubernetes 用于容器化应用程序的部署</p><p>官网</p><p><a href="https://kubernetes.io/zh/">https://kubernetes.io/zh/</a></p><h2 id="03-kubernetes架构与组件"><a href="#03-kubernetes架构与组件" class="headerlink" title="03-kubernetes架构与组件"></a>03-kubernetes架构与组件</h2><p>master 管理节点（管理集群）</p><p>Kubectl 管理工具</p><p>Api server,scheduler, conttroller manager，etcd分布式键值存储数据库</p><p>API Server：所有服务访问的唯一入口，提供认证、授权、访问控制、API 注册和发现等机制</p><p>Scheduler：负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上</p><p>Controller Manager：负责维护集群的状态，比如副本期望数量、故障检测、自动扩展、滚动更新等</p><p>etcd：键值对数据库，保存了整个集群的状态</p><p>Node 工作节点（运行应用）</p><p>Kubelet 代理</p><p>负责维护容器的生命周期，同时也负责 Volume 和网络的管理</p><p>kubelet的主要功能就是定时从node上获取 pod&#x2F;container 的期望状态。并负责管理pod和它们上面的容器，如images镜像、volumes、etc并确保这些Pod正常运行，能实时返回Pod的运行状态。</p><p>注意</p><p>在kubernetes 的设计中，最基本的管理单位是 pod，而不是 container。pod 是 kubernetes 在容器上的一层封装，由一组运行在同一主机的一个或者多个容器组成。</p><p>Kube-proxy </p><p>负责为 Service 提供 cluster 内部的服务发现和负载均衡</p><p>kube-proxy的作用主要是负责service的实现，具体来说，就是实现了内部从pod到service和外部的从node port向service的访问，其实就是pod的网络代理</p><p>Container runtime：</p><p>负责镜像管理以及 Pod 和容器的真正运行</p><p>除了核心组件，还有一些推荐的插件：<br>CoreDNS：可以为集群中的 SVC 创建一个域名 IP 的对应关系解析的 DNS 服务<br>Dashboard：为Kubernetes 集群提供了一个 B&#x2F;S 架构的访问入口<br>Ingress Controller：官方只能够实现四层的网络代理，而 Ingress 可以实现七层的代理<br>Prometheus：给 Kubernetes 集群提供资源监控的能力<br>Federation： 提供一个可以跨集群中心多 Kubernetes 的统一管理功能，提供跨可用区的集群</p><h2 id="05-kubernetes各个组件调用关系"><a href="#05-kubernetes各个组件调用关系" class="headerlink" title="05-kubernetes各个组件调用关系"></a>05-kubernetes各个组件调用关系</h2><p>master：集群的控制平面，负责集群的决策 ( 管理 )<br>ApiServer : 资源操作的唯一入口，接收用户输入的命令，提供认证、授权、API注册和发现等机制<br>Scheduler : 负责集群资源调度，按照预定的调度策略将Pod调度到相应的node节点上<br>ControllerManager : 负责维护集群的状态，比如程序部署安排、故障检测、自动扩展、滚动更新等<br>Etcd ：负责存储集群中各种资源对象的信息</p><p>node：集群的数据平面，负责为容器提供运行环境 ( 干活 )<br>Kubelet : 负责维护容器的生命周期，即通过控制docker，来创建、更新、销毁容器<br>KubeProxy : 负责提供集群内部的服务发现和负载均衡<br>Docker : 负责节点上容器的各种操作</p><!-- ![img](图片素材/组件调用流程.png) --><p>下面，以部署一个nginx服务来说明kubernetes系统各个组件调用关系：<br>kubernetes环境启动之后，master和node都会将自身的信息存储到etcd数据库中，一个nginx服务的安装请求会首先被发送到master节点的apiServer组件,apiServer组件会调用scheduler组件来决定到底应该把这个服务安装到哪个node节点上,此时它会从etcd中读取各个node节点的信息，然后按照特定的算法进行选择，并将结果告知apiServer，apiServer调用controller-manager去调度Node节点安装nginx服务，kubelet接收到指令后，会通知docker，然后由docker来启动一个nginx的pod，pod是kubernetes的最小操作单元，容器必须跑在pod中至此，一个nginx服务就运行了，如果需要访问nginx，就需要通过kube-proxy来对pod产生访问的代理。这样外界用户就可以访问集群中的nginx服务了</p><h2 id="05-kubernetes概念"><a href="#05-kubernetes概念" class="headerlink" title="05-kubernetes概念"></a>05-kubernetes概念</h2><p>Master：集群控制节点，每个集群需要至少一个master节点负责集群的管控<br>Node：工作负载节点，由master分配容器到这些node工作节点上，然后node节点上的docker负责容器的运行<br>Pod：kubernetes的最小控制单元，容器都是运行在pod中的，一个pod中可以有1个或者多个容器<br>Controller：控制器，通过它来实现对pod的管理，比如启动pod、停止pod、伸缩pod的数量等等<br>Service：pod对外服务的统一入口，下面可以维护者同一类的多个pod<br>Label：标签，用于对pod进行分类，同一类pod会拥有相同的标签<br>NameSpace：命名空间，用来隔离pod的运行环境</p><h2 id="06-kubernetes集群架构"><a href="#06-kubernetes集群架构" class="headerlink" title="06-kubernetes集群架构"></a>06-kubernetes集群架构</h2><p>官方文档 概念 和任务。</p><p>官网</p><p><a href="https://kubernetes.io/zh/">https://kubernetes.io/zh/</a></p><p>k8s的部署方式：</p><p>minikube：一个用于快速搭建单节点kubernetes的工具，minikube顾名思义即迷你型Kubernetes，非常适合快速学习k8s的各个组件的作用及yml的编写。</p><p>Kubeadm 部署  ：官方提供的一个用于快速搭建kubernetes集群的工具</p><ul><li><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/">https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/</a></li></ul><p>二进制部署  ： 从官网下载每个组件的二进制包，依次去安装，此方式对于理解kubernetes组件更加有效</p><ul><li><a href="https://github.com/kubernetes/kubernetes">https://github.com/kubernetes/kubernetes</a></li></ul><p>第三方工具部署： rancher  webui</p><p>k8s服务器硬件配置：</p><!-- <img src="图片素材/image-20220607193947482.png" alt="image-20220607193947482" style="zoom:50%;" /> --><p>kubernetes集群大体上分为两类：一主多从和多住多从</p><ul><li><p>一主多从：一台master节点和多台node节点，搭建简单，但是有单机故障风险，适用于测试环境</p></li><li><p>多主多从：多台master节点和多台node节点，搭建相对负载，安全性高，适用于生产环境</p><!-- ![image-20220607194851548](图片素材/image-20220607194851548.png) --></li></ul><h2 id="07-kubeadm快速部署k8s"><a href="#07-kubeadm快速部署k8s" class="headerlink" title="07-kubeadm快速部署k8s"></a>07-kubeadm快速部署k8s</h2><p>第一步首先安装 Linux ubuntu18.04</p><p>ubuntu下载地址：<a href="http://cdimage.ubuntu.com/releases/18.04.2/release/">http://cdimage.ubuntu.com/releases/</a></p><p>Ubuntu18.04设置国内源</p><p>阿里源</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure><p>中科大源</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse</span><br></pre></td></tr></table></figure><p>网易源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">deb http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure><p>清华源</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse</span><br></pre></td></tr></table></figure><p>第二步： 安装docker</p><p>第三步:    安装对应版本的kubeadm工具</p><p>第四步： 初始k8s集群</p><p>第五步： 安装k8s网络插件</p><h2 id="08-部署k8s-v1-26版本"><a href="#08-部署k8s-v1-26版本" class="headerlink" title="08-部署k8s v1.26版本"></a>08-部署k8s v1.26版本</h2><h3 id="8-1-系统环境准备"><a href="#8-1-系统环境准备" class="headerlink" title="8.1 系统环境准备"></a>8.1 系统环境准备</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">1.1 安装所需工具</span><br><span class="line">yum -y install vim</span><br><span class="line">yum -y install wget</span><br><span class="line"># 设置yum源</span><br><span class="line">mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup</span><br><span class="line">wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line">1.2 修改主机名</span><br><span class="line">#master</span><br><span class="line">hostnamectl set-hostname master-01</span><br><span class="line">#node1</span><br><span class="line">hostnamectl set-hostname node-01</span><br><span class="line">#node2</span><br><span class="line">hostnamectl set-hostname node-02</span><br><span class="line">1.3 编辑hosts</span><br><span class="line">[root@localhost ~]# vim /etc/hosts</span><br><span class="line"></span><br><span class="line"># 增加以下内容</span><br><span class="line">192.168.31.249 master-01</span><br><span class="line">192.168.31.250 node-01</span><br><span class="line">192.168.31.251 node-02</span><br><span class="line"></span><br><span class="line">1.4 安装ntpdate并同步时间</span><br><span class="line">yum -y install ntpdate</span><br><span class="line">ntpdate ntp1.aliyun.com</span><br><span class="line"></span><br><span class="line">1.5 安装并配置 bash-completion，添加命令自动补充</span><br><span class="line">yum -y install bash-completion</span><br><span class="line">source /etc/profile</span><br><span class="line">1.6 关闭防火墙</span><br><span class="line">systemctl stop firewalld.service </span><br><span class="line">systemctl disable firewalld.service</span><br><span class="line">1.7 关闭selinux</span><br><span class="line">sed -i &#x27;s/enforcing/disabled/&#x27; /etc/selinux/config  # 永久关闭</span><br><span class="line">1.8 关闭 swap</span><br><span class="line">free -h</span><br><span class="line">sudo swapoff -a</span><br><span class="line">sudo sed -i &#x27;s/.*swap.*/#&amp;/&#x27; /etc/fstab</span><br><span class="line">free -h</span><br><span class="line">二：安装k8s 1.26.x</span><br><span class="line">2.1 安装 Containerd</span><br><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo </span><br><span class="line">sudo yum install -y containerd.io</span><br><span class="line"></span><br><span class="line">systemctl stop containerd.service</span><br><span class="line"></span><br><span class="line">cp /etc/containerd/config.toml /etc/containerd/config.toml.bak</span><br><span class="line">sudo containerd config default &gt; $HOME/config.toml</span><br><span class="line">sudo cp $HOME/config.toml /etc/containerd/config.toml</span><br><span class="line"># 修改 /etc/containerd/config.toml 文件后，要将 docker、containerd 停止后，再启动</span><br><span class="line">sudo sed -i &quot;s#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&quot; /etc/containerd/config.toml</span><br><span class="line"># https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#containerd-systemd</span><br><span class="line"># 确保 /etc/containerd/config.toml 中的 disabled_plugins 内不存在 cri</span><br><span class="line">sudo sed -i &quot;s#SystemdCgroup = false#SystemdCgroup = true#g&quot; /etc/containerd/config.toml</span><br><span class="line"></span><br><span class="line">#启动containerd</span><br><span class="line">systemctl start containerd.service</span><br><span class="line">systemctl status containerd.service</span><br><span class="line">2.2 添加阿里云 k8s 镜像仓库</span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="line"># 是否开启本仓库</span><br><span class="line">enabled=1</span><br><span class="line"># 是否检查 gpg 签名文件</span><br><span class="line">gpgcheck=0</span><br><span class="line"># 是否检查 gpg 签名文件</span><br><span class="line">repo_gpgcheck=0</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line">2.3 将桥接的 IPv4 流量传递到 iptables 的链</span><br><span class="line"># 设置所需的 sysctl 参数，参数在重新启动后保持不变</span><br><span class="line">cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-iptables  = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.ipv4.ip_forward                 = 1</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 应用 sysctl 参数而不重新启动</span><br><span class="line">sudo sysctl --system</span><br><span class="line"></span><br><span class="line"># 启动br_netfilter</span><br><span class="line">modprobe br_netfilter</span><br><span class="line">echo 1 &gt; /proc/sys/net/ipv4/ip_forward</span><br><span class="line">2.4 安装k8s</span><br><span class="line"># 2023-03-02，经过测试，版本号：1.26.2，</span><br><span class="line">yum install -y kubelet-1.26.3-0 kubeadm-1.26.3-0 kubectl-1.26.3-0 --disableexcludes=kubernetes --nogpgcheck</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">systemctl enable kubelet</span><br></pre></td></tr></table></figure><h3 id="8-2-初始化master集群"><a href="#8-2-初始化master集群" class="headerlink" title="8.2 初始化master集群"></a>8.2 初始化master集群</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">如需订制版本安装：</span><br><span class="line">需获取镜像文件,查看需要的镜像</span><br><span class="line">kubeadm config images list</span><br><span class="line"></span><br><span class="line">系统输出:</span><br><span class="line">[root@master-01 ~]# kubeadm config images list</span><br><span class="line">I1122 10:13:55.024056   66494 version.go:256] remote version is much newer: v1.28.4; falling back to: stable-1.26</span><br><span class="line">registry.k8s.io/kube-apiserver:v1.26.11</span><br><span class="line">registry.k8s.io/kube-controller-manager:v1.26.11</span><br><span class="line">registry.k8s.io/kube-scheduler:v1.26.11</span><br><span class="line">registry.k8s.io/kube-proxy:v1.26.11</span><br><span class="line">registry.k8s.io/pause:3.9</span><br><span class="line">registry.k8s.io/etcd:3.5.6-0</span><br><span class="line">registry.k8s.io/coredns/coredns:v1.9.3</span><br><span class="line">[root@master-01 ~]# </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">编辑一个文件, 命名为： install_k8s_images.sh</span><br><span class="line"></span><br><span class="line">vim install_k8s_images.sh</span><br><span class="line"></span><br><span class="line">#! /bin/bash</span><br><span class="line">images=(</span><br><span class="line">    kube-apiserver:v1.26.11</span><br><span class="line">    kube-controller-manager:v1.26.11</span><br><span class="line">    kube-scheduler:v1.26.11</span><br><span class="line">    kube-proxy:v1.26.11</span><br><span class="line">    pause:3.9</span><br><span class="line">    etcd:3.5.6-0</span><br><span class="line">    coredns:1.9.3</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">for imageName in $&#123;images[@]&#125; ; do</span><br><span class="line">    docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName</span><br><span class="line">    docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">将文件设置为可运行：</span><br><span class="line">chmod a+x install_k8s_images.sh</span><br><span class="line"></span><br><span class="line">运行 install_k8s_images.sh 安装所需要的镜像</span><br><span class="line">./install_k8s_images.sh</span><br><span class="line"></span><br><span class="line">注意：以上步骤需要在Master和work机器完成</span><br></pre></td></tr></table></figure><p>通过kubeadm命令初始化master集群</p><p>–kubernetes-version: 用于指定k8s版本；<br>–apiserver-advertise-address：用于指定kube-apiserver监听的ip地址,就是 master本机IP地址。<br>–pod-network-cidr：用于指定Pod的网络范围； 10.244.0.0&#x2F;16<br>–service-cidr：用于指定SVC的网络范围；<br>–image-repository: 指定阿里云镜像仓库地址</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init --kubernetes-version=v1.26.11 --apiserver-advertise-address=0.0.0.0 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.26.11 --pod-network-cidr=10.244.0.0/16</span><br></pre></td></tr></table></figure><!-- ![](01-kubernetes核心概念.assets/kuadm01-17006173052243.png) --><p>8.2.1配置集群配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">设置配置文件变量</span><br><span class="line">export KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line"></span><br><span class="line">echo export KUBECONFIG=/etc/kubernetes/admin.conf &gt;&gt; /etc/profile</span><br><span class="line">source /etc/profile</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>8.2.2 各个工作节点，执行如下命令将工作节点加入集群</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join 192.168.31.249:6443 --token f1226v.yjwsbpyrjs9oojmc --discovery-token-ca-cert-hash sha256:34220ce3bb6d29429afa0b1c6505ff14cc1aee7373a9b973904abd592aec3b3c</span><br></pre></td></tr></table></figure><p>查看加入集群的工作节点与token</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@master-01 ~]# kubeadm token create --print-join-command</span><br><span class="line">kubeadm join 192.168.31.249:6443 --token ojjtbl.lkchb9823sltuhwn --discovery-token-ca-cert-hash sha256:34220ce3bb6d29429afa0b1c6505ff14cc1aee7373a9b973904abd592aec3b3c </span><br><span class="line">[root@master-01 ~]# kubectl get node</span><br><span class="line">NAME        STATUS     ROLES           AGE     VERSION</span><br><span class="line">master-01   NotReady   control-plane   20m     v1.26.3</span><br><span class="line">node-01     NotReady   &lt;none&gt;          15m     v1.26.3</span><br><span class="line">node-02     NotReady   &lt;none&gt;          9m37s   v1.26.3</span><br><span class="line">[root@master-01 ~]# </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>8.2.3 配置k8s网络</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"> maste节点配置网络,使用Calico</span><br><span class="line"># 下载</span><br><span class="line">wget --no-check-certificate https://projectcalico.docs.tigera.io/archive/v3.25/manifests/calico.yaml</span><br><span class="line"></span><br><span class="line"># 修改 calico.yaml 文件</span><br><span class="line">vim calico.yaml</span><br><span class="line"># 在 - name: CLUSTER_TYPE 下方添加如下内容</span><br><span class="line">- name: CLUSTER_TYPE</span><br><span class="line">  value: &quot;k8s,bgp&quot;</span><br><span class="line">  # 下方为新增内容</span><br><span class="line">- name: IP_AUTODETECTION_METHOD</span><br><span class="line">  value: &quot;interface=网卡名称&quot;</span><br><span class="line">  # INTERFACE_NAME=ens33</span><br><span class="line">  </span><br><span class="line">设置pod网络为10.244.0.0/16 与前面初始步骤一致</span><br><span class="line">- name: CALICO_IPV4POOL_CIDR</span><br><span class="line">  value: &quot;10.244.0.0/16&quot;</span><br><span class="line"></span><br><span class="line">kubeadm 支持多种网络插件，我们选择 Calico 网络插件（kubeadm 仅支持基于容器网络接口（CNI）的网络（不支持kubenet）。），默认情况下，它给出的pod的IP段地址是 192.168.0.0/16 ,如果你的机器已经使用了此IP段，就需要修改这个配置项，将其值改为在初始化 Master 节点时使用 kubeadm init –pod-network-cidr=x.x.x.x/x 的IP地址段</span><br><span class="line"></span><br><span class="line">然后在部署 Pod 网络组件，当然对于现在的网络环境来说这些都不是必须的</span><br><span class="line">kubectl apply -f calico.yaml</span><br><span class="line"> 稍等片刻查询 pod 详情，你也可以使用 watch 命令来实时查看 pod 的状态，等待 Pod 网络组件部署成功后，就可以看到一些信息了，包括 Pod 的 IP 地址信息，这个过程时间可能会有点长。</span><br><span class="line"> </span><br><span class="line">kubectl get pods -n &lt;namespace&gt; --watch</span><br><span class="line">可以通过Ctrl+C终止这个Watch模式</span><br><span class="line"></span><br><span class="line">查看日志命令用于拍错</span><br><span class="line">journalctl -xefu kubelet</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="8-3-kubelet启用lvs负载"><a href="#8-3-kubelet启用lvs负载" class="headerlink" title="8.3 kubelet启用lvs负载"></a>8.3 kubelet启用lvs负载</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">安装ipvsadm</span><br><span class="line">yum install -y ipvsadm</span><br><span class="line"></span><br><span class="line">手动载入如下模块</span><br><span class="line">modprobe ip_vs</span><br><span class="line">modprobe ip_vs_rr</span><br><span class="line">modprobe ip_vs_wrr</span><br><span class="line">modprobe ip_vs_sh</span><br><span class="line"></span><br><span class="line">确认ipvs模块以及载入</span><br><span class="line">root@manager:~# lsmod|grep ip_vs</span><br><span class="line">ip_vs_sh               16384  0</span><br><span class="line">ip_vs_wrr              16384  0</span><br><span class="line">ip_vs_rr               16384  12</span><br><span class="line">ip_vs                 172032  18 ip_vs_rr,ip_vs_sh,ip_vs_wrr</span><br><span class="line">nf_conntrack          172032  7 xt_conntrack,nf_nat,nf_nat_ipv6,ipt_MASQUERADE,nf_nat_ipv4,nf_conntrack_netlink,ip_vs</span><br><span class="line">nf_defrag_ipv6         20480  2 nf_conntrack,ip_vs</span><br><span class="line">libcrc32c              16384  4 nf_conntrack,nf_nat,xfs,ip_vs</span><br><span class="line"></span><br><span class="line">否则要自己构建模块载入配置文件 并设置权限,以下是debian10的配置实例</span><br><span class="line">echo &gt; /etc/systemd/system/kubelet.service.d/10-proxy-ipvs.conf &lt;&lt;-&#x27;EOF&#x27;</span><br><span class="line">#!/bin/bash</span><br><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack</span><br><span class="line">modprobe -- br_netfilter</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">chmod +x /etc/systemd/system/kubelet.service.d/10-proxy-ipvs.conf </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">本例是cenntos配置实例</span><br><span class="line">cat &gt; /usr/lib/systemd/system/kubelet.service.d/10-proxy-ipvs.conf &lt;&lt; &#x27;EOF&#x27;</span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=-modprobe ip_vs</span><br><span class="line">ExecStartPre=-modprobe ip_vs_rr</span><br><span class="line">ExecStartPre=-modprobe ip_vs_wrr</span><br><span class="line">ExecStartPre=-modprobe ip_vs_sh</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">chmod +x /usr/lib/systemd/system/kubelet.service.d/10-proxy-ipvs.conf</span><br><span class="line">scp /usr/lib/systemd/system/kubelet.service.d/10-proxy-ipvs.conf root@192.168.3.166:/usr/lib/systemd/system/kubelet.service.d/10-proxy-ipvs.conf</span><br><span class="line">scp /usr/lib/systemd/system/kubelet.service.d/10-proxy-ipvs.conf root@192.168.3.167:/usr/lib/systemd/system/kubelet.service.d/10-proxy-ipvs.conf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">更改kube-proxy配置</span><br><span class="line">kubectl edit configmap kube-proxy -n kube-system</span><br><span class="line"></span><br><span class="line">找到如下部分的内容41行左右。</span><br><span class="line"></span><br><span class="line">   minSyncPeriod: 0s</span><br><span class="line">      scheduler: &quot;&quot;</span><br><span class="line">      syncPeriod: 30s</span><br><span class="line">    kind: KubeProxyConfiguration</span><br><span class="line">    metricsBindAddress: 127.0.0.1:10249</span><br><span class="line">    mode: &quot;ipvs&quot;                          # 加上这个</span><br><span class="line">    nodePortAddresses: null</span><br><span class="line">其中mode原来是空，默认为iptables模式，改为ipvs保存退出</span><br><span class="line">scheduler默认是空，默认负载均衡算法为轮训</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">重启验证：   </span><br><span class="line">kubectl logs -n kube-system -f kube-proxy-8kzr2     #(8kzr2为ID)</span><br><span class="line">I1122 06:58:15.474822       1 node.go:136] Successfully retrieved node IP: 192.168.3.167</span><br><span class="line">I1122 06:58:15.474933       1 server_others.go:111] kube-proxy node IP is an IPv4 address (192.168.3.167), assume IPv4 operation</span><br><span class="line">I1122 06:58:15.847846       1 server_others.go:259] Using ipvs Proxier.</span><br><span class="line">W1122 06:58:15.848556       1 proxier.go:434] IPVS scheduler not specified, use rr by default</span><br><span class="line">I1122 06:58:15.848926       1 server.go:650] Version: v1.19.2</span><br><span class="line">I1122 06:58:15.849742       1 conntrack.go:100] Set sysctl &#x27;net/netfilter/nf_conntrack_max</span><br><span class="line"> </span><br><span class="line">ipvsadm -ln  #查看ipvsadm规则</span><br><span class="line">[root@master-01 ~]# ipvsadm -Ln</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  10.96.0.1:443 rr</span><br><span class="line">  -&gt; 192.168.31.249:6443          Masq    1      4          0         </span><br><span class="line">TCP  10.96.0.10:53 rr</span><br><span class="line">  -&gt; 10.244.184.4:53              Masq    1      0          0         </span><br><span class="line">  -&gt; 10.244.184.5:53              Masq    1      0          0         </span><br><span class="line">TCP  10.96.0.10:9153 rr</span><br><span class="line">  -&gt; 10.244.184.4:9153            Masq    1      0          0         </span><br><span class="line">  -&gt; 10.244.184.5:9153            Masq    1      0          0         </span><br><span class="line">UDP  10.96.0.10:53 rr</span><br><span class="line">  -&gt; 10.244.184.4:53              Masq    1      0          0         </span><br><span class="line">  -&gt; 10.244.184.5:53              Masq    1      0          0         </span><br><span class="line">[root@master-01 ~]# </span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="8-4-kubectl-命令补全"><a href="#8-4-kubectl-命令补全" class="headerlink" title="8.4 kubectl 命令补全"></a>8.4 kubectl 命令补全</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install -y bash-completion</span><br><span class="line">source &lt;(kubectl completion bash)</span><br><span class="line">echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc</span><br><span class="line">source  ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="8-5kubectl工具常用命令"><a href="#8-5kubectl工具常用命令" class="headerlink" title="8.5kubectl工具常用命令"></a>8.5kubectl工具常用命令</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">查看帮助：kubectl -h</span><br><span class="line">查看可配置的资源对象：kubectl api-resources</span><br><span class="line">查看所有node信息：kubectl get node</span><br><span class="line">查看特定ns中的pod : kubectl get pod -n kube-system</span><br><span class="line">查看RC和service列表：kubectl get rc,svc</span><br><span class="line">显示Pod的详细信息：kubectl describe pod pod-name</span><br><span class="line">查看节点my-node的详细信息:   kubectl describe nodes my-node </span><br><span class="line">根据yaml创建资源：kubectl create -f pod.yaml  或 kubectl apply -f pod.yaml</span><br><span class="line">#apply 可以重复执行，create 不行</span><br><span class="line">基于pod.yaml定义的名称删除pod：kubectl delete -f pod.yaml </span><br><span class="line">删除所有包含某个label的pod和service：kubectl delete pod,svc -l name=label-name</span><br><span class="line">删除所有Pod：kubectl delete pod --all</span><br><span class="line">查看endpoint列表：kubectl get endpoints</span><br><span class="line">执行pod的date命令：</span><br><span class="line">kubectl exec pod-name -- date</span><br><span class="line">kubectl exec pod-name -- bash</span><br><span class="line">kubectl exec pod-name -- ping 10.24.51.9</span><br><span class="line"></span><br><span class="line">获得pod中某个容器的TTY（相当于登录容器）：</span><br><span class="line">kubectl exec -it pod-name -c container-name -- bash</span><br><span class="line">#查看容器的日志</span><br><span class="line">kubectl logs pod-name</span><br><span class="line">#实时查看日志</span><br><span class="line">kubectl logs -f pod-name</span><br><span class="line">#若pod只有一个容器，可以不加-c</span><br><span class="line">kubectl log pod-name -c container_name</span><br><span class="line">查看注释：</span><br><span class="line">kubectl explain pod</span><br><span class="line">kubectl explain pod.apiVersion</span><br><span class="line">查看节点labels：kubectl get node --show-label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">kubectl创建pod实例：</span><br><span class="line">kubectl create deployment nginx --image=nginx</span><br><span class="line">kubectl get pod nginx-748c667d99-b542k --watch  #查看容器启动状态</span><br><span class="line">kubectl get pod -n default -o wide</span><br><span class="line">kubectl get deployments.apps</span><br><span class="line">kubectl expose deployment nginx --port=80 --type=NodePort</span><br><span class="line">kubectl get pod,svc</span><br><span class="line"></span><br><span class="line">扩容多个副本 拉伸实例</span><br><span class="line">kubectl scale deployment nginx --replicas=2</span><br><span class="line">查看pods会看到两个nginx容器</span><br><span class="line">kubectl get pods</span><br><span class="line"></span><br><span class="line">如需修改端口为31000 可以使用edit选项直接指定。端口固定范围 --service-node-port-range=30000-50000。</span><br><span class="line"></span><br><span class="line">kubectl edit service nginx</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="8-6-registry私用仓库"><a href="#8-6-registry私用仓库" class="headerlink" title="8.6 registry私用仓库"></a>8.6 registry私用仓库</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">docker创建本地局域网私有仓库:</span><br><span class="line"></span><br><span class="line">但有时候使用公共仓库可能不方便，这种情况下用户可以使用registry创建一个本地仓库供私人使用，使用私有仓库有许多优点：节省网络带宽，针对于每个镜像不用每个人都去中央仓库上面去下载，只需要从私有仓库中下载即可；</span><br><span class="line">提供镜像资源利用，针对于公司内部使用的镜像，推送到本地的私有仓库中，以供公司内部相关人员使用。</span><br><span class="line">目前Docker Registry已经升级到了v2，Registryv2使用Go语言编写，在性能和安全性上做了很多优化，重新设计了镜像的存储格式。如果需要安装registry v2，就需要下载registry:2.2的版本。Docker官方提供的工具docker-registry可以用于构建私有的镜像仓库</span><br><span class="line">docker pull registry   #下载私有仓库镜像</span><br><span class="line">docker run -d -p 5000:5000 registry:latest #运行私有仓库容器映射端口号5000</span><br><span class="line">docker run -d --name=my_registry -p 5000:5000 -v /opt/data/registry:/tmp/registry docker.io/registry     #指定本地仓库路径 这样有利于路径的归纳如下</span><br><span class="line"></span><br><span class="line">这将使用官方的   镜像来启动私有仓库。默认情况下，仓库会被创建在 容器的   目录下。你可以通过 -v 参数来将镜像文件存放在 本地的指定路径。例如下面的例子将上传的镜像放到本地的/opt/data/registry 目录</span><br><span class="line"></span><br><span class="line">docker run -d --name=my_registry -p 5000:5000 -v /opt/data/registry:/var/lib/registry docker.io/registry</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">然后回到要上传的客户端主机给要上传的本地镜像&quot;test_nginx&quot;打标签为仓库ip地址/后面是上传仓库后的镜像名称(centos7-nginx)。如下所示：</span><br><span class="line">docker tag test_nginx 192.168.3.138:5000/centos7-nginx</span><br><span class="line"></span><br><span class="line">docker push 192.168.3.138:5000/centos7-nginx  #上载镜像</span><br><span class="line">docker pull 192.168.3.138:5000/centos7-nginx  #下载镜像</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">设置本地docker主机使用本地局域网http私有仓库,默认为https</span><br><span class="line">vi /etc/docker/daemon.json</span><br><span class="line">&#123;&quot;insecure-registries&quot;:[&quot;192.168.3.138:5000&quot;]&#125;</span><br><span class="line"></span><br><span class="line">如下例所示：</span><br><span class="line">cat &gt; /etc/docker/daemon.json &lt;&lt; &#x27;EOF&#x27;</span><br><span class="line">&#123;</span><br><span class="line">        &quot;graph&quot;:&quot;/docker&quot;,</span><br><span class="line">        &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">&quot;insecure-registries&quot;:[&quot;192.168.3.25:5000&quot;],</span><br><span class="line">        &quot;registry-mirrors&quot;: [</span><br><span class="line">               &quot;https://q2gr04ke.mirror.aliyuncs.com&quot;,</span><br><span class="line">               &quot;http://hub-mirror.c.163.com&quot;,</span><br><span class="line">               &quot;https://docker.mirrors.ustc.edu.cn&quot;</span><br><span class="line">          ],</span><br><span class="line">         &quot;bip&quot;: &quot;172.17.0.1/16&quot;,</span><br><span class="line">         &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class="line">         &quot;live-restore&quot;: false,</span><br><span class="line">         &quot;dns&quot; : [ &quot;114.114.114.114&quot;,&quot;8.8.8.8&quot; ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cat &gt; /etc/docker/daemon.json &lt;&lt; &#x27;EOF&#x27;</span><br><span class="line">&#123;</span><br><span class="line">        &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">&quot;insecure-registries&quot;:[&quot;192.168.3.153:5000&quot;],</span><br><span class="line">        &quot;registry-mirrors&quot;: [</span><br><span class="line">               &quot;https://q2gr04ke.mirror.aliyuncs.com&quot;,</span><br><span class="line">               &quot;http://hub-mirror.c.163.com&quot;,</span><br><span class="line">               &quot;https://docker.mirrors.ustc.edu.cn&quot;</span><br><span class="line">          ],</span><br><span class="line">         &quot;bip&quot;: &quot;172.17.0.1/16&quot;,</span><br><span class="line">         &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class="line">         &quot;live-restore&quot;: false,</span><br><span class="line">         &quot;dns&quot; : [ &quot;114.114.114.114&quot;,&quot;8.8.8.8&quot; ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl restart docker  #重启docker服务使设置生效</span><br><span class="line"></span><br><span class="line">查看私有仓库镜像</span><br><span class="line">curl http://192.168.3.138:5000/v2/_catalog</span><br><span class="line">curl -XGET http://192.168.3.138:5000/v2/centos7-nginx/tags/list</span><br><span class="line">docker pull 192.168.3.153:5000/centos7-nginx</span><br><span class="line">2.11 安装registry-web</span><br><span class="line"></span><br><span class="line">docker pull hyper/docker-registry-web</span><br><span class="line"></span><br><span class="line">docker run -d -p 5001:8080 --name registry-web --link my_registry -e REGISTRY_URL=http://192.168.3.25:5000/v2 -e REGISTRY_NAME=192.168.3.25:5000 hyper/docker-registry-web</span><br><span class="line"></span><br><span class="line">命令注释</span><br><span class="line">docker run                                      #运行</span><br><span class="line">-d                                                     #后台运行</span><br><span class="line">-p 5001:8080                               #端口映射</span><br><span class="line">--name registry-web                    #容器命名</span><br><span class="line">--link registry                                 #连接其他容器  加入registry到host</span><br><span class="line">-e REGISTRY_URL=http://registry:5000/v2    #指定仓库地址</span><br><span class="line">-e REGISTRY_NAME=localhost:5000               #仓库命名</span><br><span class="line">hyper/docker-registry-web                                 #被启动的镜像</span><br><span class="line"></span><br><span class="line">curl http://127.0.0.1:5001</span><br><span class="line"></span><br><span class="line">镜像仓库的IP为192.168.3.153实例：</span><br><span class="line">docker run -d -p 5001:8080 --name registry-web --link my_registry -e REGISTRY_URL=http://192.168.3.153:5000/v2 -e REGISTRY_NAME=192.168.3.153:5000 hyper/docker-registry-web</span><br><span class="line"></span><br><span class="line">设置本地docker主机使用本地局域网http私有仓库,默认为https</span><br><span class="line">vim /etc/docker/daemon.json </span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https://4pefdwvq.mirror.aliyuncs.com&quot;],</span><br><span class="line">  &quot;insecure-registries&quot;: [ &quot;192.168.3.25:5000&quot; ],</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check=true&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">systemctl restart docker</span><br><span class="line"></span><br><span class="line">2.12 设置容器开机自启：</span><br><span class="line">docker run --restart=always --net=host --privileged=true -v /web:/usr/local/nginx/html/ -d b8ad90037e3a /bin/bash -c &#x27;/usr/local/nginx/sbin/nginx&#x27;</span><br><span class="line"></span><br><span class="line">注： --restart=always 实用容器开机自启</span><br><span class="line">restart的取值可以是以下3种情况：</span><br><span class="line">no -  容器退出时，不重启容器；</span><br><span class="line">on-failure - 只有在非0状态退出时才从新启动容器；</span><br><span class="line">always - 无论退出状态是如何，都重启容器；</span><br></pre></td></tr></table></figure><p>8.7 containerd指向本地私有仓库</p><p>修改config.toml配置文件</p><p>第61行 指定国内镜像仓库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">61     sandbox_image = &quot;registry.aliyuncs.com/google_containers/pause:3.6&quot;</span><br></pre></td></tr></table></figure><p>114行区域可以指定http私库地址 如下所示</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">144     [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry]</span><br><span class="line">145       config_path = &quot;&quot;</span><br><span class="line">146 </span><br><span class="line">147       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors]</span><br><span class="line">148         [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]</span><br><span class="line">149           endpoint = [&quot;https://registry.cn-hangzhou.aliyuncs.com&quot;]</span><br><span class="line">150         [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;192.168.31.252:5000&quot;]</span><br><span class="line">151           endpoint = [&quot;http://192.168.31.252:5000&quot;]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如下为完整config.toml实例</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br></pre></td><td class="code"><pre><span class="line">[root@master-01 ~]# cat config.toml </span><br><span class="line">disabled_plugins = []</span><br><span class="line">imports = []</span><br><span class="line">oom_score = 0</span><br><span class="line">plugin_dir = &quot;&quot;</span><br><span class="line">required_plugins = []</span><br><span class="line">root = &quot;/var/lib/containerd&quot;</span><br><span class="line">state = &quot;/run/containerd&quot;</span><br><span class="line">temp = &quot;&quot;</span><br><span class="line">version = 2</span><br><span class="line"></span><br><span class="line">[cgroup]</span><br><span class="line">  path = &quot;&quot;</span><br><span class="line"></span><br><span class="line">[debug]</span><br><span class="line">  address = &quot;&quot;</span><br><span class="line">  format = &quot;&quot;</span><br><span class="line">  gid = 0</span><br><span class="line">  level = &quot;&quot;</span><br><span class="line">  uid = 0</span><br><span class="line"></span><br><span class="line">[grpc]</span><br><span class="line">  address = &quot;/run/containerd/containerd.sock&quot;</span><br><span class="line">  gid = 0</span><br><span class="line">  max_recv_message_size = 16777216</span><br><span class="line">  max_send_message_size = 16777216</span><br><span class="line">  tcp_address = &quot;&quot;</span><br><span class="line">  tcp_tls_ca = &quot;&quot;</span><br><span class="line">  tcp_tls_cert = &quot;&quot;</span><br><span class="line">  tcp_tls_key = &quot;&quot;</span><br><span class="line">  uid = 0</span><br><span class="line"></span><br><span class="line">[metrics]</span><br><span class="line">  address = &quot;&quot;</span><br><span class="line">  grpc_histogram = false</span><br><span class="line"></span><br><span class="line">[plugins]</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.gc.v1.scheduler&quot;]</span><br><span class="line">    deletion_threshold = 0</span><br><span class="line">    mutation_threshold = 100</span><br><span class="line">    pause_threshold = 0.02</span><br><span class="line">    schedule_delay = &quot;0s&quot;</span><br><span class="line">    startup_delay = &quot;100ms&quot;</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.grpc.v1.cri&quot;]</span><br><span class="line">    device_ownership_from_security_context = false</span><br><span class="line">    disable_apparmor = false</span><br><span class="line">    disable_cgroup = false</span><br><span class="line">    disable_hugetlb_controller = true</span><br><span class="line">    disable_proc_mount = false</span><br><span class="line">    disable_tcp_service = true</span><br><span class="line">    enable_selinux = false</span><br><span class="line">    enable_tls_streaming = false</span><br><span class="line">    enable_unprivileged_icmp = false</span><br><span class="line">    enable_unprivileged_ports = false</span><br><span class="line">    ignore_image_defined_volumes = false</span><br><span class="line">    max_concurrent_downloads = 3</span><br><span class="line">    max_container_log_line_size = 16384</span><br><span class="line">    netns_mounts_under_state_dir = false</span><br><span class="line">    restrict_oom_score_adj = false</span><br><span class="line">    sandbox_image = &quot;registry.aliyuncs.com/google_containers/pause:3.6&quot;</span><br><span class="line">    selinux_category_range = 1024</span><br><span class="line">    stats_collect_period = 10</span><br><span class="line">    stream_idle_timeout = &quot;4h0m0s&quot;</span><br><span class="line">    stream_server_address = &quot;127.0.0.1&quot;</span><br><span class="line">    stream_server_port = &quot;0&quot;</span><br><span class="line">    systemd_cgroup = false</span><br><span class="line">    tolerate_missing_hugetlb_controller = true</span><br><span class="line">    unset_seccomp_profile = &quot;&quot;</span><br><span class="line"></span><br><span class="line">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.cni]</span><br><span class="line">      bin_dir = &quot;/opt/cni/bin&quot;</span><br><span class="line">      conf_dir = &quot;/etc/cni/net.d&quot;</span><br><span class="line">      conf_template = &quot;&quot;</span><br><span class="line">      ip_pref = &quot;&quot;</span><br><span class="line">      max_conf_num = 1</span><br><span class="line"></span><br><span class="line">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd]</span><br><span class="line">      default_runtime_name = &quot;runc&quot;</span><br><span class="line">      disable_snapshot_annotations = true</span><br><span class="line">      discard_unpacked_layers = false</span><br><span class="line">      ignore_rdt_not_enabled_errors = false</span><br><span class="line">      no_pivot = false</span><br><span class="line">      snapshotter = &quot;overlayfs&quot;</span><br><span class="line"></span><br><span class="line">      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.default_runtime]</span><br><span class="line">        base_runtime_spec = &quot;&quot;</span><br><span class="line">        cni_conf_dir = &quot;&quot;</span><br><span class="line">        cni_max_conf_num = 0</span><br><span class="line">        container_annotations = []</span><br><span class="line">        pod_annotations = []</span><br><span class="line">        privileged_without_host_devices = false</span><br><span class="line">        runtime_engine = &quot;&quot;</span><br><span class="line">        runtime_path = &quot;&quot;</span><br><span class="line">        runtime_root = &quot;&quot;</span><br><span class="line">        runtime_type = &quot;&quot;</span><br><span class="line"></span><br><span class="line">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.default_runtime.options]</span><br><span class="line"></span><br><span class="line">      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes]</span><br><span class="line"></span><br><span class="line">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc]</span><br><span class="line">          base_runtime_spec = &quot;&quot;</span><br><span class="line">          cni_conf_dir = &quot;&quot;</span><br><span class="line">          cni_max_conf_num = 0</span><br><span class="line">          container_annotations = []</span><br><span class="line">          pod_annotations = []</span><br><span class="line">          privileged_without_host_devices = false</span><br><span class="line">          runtime_engine = &quot;&quot;</span><br><span class="line">          runtime_path = &quot;&quot;</span><br><span class="line">          runtime_root = &quot;&quot;</span><br><span class="line">          runtime_type = &quot;io.containerd.runc.v2&quot;</span><br><span class="line"></span><br><span class="line">          [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]</span><br><span class="line">            BinaryName = &quot;&quot;</span><br><span class="line">            CriuImagePath = &quot;&quot;</span><br><span class="line">            CriuPath = &quot;&quot;</span><br><span class="line">            CriuWorkPath = &quot;&quot;</span><br><span class="line">            IoGid = 0</span><br><span class="line">            IoUid = 0</span><br><span class="line">            NoNewKeyring = false</span><br><span class="line">            NoPivotRoot = false</span><br><span class="line">            Root = &quot;&quot;</span><br><span class="line">            ShimCgroup = &quot;&quot;</span><br><span class="line">            SystemdCgroup = false</span><br><span class="line"></span><br><span class="line">      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.untrusted_workload_runtime]</span><br><span class="line">        base_runtime_spec = &quot;&quot;</span><br><span class="line">        cni_conf_dir = &quot;&quot;</span><br><span class="line">        cni_max_conf_num = 0</span><br><span class="line">        container_annotations = []</span><br><span class="line">        pod_annotations = []</span><br><span class="line">        privileged_without_host_devices = false</span><br><span class="line">        runtime_engine = &quot;&quot;</span><br><span class="line">        runtime_path = &quot;&quot;</span><br><span class="line">        runtime_root = &quot;&quot;</span><br><span class="line">        runtime_type = &quot;&quot;</span><br><span class="line"></span><br><span class="line">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.untrusted_workload_runtime.options]</span><br><span class="line"></span><br><span class="line">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.image_decryption]</span><br><span class="line">      key_model = &quot;node&quot;</span><br><span class="line"></span><br><span class="line">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry]</span><br><span class="line">      config_path = &quot;&quot;</span><br><span class="line"></span><br><span class="line">      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors]</span><br><span class="line">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]</span><br><span class="line">          endpoint = [&quot;https://registry.cn-hangzhou.aliyuncs.com&quot;]</span><br><span class="line">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;192.168.31.252:5000&quot;]</span><br><span class="line">          endpoint = [&quot;http://192.168.31.252:5000&quot;]</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.internal.v1.opt&quot;]</span><br><span class="line">    path = &quot;/opt/containerd&quot;</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.internal.v1.restart&quot;]</span><br><span class="line">    interval = &quot;10s&quot;</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.internal.v1.tracing&quot;]</span><br><span class="line">    sampling_ratio = 1.0</span><br><span class="line">    service_name = &quot;containerd&quot;</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.metadata.v1.bolt&quot;]</span><br><span class="line">    content_sharing_policy = &quot;shared&quot;</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.monitor.v1.cgroups&quot;]</span><br><span class="line">    no_prometheus = false</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.runtime.v1.linux&quot;]</span><br><span class="line">    no_shim = false</span><br><span class="line">    runtime = &quot;runc&quot;</span><br><span class="line">    runtime_root = &quot;&quot;</span><br><span class="line">    shim = &quot;containerd-shim&quot;</span><br><span class="line">    shim_debug = false</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.runtime.v2.task&quot;]</span><br><span class="line">    platforms = [&quot;linux/amd64&quot;]</span><br><span class="line">    sched_core = false</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.service.v1.diff-service&quot;]</span><br><span class="line">    default = [&quot;walking&quot;]</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.service.v1.tasks-service&quot;]</span><br><span class="line">    rdt_config_file = &quot;&quot;</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.snapshotter.v1.aufs&quot;]</span><br><span class="line">    root_path = &quot;&quot;</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.snapshotter.v1.btrfs&quot;]</span><br><span class="line">    root_path = &quot;&quot;</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.snapshotter.v1.devmapper&quot;]</span><br><span class="line">    async_remove = false</span><br><span class="line">    base_image_size = &quot;&quot;</span><br><span class="line">    discard_blocks = false</span><br><span class="line">    fs_options = &quot;&quot;</span><br><span class="line">    fs_type = &quot;&quot;</span><br><span class="line">    pool_name = &quot;&quot;</span><br><span class="line">    root_path = &quot;&quot;</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.snapshotter.v1.native&quot;]</span><br><span class="line">    root_path = &quot;&quot;</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.snapshotter.v1.overlayfs&quot;]</span><br><span class="line">    mount_options = []</span><br><span class="line">    root_path = &quot;&quot;</span><br><span class="line">    sync_remove = false</span><br><span class="line">    upperdir_label = false</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.snapshotter.v1.zfs&quot;]</span><br><span class="line">    root_path = &quot;&quot;</span><br><span class="line"></span><br><span class="line">  [plugins.&quot;io.containerd.tracing.processor.v1.otlp&quot;]</span><br><span class="line">    endpoint = &quot;192.168.31.252:5000&quot;</span><br><span class="line">    insecure = true</span><br><span class="line">    protocol = &quot;http&quot;</span><br><span class="line"></span><br><span class="line">[proxy_plugins]</span><br><span class="line"></span><br><span class="line">[stream_processors]</span><br><span class="line"></span><br><span class="line">  [stream_processors.&quot;io.containerd.ocicrypt.decoder.v1.tar&quot;]</span><br><span class="line">    accepts = [&quot;application/vnd.oci.image.layer.v1.tar+encrypted&quot;]</span><br><span class="line">    args = [&quot;--decryption-keys-path&quot;, &quot;/etc/containerd/ocicrypt/keys&quot;]</span><br><span class="line">    env = [&quot;OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf&quot;]</span><br><span class="line">    path = &quot;ctd-decoder&quot;</span><br><span class="line">    returns = &quot;application/vnd.oci.image.layer.v1.tar&quot;</span><br><span class="line"></span><br><span class="line">  [stream_processors.&quot;io.containerd.ocicrypt.decoder.v1.tar.gzip&quot;]</span><br><span class="line">    accepts = [&quot;application/vnd.oci.image.layer.v1.tar+gzip+encrypted&quot;]</span><br><span class="line">    args = [&quot;--decryption-keys-path&quot;, &quot;/etc/containerd/ocicrypt/keys&quot;]</span><br><span class="line">    env = [&quot;OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf&quot;]</span><br><span class="line">    path = &quot;ctd-decoder&quot;</span><br><span class="line">    returns = &quot;application/vnd.oci.image.layer.v1.tar+gzip&quot;</span><br><span class="line"></span><br><span class="line">[timeouts]</span><br><span class="line">  &quot;io.containerd.timeout.bolt.open&quot; = &quot;0s&quot;</span><br><span class="line">  &quot;io.containerd.timeout.shim.cleanup&quot; = &quot;5s&quot;</span><br><span class="line">  &quot;io.containerd.timeout.shim.load&quot; = &quot;5s&quot;</span><br><span class="line">  &quot;io.containerd.timeout.shim.shutdown&quot; = &quot;3s&quot;</span><br><span class="line">  &quot;io.containerd.timeout.task.state&quot; = &quot;2s&quot;</span><br><span class="line"></span><br><span class="line">[ttrpc]</span><br><span class="line">  address = &quot;&quot;</span><br><span class="line">  gid = 0</span><br><span class="line">  uid = 0</span><br><span class="line">[root@master-01 ~]# </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>复制配置文件并重启containerd</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart containerd</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="K8s" scheme="http://blog.ioimp.top/categories/K8s/"/>
    
    
    <category term="K8s基础学习" scheme="http://blog.ioimp.top/tags/K8s%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="编程知识" scheme="http://blog.ioimp.top/tags/%E7%BC%96%E7%A8%8B%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>centos7 卸载docker</title>
    <link href="http://blog.ioimp.top/2023/11/23/centos7-%E5%8D%B8%E8%BD%BDdocker/"/>
    <id>http://blog.ioimp.top/2023/11/23/centos7-%E5%8D%B8%E8%BD%BDdocker/</id>
    <published>2023-11-23T02:52:31.000Z</published>
    <updated>2023-11-23T02:55:27.513Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="centos7-卸载docker"><a href="#centos7-卸载docker" class="headerlink" title="centos7 卸载docker"></a>centos7 卸载docker</h1><h2 id="CentOS-7下如何卸载Docker"><a href="#CentOS-7下如何卸载Docker" class="headerlink" title="CentOS 7下如何卸载Docker"></a>CentOS 7下如何卸载Docker</h2><p>![CentOS 7下如何卸载Docker](</p><blockquote><p>本文介绍了在CentOS 7上如何卸载Docker。我们将使用命令行来完成这个过程。在开始之前，请确保您具有管理员权限。</p></blockquote><h3 id="1-检查Docker安装情况"><a href="#1-检查Docker安装情况" class="headerlink" title="1. 检查Docker安装情况"></a>1. 检查Docker安装情况</h3><p>在卸载Docker之前，首先我们需要检查系统中是否已经安装了Docker。可以使用以下命令来验证：</p><p>登录后复制</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker version</span><br><span class="line">1.</span><br></pre></td></tr></table></figure><p>如果您看到有关Docker的版本信息，则表示Docker已经安装在您的系统中。如果没有任何输出，说明您的系统中没有安装Docker。</p><h3 id="2-停止Docker服务"><a href="#2-停止Docker服务" class="headerlink" title="2. 停止Docker服务"></a>2. 停止Docker服务</h3><p>在卸载Docker之前，我们需要停止正在运行的Docker服务。可以使用以下命令来停止Docker服务：</p><p>登录后复制</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl stop docker</span><br><span class="line">1.</span><br></pre></td></tr></table></figure><h3 id="3-卸载Docker软件包"><a href="#3-卸载Docker软件包" class="headerlink" title="3. 卸载Docker软件包"></a>3. 卸载Docker软件包</h3><p>现在我们可以开始卸载Docker软件包了。可以使用以下命令来卸载Docker：</p><p>登录后复制</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo yum remove docker</span><br><span class="line">1.</span><br></pre></td></tr></table></figure><p>执行以上命令后，系统将提示您确认是否要卸载Docker软件包。输入<code>y</code>并按下回车键继续。</p><h3 id="4-删除Docker数据目录"><a href="#4-删除Docker数据目录" class="headerlink" title="4. 删除Docker数据目录"></a>4. 删除Docker数据目录</h3><p>卸载Docker软件包后，还需要手动删除Docker的数据目录。可以使用以下命令来删除Docker数据目录：</p><p>登录后复制</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo rm -rf /var/lib/docker</span><br><span class="line">1.</span><br></pre></td></tr></table></figure><h3 id="5-删除Docker镜像和容器"><a href="#5-删除Docker镜像和容器" class="headerlink" title="5. 删除Docker镜像和容器"></a>5. 删除Docker镜像和容器</h3><p>卸载Docker后，您可能还需要删除已经下载的Docker镜像和容器。可以使用以下命令来删除所有Docker镜像和容器：</p><p>登录后复制</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker rm -f $(docker ps -a -q)</span><br><span class="line">docker rmi -f $(docker images -a -q)</span><br><span class="line">1.2.</span><br></pre></td></tr></table></figure><p>执行以上命令后，系统将删除所有已经停止的容器和所有的镜像。</p><h3 id="6-清理Docker残留配置"><a href="#6-清理Docker残留配置" class="headerlink" title="6. 清理Docker残留配置"></a>6. 清理Docker残留配置</h3><p>有时候，即使卸载了Docker软件包，系统中仍然可能会有一些残留的配置文件。可以使用以下命令来清理Docker的残留配置：</p><p>登录后复制</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo rm -rf /etc/docker</span><br><span class="line">sudo rm -rf ~/.docker</span><br><span class="line">1.2.</span><br></pre></td></tr></table></figure><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>在本文中，我们介绍了在CentOS 7上如何卸载Docker。通过使用命令行，您可以轻松地完成这个过程。请记住，在卸载Docker之前，一定要停止正在运行的Docker服务，并且备份您的重要数据。希望本文对您有所帮助！</p><p>卸载Docker的过程</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="疑难解答" scheme="http://blog.ioimp.top/categories/%E7%96%91%E9%9A%BE%E8%A7%A3%E7%AD%94/"/>
    
    
    <category term="文章收录" scheme="http://blog.ioimp.top/tags/%E6%96%87%E7%AB%A0%E6%94%B6%E5%BD%95/"/>
    
    <category term="拯救小白系列" scheme="http://blog.ioimp.top/tags/%E6%8B%AF%E6%95%91%E5%B0%8F%E7%99%BD%E7%B3%BB%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>拯救小白焦虑手册</title>
    <link href="http://blog.ioimp.top/2023/11/22/%E6%8B%AF%E6%95%91%E5%B0%8F%E7%99%BD%E7%84%A6%E8%99%91%E6%89%8B%E5%86%8C/"/>
    <id>http://blog.ioimp.top/2023/11/22/%E6%8B%AF%E6%95%91%E5%B0%8F%E7%99%BD%E7%84%A6%E8%99%91%E6%89%8B%E5%86%8C/</id>
    <published>2023-11-22T03:21:16.000Z</published>
    <updated>2023-11-22T03:21:52.873Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="焦虑自救小册"><a href="#焦虑自救小册" class="headerlink" title="焦虑自救小册"></a>焦虑自救小册</h1><hr><p>最近有的读者找我咨询问题，聊下来我发现 IT 从业人员共有的突出问题就是焦虑。现在疫情、政策等大环境原因导致行业不稳定和生活艰难，焦虑是很多人的常态。</p><p>我也曾经焦虑过大概两年，这种状态刚开始自己还没意识到，直到出现了一些身体上很明显的症状才引起重视。后来经过一些自我调整我基本解决了这个问题，我之前写了篇文章分享自己的方法：<a href="https://catcoding.me/p/how-to-beat-anxiety/">35 岁，我用这三种方法克服焦虑</a></p><p>虽然相对前两年，我的焦虑感确实少了很多，但我并不认为已经一劳永逸地解决了这个问题，每个年龄段有不同的焦虑点，这似乎是一个一生需要关注的事情。我的这些方法虽然对自己有用，也不一定适合所有人，而且换城市、换工作的隐形成本是很大的。</p><p>我最近和学心理学的老同学聊了聊，顺便收集了一些工具、方法和书籍，总结出来供大家参考。</p><p>如果大家能一起来完善这个小手册就更好了，这文档共享在 Github：<a href="https://github.com/chenyukang/anxiety-handbook">anxiety-handbook</a></p><h2 id="自我测试"><a href="#自我测试" class="headerlink" title="自我测试"></a>自我测试</h2><p><a href="https://www.qqtest.com/s/20.htm">SAS焦虑自评量表 - 健康心理测试</a></p><p><a href="https://www.idrlabs.com/cn/anxiety-stress-depression/test.php">焦虑、压力、抑郁测试</a></p><p>关于测试：</p><ul><li><strong>正确对待测试，不要对结果恐慌</strong>。测试前做好心理建设，不管结果多遭，不要害怕它，想想自己参与测试的初衷是想让事情朝正确的方向发展。</li><li><strong>将测试当作解决问题的工具</strong>。测试结果可以用来检测自己应对焦虑的情况：当焦虑变严重时，思考自己哪里没有做到位，不断改进克服它的方法；当焦虑减轻时，给自己一些奖励，让自己再放松一些。</li></ul><h2 id="认识焦虑"><a href="#认识焦虑" class="headerlink" title="认识焦虑"></a>认识焦虑</h2><p>焦虑多是由不确定引起的，是<strong>人类进化过程中保留下来的对外界的戒备</strong>。对于大部分人来说，焦虑是无法完全避免的，甚至适度的焦虑是能促使人进步。但是焦虑达到一定程度就会影响健康，特别是影响睡眠的时候情况更为糟糕。</p><blockquote><p>焦虑患者常常对现实生活中的某些事情或将来的某些事情表现的过分担忧，有时患者也可以无明确目标的担忧。这种担心往往是与现实极不相称的，使患者感到非常的痛苦。还伴有自主神经亢进，肌肉紧张或跳动等<a href="https://zh.wikipedia.org/wiki/%E8%87%AA%E5%BE%8B%E7%A5%9E%E7%B6%93%E5%A4%B1%E8%AA%BF" title="自律神经失调">自律神经失调</a> 的症状。部分患者会自觉身体总是不舒服多次去医院看医生，又检查不出症状。但是对于患者来说，总会一直担心。</p><p>– 维基百科</p></blockquote><p>就我个人体会，焦虑的时候会失眠、消化不良、精神紧张、心跳有时候感觉速度快。</p><h2 id="产生原因"><a href="#产生原因" class="headerlink" title="产生原因"></a>产生原因</h2><p>认识到自己焦虑的原因是缓解焦虑的第一步，IT 从业人员的焦虑是很多是由国内的行业氛围、社会大环境导致的。</p><p>职场的 996 工作节奏、35 岁现象、末尾淘汰、职场 PUA 等等，这些让人没有足够的安全感。</p><p>另一部分原因是自我认知造成的，我们大多数人沿着社会预期的道路而行，并未想过自己想要什么样、适合什么样的生活。</p><p>这与教育和文化背景有关系，我们一直都存在一个较为单一的评价体系，在学校里面我们为了分数而竞争，毕业时我们为了一份的工作而竞争，职场上我们为了更好的绩效、更高的工资而竞争。竞争是我们的生活常态，而既然有竞争就会导致人有压力，日积月累形成焦虑。</p><p>而且，焦虑特别容易出现在对自己有要求的人群中，出现的年龄段也比较集中，焦虑大多出现在 25 ~ 40 左右，因为这个年龄段是职场的关键时期，也逐渐需要承担家庭的责任。</p><p>往往我们过了某些时间关口就会好一些，这个关口可能是认知和价值观上的改变，可能是学会了接纳自己，或者是学会了和焦虑相处。希望这个手册能帮你更快地闯过关口。</p><h2 id="职场焦虑"><a href="#职场焦虑" class="headerlink" title="职场焦虑"></a>职场焦虑</h2><p>摆脱焦虑一个很重要的部分是认识自己所处的环境，识别出周围环境中的有害因素，这包括国内 IT 行业的两个常见问题：</p><h3 id="内卷"><a href="#内卷" class="headerlink" title="内卷"></a>内卷</h3><p>内卷会让人长期处于无意义的竞争状态，非常容易让人滋生焦虑。国内很多 IT 公司存在内卷的情况，这里有一个 IT 从业者维护的 996 公司列表你可以参考。以我的经验来说，对于大型 IT 公司，个人所处于的小组和部门可能更重要，直属 Leader 对你影响很大程度上决定了你在公司的工作感受：</p><p><a href="https://github.com/996icu/996.ICU/tree/master/blacklist">996.ICU&#x2F;blacklist · 996icu&#x2F;996.ICU</a></p><p>如果你想改变内卷的环境，可以通过换组或者是换工作到 955 的公司，换城市、或者甚至去国外：</p><ul><li><a href="https://github.com/formulahendry/955.WLB">955.WLB 955 不加班的公司名单</a></li><li><a href="https://github.com/623637646/996.Leave">996.Leave 逃离996</a></li><li><a href="https://github.com/chenyukang/remote-jobs-cn">remote-jobs-cn 国内远程办公职位</a></li></ul><h3 id="职场-PUA"><a href="#职场-PUA" class="headerlink" title="职场 PUA"></a>职场 PUA</h3><p>PUA 全称 “Pick-up Artist”，起初指的是受过系统化学习实践精神控制者，让异性着迷的男女们，字面上的解释 PUA 指的是搭讪艺术家。</p><p>PUA 是一种诱骗和洗脑的技术，从而神不知鬼不觉的达到自己的目的，而且这是一种很难发现，非常隐晦的一种欺骗方式。</p><p>一些管理者并没有良好的管理能力，倒是学会了一些抓人的手法，所以职场 PUA 是很多 IT 人面临的困境。常见的职场 PUA 手法：</p><ul><li>否定，不断批评和挑刺，有的时候会美其名曰鞭策你进步</li><li>打压，分配过多的任务，但是会告诉你在锻炼你</li><li>对比，拿你和别人对比，让你造成心理落差，或者怀疑自己的能力</li></ul><p><a href="https://www.bilibili.com/video/BV1pr4y1p7pB?spm_id_from=333.999.0.0">怎么看领导是培养还是PUA我？</a></p><p>当然还有很多手法，其实 PUA 的本质是让你觉得一切都是为了你好，从而让你不用怀疑地去执行任务。被 PUA 的人往往自身比较难以认识到，需要跳出来才能发现。</p><p>如果你的 Leader 让你太累，心理压力大，可以往这方面考虑一下自己是否在被 PUA。如果遭遇 PUA 解决办法就是换组或者换公司。</p><h2 id="改变认知"><a href="#改变认知" class="headerlink" title="改变认知"></a>改变认知</h2><p>焦虑部分是因为认知局限所造成的，所谓庸人自扰。提升思维高度可以从根本上解决一些问题。思维和认知的高度往往也涉及到对人生中重要事项的看法和选择，这包括：</p><ul><li>关于钱、工作的看法</li><li>是否待一线城市</li><li>是否结婚和要小孩</li><li>如何衡量成功</li><li>什么是幸福</li><li>对死亡的看法</li></ul><p>个人的经历可能会改变认知，除此之外阅读、观影、思考、交谈也可能会改变认知。下面是我收集的一些相关书籍和纪录片。</p><h3 id="书籍推荐"><a href="#书籍推荐" class="headerlink" title="书籍推荐"></a>书籍推荐</h3><p>解决焦虑问题类的书籍，往往被当作心灵鸡汤，鸡汤并没有不好，当你过于焦虑的时候，喝碗鸡汤有时候挺有用。另外一些哲学、历史方面的书也有用：</p><p><a href="https://book.douban.com/subject/2277299/">《当下的力量》</a> 生活在过去使人忧愁，生活在未来使人焦虑，最好的状态是活在当下。</p><p>《象与骑象人》如何获取幸福，过有意义的生活</p><p>《人生的智慧》叔本华关于健康、财富、名声、荣誉、养生和待人接物所应遵守的原则等。非常推荐。</p><p>《沉思录》</p><p>《被讨厌的勇气》</p><p>《获得幸福的勇气》</p><p><a href="https://book.douban.com/subject/35539713/">《焦虑的人》</a> 这是一本小说，情节跌宕起伏，故事温暖又治愈。</p><p>《精神焦虑症的自救》分为病例分析卷和访谈卷，包含对焦虑的全面介绍，还有大量摆脱焦虑情绪的技巧。</p><p>《焦虑是头大象，如何一口一口吃掉它》作者主张通过自我书写缓解焦虑，有书写的建议和方法，帮助人们辨别不同的焦虑状况如何用自我书写来缓解。</p><h3 id="纪录片"><a href="#纪录片" class="headerlink" title="纪录片"></a>纪录片</h3><p>好的纪录片能让人了解个人生活之外的广袤世界、历史长河，意识到人类之渺小，从而改变认知：</p><ul><li>蓝色星球</li><li>王朝</li><li>脸庞，村庄</li><li>河西走廊</li><li>人世间</li><li>徒手攀岩</li><li><a href="https://open.163.com/newview/movie/courseintro?newurl=M6HV755O6">哈佛大学公开课：幸福课</a></li></ul><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="培养爱好"><a href="#培养爱好" class="headerlink" title="培养爱好"></a>培养爱好</h3><p>培养爱好可以缓解部分焦虑，因为爱好让我们从日常工作生活中脱离出来。很多中年人都是通过爱好来缓解生活中的琐碎感。</p><p>运动、写作、钓鱼、摄影、乐高等，都是非常好的爱好。</p><h3 id="日常技巧"><a href="#日常技巧" class="headerlink" title="日常技巧"></a>日常技巧</h3><p>常对自己说这几个字：</p><p><strong>“无所谓”<br>“没必要”<br>“不至于”</strong></p><p>冯唐分享过一个做法，如果你焦虑或者认为自己碰上了什么迈不过去的坎，找个医院去 ICU 门口待上一段时间。</p><h3 id="心理咨询"><a href="#心理咨询" class="headerlink" title="心理咨询"></a>心理咨询</h3><p>和行业相关的朋友咨询了一下，目前主流的心理治疗价目表:</p><ul><li><p>新手 200-300</p></li><li><p>中级 500-800</p></li><li><p>专家 1000 以上</p></li></ul><h3 id="药物治疗"><a href="#药物治疗" class="headerlink" title="药物治疗"></a>药物治疗</h3><p>常见药和副作用，待补充。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="随笔" scheme="http://blog.ioimp.top/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="文章收录" scheme="http://blog.ioimp.top/tags/%E6%96%87%E7%AB%A0%E6%94%B6%E5%BD%95/"/>
    
    <category term="拯救小白系列" scheme="http://blog.ioimp.top/tags/%E6%8B%AF%E6%95%91%E5%B0%8F%E7%99%BD%E7%B3%BB%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>记一次jenkins构建失败的踩坑记录</title>
    <link href="http://blog.ioimp.top/2023/11/22/%E8%AE%B0%E4%B8%80%E6%AC%A1jenkins%E6%9E%84%E5%BB%BA%E5%A4%B1%E8%B4%A5%E7%9A%84%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/"/>
    <id>http://blog.ioimp.top/2023/11/22/%E8%AE%B0%E4%B8%80%E6%AC%A1jenkins%E6%9E%84%E5%BB%BA%E5%A4%B1%E8%B4%A5%E7%9A%84%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/</id>
    <published>2023-11-22T01:58:50.000Z</published>
    <updated>2023-11-22T01:59:17.770Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="记一次jenkins构建失败的踩坑记录"><a href="#记一次jenkins构建失败的踩坑记录" class="headerlink" title="记一次jenkins构建失败的踩坑记录"></a>记一次jenkins构建失败的踩坑记录</h1><p>核心要旨:排错要一步一步排查,一步一步确认,确认问题失败在哪一步,而不是凭空猜测错误.</p><p>异常信息:</p><p>登录后复制</p><p>Started by user admin<br>Running as SYSTEM<br>Building in workspace &#x2F;root&#x2F;.jenkins&#x2F;workspace&#x2F;app-server<br>using credential 2c84e055-ab32-4bcb-9642-e490e1fb4443  </p><blockquote><p>&#x2F;usr&#x2F;bin&#x2F;git rev-parse –is-inside-work-tree # timeout&#x3D;10<br>Fetching changes from the remote Git repository<br>&#x2F;usr&#x2F;bin&#x2F;git config remote.origin.url <a href="https://gitee.com/kinome/aggregationServicePlatform.git">https://gitee.com/kinome/aggregationServicePlatform.git</a> # timeout&#x3D;10<br>Using shallow fetch with depth 1<br>Fetching upstream changes from <a href="https://gitee.com/kinome/aggregationServicePlatform.git">https://gitee.com/kinome/aggregationServicePlatform.git</a><br>&#x2F;usr&#x2F;bin&#x2F;git –version # timeout&#x3D;10<br>using GIT_ASKPASS to set credentials<br>&#x2F;usr&#x2F;bin&#x2F;git fetch –tags –progress –depth&#x3D;1 <a href="https://gitee.com/kinome/aggregationServicePlatform.git">https://gitee.com/kinome/aggregationServicePlatform.git</a> +refs&#x2F;heads&#x2F;*:refs&#x2F;remotes&#x2F;origin&#x2F;* # timeout&#x3D;60<br>&#x2F;usr&#x2F;bin&#x2F;git rev-parse refs&#x2F;remotes&#x2F;origin&#x2F;master^{commit} # timeout&#x3D;10<br>&#x2F;usr&#x2F;bin&#x2F;git rev-parse refs&#x2F;remotes&#x2F;origin&#x2F;origin&#x2F;master^{commit} # timeout&#x3D;10<br>Checking out Revision 0e92eabfe44ed70dcc240fcd7b714c2de2f0c7c6 (refs&#x2F;remotes&#x2F;origin&#x2F;master)<br>&#x2F;usr&#x2F;bin&#x2F;git config core.sparsecheckout # timeout&#x3D;10<br>&#x2F;usr&#x2F;bin&#x2F;git checkout -f 0e92eabfe44ed70dcc240fcd7b714c2de2f0c7c6 # timeout&#x3D;10<br>Commit message: “commit”<br>&#x2F;usr&#x2F;bin&#x2F;git rev-list –no-walk 0e92eabfe44ed70dcc240fcd7b714c2de2f0c7c6 # timeout&#x3D;10<br>Parsing POMs<br>Established TCP socket on 37780<br>[app-server] $ &#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64&#x2F;jre&#x2F;bin&#x2F;java -cp &#x2F;root&#x2F;.jenkins&#x2F;plugins&#x2F;maven-plugin&#x2F;WEB-INF&#x2F;lib&#x2F;maven3-agent-1.13.jar:&#x2F;usr&#x2F;share&#x2F;maven&#x2F;boot&#x2F;plexus-classworlds.jar org.jvnet.hudson.maven3.agent.Maven3Main &#x2F;usr&#x2F;share&#x2F;maven &#x2F;root&#x2F;.jenkins&#x2F;war&#x2F;WEB-INF&#x2F;lib&#x2F;remoting-4.2.jar &#x2F;root&#x2F;.jenkins&#x2F;plugins&#x2F;maven-plugin&#x2F;WEB-INF&#x2F;lib&#x2F;maven3-interceptor-1.13.jar &#x2F;root&#x2F;.jenkins&#x2F;plugins&#x2F;maven-plugin&#x2F;WEB-INF&#x2F;lib&#x2F;maven3-interceptor-commons-1.13.jar 37780<br>ERROR: Failed to parse POMs<br>java.io.IOException: Cannot run program “&#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64&#x2F;jre&#x2F;bin&#x2F;java” (in directory “&#x2F;root&#x2F;.jenkins&#x2F;workspace&#x2F;app-server”): error&#x3D;2, 没有那个文件或目录<br>at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)<br>at hudson.Proc$LocalProc.<init>(Proc.java:252)<br>at hudson.Proc$LocalProc.<init>(Proc.java:221)<br>at hudson.Launcher$LocalLauncher.launch(Launcher.java:936)<br>at hudson.Launcher$ProcStarter.start(Launcher.java:454)<br>at hudson.maven.AbstractMavenProcessFactory.newProcess(AbstractMavenProcessFactory.java:280)<br>at hudson.maven.ProcessCache.get(ProcessCache.java:236)<br>at hudson.maven.MavenModuleSetBuild$MavenModuleSetBuildExecution.doRun(MavenModuleSetBuild.java:804)<br>at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:504)<br>at hudson.model.Run.execute(Run.java:1856)<br>at hudson.maven.MavenModuleSetBuild.run(MavenModuleSetBuild.java:543)<br>at hudson.model.ResourceController.execute(ResourceController.java:97)<br>at hudson.model.Executor.run(Executor.java:428)<br>Caused by: java.io.IOException: error&#x3D;2, 没有那个文件或目录<br>at java.lang.UNIXProcess.forkAndExec(Native Method)<br>at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)<br>at java.lang.ProcessImpl.start(ProcessImpl.java:134)<br>at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)<br>… 12 more<br>Finished: FAILURE</p></blockquote><p>本质原因是因为jdk版本升级了,之前的javahome路径失效了导致的.</p><p>但是在java升级的那天,我修改了gitee的密码,然后我从一开始就以为是因为凭证出了问题(因为在第一步就是使用凭证拉取git上的项目),然后我又看到timeout&#x3D;10这种提示,以为是真的超时了(其实只是在提示超时时间值,并没有真的超时),然后又是各种搜,各种尝试跟凭证有关的东西,甚至想用sshkey来弄结果不行.</p><p>但是其实一开始排查就发现用户名和密码正确,也没有报错,但是就是构建失败,其实这个时候我还是以为拉取失败了,这一步我应该在确认了用户名密码没错并且没报错的情况下,先检查有没有真的拉取到,然后再进行判断的,而不是理所当然的猜测.</p><p>然后检查到了其实是拉取到了,跟凭证没关系,往下走发现了java找不到的异常,修改javahome之后就可以了.</p><p>正好修改gitee的密码的那天升级了java,才对认知造成了影响.</p><p>所以以后如果出现bug,应该一步一步按照事实和异常消息来,并且检查相关配置,而不是盲目百度和把猜测作为事实.</p><p>=&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 更新 2021-05-19</p><p><img src="https://s2.51cto.com/images/blog/202208/18105254_62fda986e37fe13198.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184" alt="记一次jenkins构建失败的踩坑记录_jenkins"></p><p> -U clean install 用于解决 Jenkins构建未拉取最新的包导致自动构建失败 </p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Jenkins" scheme="http://blog.ioimp.top/categories/Jenkins/"/>
    
    
    <category term="日常小BUG" scheme="http://blog.ioimp.top/tags/%E6%97%A5%E5%B8%B8%E5%B0%8FBUG/"/>
    
    <category term="Jenkins学习" scheme="http://blog.ioimp.top/tags/Jenkins%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu 18.04 出现GLIBC_2.28 not found的解决方法(亲测有效)</title>
    <link href="http://blog.ioimp.top/2023/11/21/Ubuntu-18-04-%E5%87%BA%E7%8E%B0GLIBC-2-28-not-found%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95-%E4%BA%B2%E6%B5%8B%E6%9C%89%E6%95%88/"/>
    <id>http://blog.ioimp.top/2023/11/21/Ubuntu-18-04-%E5%87%BA%E7%8E%B0GLIBC-2-28-not-found%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95-%E4%BA%B2%E6%B5%8B%E6%9C%89%E6%95%88/</id>
    <published>2023-11-21T10:34:43.000Z</published>
    <updated>2023-11-21T10:37:29.586Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="Ubuntu-18-04-出现GLIBC-2-28-not-found的解决方法-亲测有效"><a href="#Ubuntu-18-04-出现GLIBC-2-28-not-found的解决方法-亲测有效" class="headerlink" title="Ubuntu 18.04 出现GLIBC_2.28 not found的解决方法(亲测有效)"></a>Ubuntu 18.04 出现GLIBC_2.28 not found的解决方法(亲测有效)</h2><h5 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># uname -a</span><br><span class="line">Linux Ubuntu 5.4.0-144-generic #161~18.04.1-Ubuntu SMP Fri Feb 10 15:55:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure><h5 id="分析原因"><a href="#分析原因" class="headerlink" title="分析原因"></a>分析原因</h5><p>glibc是linux底层的API库。通常情况下，有些环境需要glibc更高的版本才支持，比如<code>GLIBC_2.28</code>。</p><p>另外对它操作升级，可能有导致系统崩溃的风险。</p><h5 id="经验与教训"><a href="#经验与教训" class="headerlink" title="经验与教训"></a>经验与教训</h5><p>使用<code>GLIBC_xxx</code>的源码包编译升级的惨案:</p><ul><li><p>提醒：在其他博客教程上，有些网友(我也不另外,后面可拯救回来)就按照教程并使用<code>GLIBC_xxx</code>的源码包并去升级，结果往往是系统崩溃而告终。</p></li><li><p>glibc库对linux系统非常重要，轻易不要更换。如果需要更换，需提前备份好原本的相关库以防万一。</p></li><li><p>若在使用源码包去升级之后出现<code>segmentation fault</code>,命令无法使用的情况。</p></li><li><p>解决方法：<br>若安装失败，可能导致各指令出错，除了cd、pwd基本都不可使用，这时候千万不要关闭窗口(如果关闭将导致将无法打开，只能重装系统)，比如安装libc-2.28.so出错了，需拯救系统。可尝试输入其中一条</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export LD_PRELOAD=/lib64/librt-2.XX.so</span><br><span class="line">export LD_PRELOAD=/lib64/libm-2.XX.so</span><br><span class="line">export LD_PRELOAD=/lib64/libpthread-2.XX.so</span><br><span class="line">export LD_PRELOAD=/lib64/libc-2.XX.so</span><br><span class="line">export LD_PRELOAD=/lib/x86_64-linux-gnu/libc-2.XX.so</span><br></pre></td></tr></table></figure><p>(XX指原本的版本，看文件夹有哪个就试一下)，然后ls这些指令就可以用了，再使用ln -s把以前的库链接回来。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /lib/x86_64-linux-gnu</span><br><span class="line">ll     # 文件详细信息</span><br><span class="line"></span><br><span class="line">ln -sf libc-2.27.so libc.so.6   # libc-2.27.so是原有版本</span><br><span class="line">rm  libc-2.28.so     #删除</span><br></pre></td></tr></table></figure><h5 id="软件包升级GLIBC-2-28"><a href="#软件包升级GLIBC-2-28" class="headerlink" title="软件包升级GLIBC_2.28"></a>软件包升级<code>GLIBC_2.28</code></h5><p><code>1</code> 查看服务器当前版本，命令如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">strings /lib/x86_64-linux-gnu/libc.so.6 | grep GLIBC_</span><br></pre></td></tr></table></figure><p>返回的结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">GLIBC_2.2.5</span><br><span class="line">GLIBC_2.2.6</span><br><span class="line">GLIBC_2.3</span><br><span class="line">GLIBC_2.3.2</span><br><span class="line">GLIBC_2.3.3</span><br><span class="line">GLIBC_2.3.4</span><br><span class="line">GLIBC_2.4</span><br><span class="line">GLIBC_2.5</span><br><span class="line">GLIBC_2.6</span><br><span class="line">GLIBC_2.7</span><br><span class="line">GLIBC_2.8</span><br><span class="line">GLIBC_2.9</span><br><span class="line">GLIBC_2.10</span><br><span class="line">GLIBC_2.11</span><br><span class="line">GLIBC_2.12</span><br><span class="line">GLIBC_2.13</span><br><span class="line">GLIBC_2.14</span><br><span class="line">GLIBC_2.15</span><br><span class="line">GLIBC_2.16</span><br><span class="line">GLIBC_2.17</span><br><span class="line">GLIBC_2.18</span><br><span class="line">GLIBC_2.22</span><br><span class="line">GLIBC_2.23</span><br><span class="line">GLIBC_2.24</span><br><span class="line">GLIBC_2.25</span><br><span class="line">GLIBC_2.26</span><br><span class="line">GLIBC_2.27</span><br><span class="line">GLIBC_PRIVATE</span><br></pre></td></tr></table></figure><p>说明服务器当前是没有GLIBC_2.28</p><p><code>2</code> 使用软件包升级方式</p><ul><li><p>参考<a href="https://packages.debian.org/buster/">debian网址</a>并搜索想要的软件或者工具等，如<code>libc6</code>,有结果如下：<br><img src="https://img-blog.csdnimg.cn/5ea130bc73c341d183a14190db14e8b4.png" alt="在这里插入图片描述"><br>具体就不介绍了，请浏览官网了解。</p></li><li><p>添加软件源，<code>/etc/apt/sources.list</code>文件中像下面这样添加一行：</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deb http://security.debian.org/debian-security buster/updates main </span><br></pre></td></tr></table></figure><ul><li>系统可用的软件包更新，刷新软件包的缓存</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update  # 更新软件源</span><br></pre></td></tr></table></figure><ul><li><code>apt-get update</code>之后若出现下面提示：<br><code>由于没有公钥，无法验证下列签名： NO_PUBKEY 112695A0E562B32A NO_PUBKEY 54404762BBB6E853</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A 54404762BBB6E853</span><br></pre></td></tr></table></figure><ul><li><p>其中后面的<code>112695A0E562B32A 54404762BBB6E853</code>就是上面提到的<code>NO_PUBKEY 112695A0E562B32A NO_PUBKEY 54404762BBB6E853</code>中的公钥，替换成对应的即可。然后重新<code>apt-get update</code>即可。</p></li><li><p>查看软件包可更新列表</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt list --upgradable   </span><br></pre></td></tr></table></figure><p>如下图所示：<br><img src="https://img-blog.csdnimg.cn/de34ac38295e4c8e9f5854cbc3a18aa1.png" alt="在这里插入图片描述"></p><ul><li>安装libc6</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install libc6-dev  /sudo apt install libc6</span><br></pre></td></tr></table></figure><p><code>3</code> 查看服务器当前版本：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">strings /lib/x86_64-linux-gnu/libc.so.6 | grep GLIBC_</span><br></pre></td></tr></table></figure><p>返回的结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">GLIBC_2.2.5</span><br><span class="line">GLIBC_2.2.6</span><br><span class="line">GLIBC_2.3</span><br><span class="line">GLIBC_2.3.2</span><br><span class="line">GLIBC_2.3.3</span><br><span class="line">GLIBC_2.3.4</span><br><span class="line">GLIBC_2.4</span><br><span class="line">GLIBC_2.5</span><br><span class="line">GLIBC_2.6</span><br><span class="line">GLIBC_2.7</span><br><span class="line">GLIBC_2.8</span><br><span class="line">GLIBC_2.9</span><br><span class="line">GLIBC_2.10</span><br><span class="line">GLIBC_2.11</span><br><span class="line">GLIBC_2.12</span><br><span class="line">GLIBC_2.13</span><br><span class="line">GLIBC_2.14</span><br><span class="line">GLIBC_2.15</span><br><span class="line">GLIBC_2.16</span><br><span class="line">GLIBC_2.17</span><br><span class="line">GLIBC_2.18</span><br><span class="line">GLIBC_2.22</span><br><span class="line">GLIBC_2.23</span><br><span class="line">GLIBC_2.24</span><br><span class="line">GLIBC_2.25</span><br><span class="line">GLIBC_2.26</span><br><span class="line">GLIBC_2.27</span><br><span class="line">GLIBC_2.28     # 多出该版本，说明安装成功，系统也能正常使用。</span><br><span class="line">GLIBC_PRIVATE</span><br></pre></td></tr></table></figure><p>如下图所示：<br><img src="https://img-blog.csdnimg.cn/94237d434d92453481306bf27dba3d1b.png" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Ubuntu" scheme="http://blog.ioimp.top/categories/Ubuntu/"/>
    
    
    <category term="日常小BUG" scheme="http://blog.ioimp.top/tags/%E6%97%A5%E5%B8%B8%E5%B0%8FBUG/"/>
    
    <category term="Linux学习" scheme="http://blog.ioimp.top/tags/Linux%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
